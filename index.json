[{"categories":["Architecture"],"content":"Deciding between software architecture attributes primacy is hard. I follow some practices that I will talk a little about them here.","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Last week I was talking to a former colleague, he is running a SaaS startup now, and our conversation went around SRE, production, and reliability. We talked a lot about architectural attributes that relate or impact the incidents, from detection to recovery, and on to prevention. I thought to put some of my thoughts around architecture into a blog post since that was the core of our conversation. The next post I will cover more details on architecture for fault tolerance. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:0:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Architectural attributes, the -ities I generally put different architectural attributes into two categories: Internal: for example operability, testability, supportability, upgradability, extensibility, etc. External: for example scalability, responsiveness, security, compliance, usability, availability, etc. ) The architectural attributes can be sliced and diced in different ways, I will probably write another slicing and decision making in another post. At the end of the day, any attribute and quality of a software system are going to impact the users. But some attributes are more concerns of customers and users of the system while another group is more concerning those building and maintaining it (developers, support, QA, etc.). There is always a push and pull between different stakeholders on the importance and investment that should go into these two categories. For example, usability, availability, and responsiveness might be the product owner’s focus while a technical stakeholder would also think, and advocate about investment in extensibility, supportability, and so on. Each stakeholder advocates for the attributes they better relate to. These pushes and pulls get more interesting when other stakeholders are advocating for attributes, not that clearly understood by others. For example compliance and legal matters (I generally put this into the external category). Keeping the balance Deciding on which attributes from different categories should be the core is very much context-dependent. For example, deciding between yield and harvest would result in deciding whether reliability or availability should have a higher degree of importance. I approach deciding about architectural attributes is based on a few practices: I Talk to all stakeholders to form a clear understanding of what the software system is going to do. Why is it being built, or rebuilt? How is it going to be used and who is going to be the user/s. I think about three attributes as the heart of a system based on the above. Any other attributes form around these over time, without diminishing them I associate a degree of importance with attributes contributing to architecture. These will be considered when the architecture is evolving or when trade-offs are being made for adding a new feature, etc. Iterate over these as an understanding of the system is building up and evolving the system. Deciding on the core attributes is like setting a strategy, decision being made around how the architecture could evolve is easier when you have some core attributes that cannot be diminished or ignored. Next to the core attributes, there are some considerations that deferring or ignoring has the highest cost of reconsideration. For example, testability and operateability are important enough to be next to the core attributes. If a software system is not testable and operatable it is going to rot over time and a re-write will ensue. Some examples to consider: For a regulated business attributes contributing to compliance are the top concerns. For example data integrity, audit-ability, access control, etc. Building anything, no matter how good, that is not compliant is unlikely to see the light of production. For a web-scale service the first items to consider are scalability, availability, and reliability, the rest of the attributes would form around these. Some systems may need very little attention to scalability but an extreme emphasis on safe operation, reliability, and upgradability. A medical device, a CNC machine, and so on. Deciding on any architectural attributes should be deferred as long as it is not causing indecision. When the same subject comes up multiple times without a conclusive way forward it means there is a lack of strategy, being for the product or the organization. I would put having a decision about the architectural attributes of a product to be part of the product and in a larger scope, the organization’s strategy. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:1:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Agile Team"],"content":"What I have observed with successful teams is the trust and belief in the team's mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams.","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Agile Team"],"content":"What I have learned about successful teams is the trust and the belief in the team’s mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams. When the group of individuals that are part of the said team cannot see a mission they trust, going through the famous steps of Forming, Storming, Norming, and Performing will be harder. The mission is what the team usually communicate and collaborate on. And effectively going through FSNP stages is possible with communication and collaboration. The communication and understanding that forms between team members is what makes the team reaching the performing stage and becoming what is called a high performing team. The concept of task oriented teams is floating around with different acronyms, one being the Quick Reaction Team, the miscellaneous teams, being project based team, fluid team, and so on. The whole concept behind it is a team that has no long history of having gone through the team formation and a long term strong mission to move toward to. It is usually working through a set of bugs and firefighting, or through disjoint projects that come into the pipeline. Setting up successful development teams is hard! It is not only because people are different but also because organizations vastly differ from one another and thus one recipe won’t work for everyone and everywhere. I tried setting up teams with a similar approach with some teams focused on long-term work that required more research and prototyping and a team dedicated to quicker gains for the system’s customers. The quicker gains include bugfix or features with smaller scopes and higher demands. The result was accumulated fatigue of never-ending context switching for the quick response team. Of course, there is always the gratifying feeling of what they have developed lands in so-called production but the feeling of accomplishment is not on par with the fatigue of the context switches. That was when I thought about rotating the team members between different teams with implications of time spent getting up to speed with the feature development and transfer of the knowledge context from the team member moving out of the feature development team and getting used to ways of working of the quick response team for the newly joined member. It is possible to have a pool of engineers working together on different areas of the same bounded context, as long as the members that are coming together have already gone through knowing one another as team. But to pull seemingly random engineers between different knowledge/context in a project based manner may result in developing a feature but it will not result in positive outcomes long term. ","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/:0:0","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Developer Experience"],"content":"When a new hire joins a team dealing with the development or sustaining of a large-scale application there are a few things that they would need to form a basic understanding of the system: Deeper understanding of what the business about Get a view of the system’s architecture for the software system and the deployment view. Get a bird’s eye view of the system’s dependencies. At least the immediate dependencies. History lessons, how the system came to be at the current state. Maybe ADRs would help with this. Learn about what are/were the push and pull resulted in the current state (Lack of maintenance, lack of test, long release cycles, well automated, etc.) Learn how to go through a development cycle of checking the code out, make a small fix/change, run the tests, go through the release cycle. In some organizations, these steps take months to complete. Or some of the above may not even be possible, because nobody knows the context of some decisions and some paths taken. It always pays off to make any onboarding as easy as it can get. Onboarding is a recurring cost like reading/changing. So if clean code matters, so should a clean onboarding. There are plenty of reasons for the first days of software developers starting a new job being less than ideal. From poor security onboarding to lack of a buddy to walk them through the day-to-day of work-life for the first few days. Details of that is a subject of another series of blog posts ","date":"2022-04-26","objectID":"/2022/04/onboarding-as-important-as-code-readablity/:0:0","tags":["Team","architecture","Onboarding","Documentation","Developer Experience"],"title":"Smooth Developer Onboarding is as Important as Code Readability","uri":"/2022/04/onboarding-as-important-as-code-readablity/"},{"categories":["Architecture"],"content":"During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the  architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it.","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Introduction During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it. Wikipedia has a good definition of software architecture that I quote below: Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations. The architecture of a software system is a metaphor, analogous to the architecture of a building. It functions as a blueprint for the system and the developing project, laying out the tasks necessary to be executed by the design teams. Software, data, IT, domain, and enterprise are going to have an architecture no matter if it is intentional, developed and nurtured and well documented. Or something that is grown out of what everyone involved in the system has done to arrive to without a record of why the architecture is what it is. Below I write about how and why documenting software architecture is not as successful as it should be and how can it possibly be improved. AD and ADR During the blog I refer to Architecture description as AD and Architecture Decision Record as ADR. I see adding the ADR as an ongoing effort while the AD is the overall architectural view of the system. The AD has higher level of abstraction compared to the ADRs which are focused for a particular decision. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture description infamy The primary reasons for the infamy of software architecture description, as far as I can say, is the failure in attracting different stakeholders to read or to develop architecture descriptions. There are plenty of archetypes of architectural descriptions targeting different groups of stakeholders. Here I will put the focus on the software architecture with the target audience being software engineers. Some reasons for architecture description usually is not the favorite topic of conversation are the followings: The architecture description is too generic and is not created for software developers consumption. The vocabulary is wrong, the addressed concerns may not be relevant and so on. It has too much irrelevant information that the software developers may not need. So the noise to signal ratio is too high for the document to be considered. The architecture description is not easy to access, and if it is; it is not easily readable because of the tooling that is used to create it. No uniform theme is used. Different architecture description within the same organisation are not following the same theme. The description is not up to date From the list above “keep it up to date” might be hard to imagine and the “theme” item may not be clear. So I will go into a bit of details for each. Keep architecture description up to date Keeping documentation up to date is hard, we all know it. And there are many advice on how to approach this. Some of these advice are applicable to any documentation. Keep the documentation short: Write as little and as targeted as possible. Avoid fluff, avoid write one document for all stakeholders approach. Have a responsible person: Each ADR and the AD itself should be owned by someone who is the sole responsible to keep track of and update them. If everyone is responsible nobody is. Make updating easy and streamlined: Keep the AD and ADR in the code repo, where the devs like to spend their time in. Make it easy to contribute to the AD/R: Anyone in the team/s should be able to open a PR or contribute to clarifying the AD/R Establish a uniform theme I know almost no software developer without a very carefully selected theme for the editors, terminal and IDE. So I’d expect the same emphasis of look and feel would apply on the documentation as well. When I say theme I am referring to the following aspects of the architecture documentation: Terminology: Refer to any one concept/artefact/etc. with one term throughout the org Framework: Use the same architecture model framework, if it is C4 Model or 4+1 Model or anything else, stay with the same model everywhere Tooling: There are 100s of diagraming tools, pick one and stay with it. The lighter it is the better chance of it being used. Stay away from heavy tools for day to day work. Choose a tool that works on all operating systems used in the organisation Common icons/glyphs: Develop common icons and glyphs for internal architectural concepts and artefacts; use standard icons, e.g. cloud vendor provided icons, or framework/tools provided icons and glyphs Use a standarar template: At application server organisation we had a architecture committee (AsArch) which had a OnePager template for architectural decisions/changes that needed to review. Everyone in the committee or any attendees knew what to expect to see in the architecture description. For each of the above line items I can write down a long blog post. But let me talk a little bit about the template as the container for rest of the items (a glyph, a diagram, a term or expression wont be used without it appearing on a page) ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:1","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture descriptor template Templates have poor reputation as anyone in larger enterprises associate them with a long document with unfamiliar terminology and vague questions. The architecture description OnePager template should not endup being vague or filled with questions. It is a description not a checklist, not a questionnaire. What about ADRs? One thing to mention before I go far with describing a template, I can say that template is not a farfetched concept when using ADRs. The ADRs follow a template, for example it can contain headers for context, assumptions, decision, consequences and status and the content for each header. I see ADR mostly suitable to document the on-going decisions. Such decisions, if they impact the bigger picture must result in an update in the one page. Think about the OnePager as a summary of all the ADRs. Usual heading in ADRs Some of the most common headings in an ADR are the following: Tittle: What is it that this decision talksa about Date: When was this decision made Context/Summary: Problem definition and solution context. This is setting the scene for rest of the description/decision to come Decision: What is decided in relation to the issue/context/question Status: proposed | rejected | accepted | deprecated |… Consequences: What changes in the system (Performance, testability, cohesion, isolation, etc.), what turn more complicated or easier. You may also see the following headings being mentioned: More heading in ADRs Deciders Assumptions Constraint Related Decisions But generally speaking, the template is a set of common sense headings encapsulating the discussion that has happened over a coffee, over an email, in a meeting, etc. in an easy to follow structure. The OnePager as a story Now, down to a OnePager. What I appreciated about the OnePager that we had was the clear headings and subheadings. There was no confusion about what is needed to be mentioned and in what order. Of course not all headers were necessary to have any content or to be present, but their presense in the OnePager was a guide so that the authors do not forget about adding some details. For example, if there was something to mention in relation to performance impact it would go under “Performance impacts” header. If there was an impact on the system security it would go under the security headring and so on. Reading one pager should feel like reading a story, same as reading a well written code imho, starting with an author and an overview, what is involved in the system and what is not (the scope) and down to details. It must be open to everyone in the organisation, anyone should be able to easily find it, read it, comment on it and could reach out to the author/s for any clarification. The OnePager, at any point in time, must reflect the current state of the approach taken to build the system. Depending on the scope and size of the system the architecture description may have a single a very high level representation of the system components or a more detailed approach. For a significant enough s system there wont be more than Context diagrams (if we assume C4 Model). For example mentioning the presence of a pipeline to deliver the code to a target deployment environment can be a component in the context and described in a single paragraph. Later on each of these high level components of the architecture will have their own OnePager going into the details. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:2","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Conclusion Write less Write where it can be accessed and changed. Stay consistent through the organisation Dont write a single document for all the role; write targeted documents for different roles Have clear ownership for the description ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:2:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":null,"content":"I work in software industry; and have been involved with software development since 2000. Delphi, .Net and J2SE back in the time, and now I am more focused on technical leadership, cloud, architecture, and getting organizations closer to realizing their potentials and goals with. My responsiblities are more in ensuring alignments, and with assisting organizations with tech strategy, architecture, adoption of new technologies and practices, planning and implementations when the area of impact is across organization boundaries. I like the thrill of innovating, building or rebuilding. I enjoy it because it is as giving existence to something that is not; being software engineering or small tinkering projects. I like outdoors, mountains, open land, I like water sports; and the prime of my activities is running, mountain running that is (I am not good at it). I like writing, authored a book and co-authored another a decade ago. Whenever I get the chance I read books ranging from sci-fi and fantasy to leadership and management; every now and then I binge a bunch of articles or blogs that I have bookmarked to go through. Recently I revived this blog (Removed about 110 posts that were outdated) to start writing again How to get in touch with me? If you wanted to talk to me about any of the blog post, or anything else for that matter, send me an email to my first name at kalali.blog. ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About me, Masoud Kalali","uri":"/about/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it.","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it. For example: Have you seen the plenty of use cases that every vendor in every segment of tech stack provide as part of the adoption or success story? How about the tutorial that shows how quickly and easily something as significant as a an observability solution can be set and be up and running? And many other medium that one way or another demo how easily a solution can be used/adopted. Existence of such case studies and tutorial is amazing contribution to the overall wealth of knowledge and understanding in the tech sphere, no doubt avbout it. Sometimes I see engineering team members speaking about adopting a new infrastructure component or a brand new CNCF project with estimations that are 1/20st of the realistic time that it would take to adopt such a solution into our infrastructure. Any of those can be an entry point for a team or organisation deciding to adopt something and then endup in the pink elephant. With the adoption takes longer and longer and at the end of the day the organisation endups with half baked integrations and a team or an individual who is in charge of maintaining something that is not ready for prime time but is in widespread use. If vendors and contributors that are writing quick tutorial can be more straight forward with the write-ups in relation to things like compliance, governance, automation, seamless integration, security, etc. that an organisation must take into account before thinking about adopting project/product XYZ. Over short term there might be fewer attempt at adopting new components, products and projects but in the long run there will be more success in the industry which hopefully make everyone happier. Consumer of tech products will be happier with bug free, and less breaches in the product they are using and the practitioners will be happier with better build and well thought IT ecosystem. On the other end of it, it may result in less experiments and thus less feedback to the product owners on how to improve or enhance different aspect of the product. From the rollout to the usability of it, it is a trade-off and a balance for sure. ","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/:0:0","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Platform Engineering"],"content":"Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Intro Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. Platform? If we divide the software development within an organization into two very broad categories, there is platform development, and platform products in one layer and and application development in another, which is usually consumes the platform. The applications are usually customer/end-user facing or directly facilitating a particular business use-case, for example a login routine or a batch job that runs every night to process schedule payments. Applications most often have a clear business goal and defining OKRs is easier and ROI can be observed within a shorter period of time. On the other hand, platform development which can be anything from shared components for different applications to release pipelines, development tools, or combination of many things together is not as straight forward. Platforms and platform teams are there to not only save initial cost of starting a new project or product but to save on the cost of maintenance and owning such product but introducing familiar concepts and ways of working across the vertical and horizontal integration points. Sometimes a platform/team is hard to justify, not easy to have any meaningful OKRs and usually have no immediate ROI. For example, If the platform team develops a pipeline and related libraries, the adoption of such pipeline takes time. After a reasonable number of applications have started using the said pipeline, product owner would slowly see an indirect ROI from: Better compliance baked into the pipeline Automation in release practices rather than throw over the wall and hand overs Uniform quality gates as pipeline would enforce some level conde analysis and apply some gates Faster time to production as it reduces the number of manual steps and communication costs Less code sediment as continues or even frequent deployment reduces the code that is sitting in the codebase without being exercised Less regressions because of frequent small releases etc. That is when the justification for existence of a platform become easier and the platform teams and products will become necessities. Why Platforms? Simply put platforms help with the following: Time saving as they prevent inventing the same thing again and again; e.g. a pipeline. Time saving as, when done right, they introduce a consistent way of working across diferent layers. Being applications, infrastructure or tooling. That will in turn makes transfer of knowledge and understanding easier Time saving as teams with expertise build re-usable components that other team can incorporate rather than every time having to acquire such expertise Governance, and compliance and other orthogonal aspects can be addressed via automation backing variety of self services. Let’s say that there an enterprise with the very well established process for software development, delivery, operation and support. This has been the case for the past couple of decades and individuals, teams, organisational units, and organisation as a whole developed a culture around the development process (for good or bad). The process can be something along the line of the following for simple features/functionalities. Imagine a handover between most steps. Long contemplations and many meetings of the domain architect/s Retirement gathering and certifying it with stakeholders Doing any risk assessment, compliance check in relation to data being processed and/or collected Planning (usually includes capacity planning and schedule as well as development timeline) Development and figuring o","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:0","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Setting up the expectations Being a product owner or an advocate for a platform requires more conversation and investigation about painpoints existing in the organization as a whole. Being the application/program development teams or the relationship between compliance/security/process divisions with the application development hierarchy. Setting clear expectations for external customers and goals for the platform team itself is an important task that the product owner, in combination with engineering teams (and management) should set. This expectation setting will make it clear to management on what timeline and outcome to expect for given organisational support, runway and budget. At the same time, a platform development team must make it clear to the management that having a platform is a tradeoff. For example if the current way of working is that each team entirely choose their guidelines and architectural patterns, a development platform may enforce certain principles and guidelines using automation as much as possible, so no exception and exemptions. An example can be use of certain libraries for logging, or certain pipeline for deployment, certain number of active engineers per application and so on. Platform adoption A development platform like any other product will need adoption and use to get the feedback cycle going. Let’s say that the platform teams understood the painpoints and the platform is developed to not only remedy the painpoints but also ensure the viability of the application ecosystem over the next decad. If the platform is not being used and has an established feedback cycle it will be hard for it to grow and cover the majority of use-cases or valid viewpoints. There are some general approach in spreading the adoption of the platform and starting a feedback cycle. Platform team owning applications One approach that I have employed, and seems to work for adoption of the platform, is ensuring the platform team owns some programs built on top of the platform. This service can be the example to showcase how platform works and how does it help with the pain-points that it exists to remedy and how does it propel the organisation into the new ways of working and tech landscape. Of course, platform owners developing applications has some drawbacks: The application/s may turn into a an unintended blueprint and others may follow something that is not fully baked yet. The platform team too far get into habit of optimising the platform for their own use/level of competence. The platform team get too far distracted by the application to pay attention to the platform itself. This may result in mental overload for the team and far too many context switching. Platform team helping an application team convert Another approach is locating a team that struggling the most and is vocal the most about the painpoints, of course within reason, and help them convert their service development to use the new platform. This would require multiple criteria: The service in question does not have too many dependencies that can impact the conversion The service team is willing to take the steps. The steps that the platform is advocating for. For example code review, use of pipeline, test automation and so on that is a service provided by the platform. The platform team has some understanding of the domain that the service team is driving. Combination of the above two One thing to consider in the combination or even in the second approach is to identify the players and influencers and get buy-in from them. In every organisation there are people who are sitting far behind the scene without any title with more influence on what different teams may adopt that any line managers. Convincing them will multiply the rate of adoption. Next In the next instalments of this series I will write more about each aspect of the development platform mentioned here. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:1","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Identity, something that we hear more often these days with the whole web 2.0 and social services and more and more web based public services growing around us. The identity notion is an integral part of a security system in distributed services. Developing effective software system require an effective security and access control system which java provides, not exactly in the way that it should be in 2011 but it does provide what is the bare bone necessity to develop applications and frameworks on top of it and benefit from its presence. The identity API is going to ease the interaction between the identity providers and those who consume the identity and trust the identity providers in addition to governing and managing the identity attributes. I was studying the JSR details and it seems to be covering everything required for the identity attributes governance and the required API for both ends of the usage including the client API the governing/producing API. The identity producing and consuming is not new and there are fair number of public identity producers like facebook, twitter, etc. and also products that system integrators can use like OpenAM as an open source product or gorilla Commercial software products like ORACLE identity management or IBM tivoli identity management software, etc. In a very simple set of words, the JSR 351: The Java Identity API will be as successful as it is going to be adopted. No adoption and it will endup dying some dark corner… Design a simple and elegant API and try to ship it with some free and easy to use service implementations and it may get some momentum, otherwise it will be a goner and people will stick with what they have. I like the new features that it is going to introduce in the decision making or authorization part but we should see how well it will be adopted by identity providers to develop the services that provides the interaction point between the JSR interface and their repositories. Pushing it as JSR wont really do that much without a wide adoption in the community. Look at how many implementation of the JSR 115 and JSR 196 exits to be plugged into application servers supporting the contract and you will get what I am referring to by community adoption. ","date":"2011-10-24","objectID":"/2011/10/some-thought-on-jsr-351-java-identity-api/:0:0","tags":["Java","JCP"],"title":"My thoughts on JSR 351, Java Identity API","uri":"/2011/10/some-thought-on-jsr-351-java-identity-api/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Well, as many of us already know Oracle submitted the JSR for Java EE 7 which is sort of an umbrella JSR for many update in already existing specifications, new versions of some JSRs and some completely new JSRs which will be part of the grand Java EE 7 - JSR 342. One of these JSRs is the JSR 343 which introduces a new version of JMS into the platform as an evolution of its previous version, JSR-914, and will unify the JMS usage with what added to the platform in the past 8 years. The following represent some very simple usecases of JMS in the enterprise while complex multiphase transactional usecases are not unusual when MDBs and XA data sources are involved. JMS API architecture JMS itself is for asynchronous communication and widely used to communicate some execution instructions from one node or point to another or a set of other points. For example long running queries can be queued using a JMS queue to get processed by another point in the system while the query client is not blocked for the query result. Or it can be used to communicate a common set of instructions to many interested parties which may or may not be around the communication happens, durable subscriptions and persisted topics. For example when clients need to get an ordered set of update messages to update a localcache when they get online after some times. Each client will get its own copy of messages it should receive when getting online. JMS API provides enough functionalities to realize most of our design out of the specification and the minor features and functionalities not included in the JSR while required by some designs are covered by the vendor specific pack of enhancement and tweaks provided in the broker level and through the vendor specific API. You may ask if the current JMS API provides all we need, why a new JSR should be put on the table, the answer mainly relies on the theme for Java EE 7 which is making Java EE more cloud friendly and sort of cloud enabled by nature rather than by product. The details of JMS 2.0 spec goals are listed at the JSR homepage but a brief list can be seen as follow: Community requested features and enhancements. Make the JSR more cloud friendly based on how Java EE 7 will define “to be cloud friendly” Cleanup of some ambiguity in the relation of JMS with other Java EE specs. Make the API easier to use, more annotations and more generics will be involved for the least of the things or maybe reducing number of boxes and lines in the aove figure could help many to start with the API faster. Make necessary changes to benefit from the JSR-299 or Contexts and Dependency Injection to easier and more unified use of API. In the follow up posts I will iterate over each one of these bullet points in more details. I am member of the JMS 2.0 expert group but this post or any other post in my personal blog does not reflect the expert group opinion or the opinion of my employer on the subject unless you could not see this paragraph at the end of the post :-). ","date":"2011-05-17","objectID":"/2011/05/jms2-jsr/:0:0","tags":["Java","JCP","JMS"],"title":"Brief overview of JSR 343: JavaTM Message Service 2.0","uri":"/2011/05/jms2-jsr/"},{"categories":["SysAdm","Old Blog Migrated Content"],"content":"SMF services are basically daemons staying in background and waiting for the requests which they should server, when the request come the daemon wake ups, serve the request and then wait for the next request to come. The services are building using software development platforms and languages but they have one common aspect which we are going to discuss here. The service manifests which describe the service for the SMF and let the SMF manage and understand the service life cycle. To write a service manifest we should be familiar with XML and the service manifest schema located at /usr/share/lib/xml/dtd/service_bundle.dtd.1. This file specifies what elements can be used for describing a service for the SMF. Next thing we need is a text editor and preferable an XML aware text editor. The Gnome gedit can do the task for us. The service manifest file composed of 6 important elements which are listed below: The service declaration: specifies the service name, type and instantiation model Zero or more dependency declaration elements: Specifies the service dependencies Lifecycle methods: Specifies the start, stop and refresh methods Property groups: Which property groups the service description has. Stability element: how stable the service interface is considering version changes Template element: more human readable information for the service. To describe a service, first thing we need to do is identifying the service itself. Following snippet shows how we can declare a service named jws. \u003cservice name='network/jws’ type='service' version='1'\u003e \u003csingle_instance/\u003e The first line specifies the service name, version and its type. The service name attribute forms the FMRI of the service which for this instance will be svc:/network/jws. In the second line we are telling SMF that it should only instantiate one instance of this service which will be svc:/network/jws:default. We can use the create_default_instance element to manipulate automatic creation of the default instance. All of the elements which we are going to mention in the following sections of this article are immediate child elements of the service element which itself is a child element of the service_bundle element. The next important element is dependency declaration element. We can have one or more of this element in our service manifest. \u003cdependency name='net-physica' grouping='require_all ' restart_on='none' type='service'\u003e \u003cservice_fmri value='svc:/network/physical'/\u003e \u003c/dependency\u003e Here we are telling that our service depends on the svc:/network/physical service and this service needs to be online before our service can start. Some of the values for the grouping attribute are as follow: The require_all which represent that all services marked with this grouping must be online before our service came online The require_any which represents that any of the services in this grouping suffice and our service can become online if one of them is online The optional_all presence of the services marked with this grouping is optional for our service. Our service works with or without them. The exclude_all: specifies the services which may have conflict with our service and we cannot become online in presence of them The next important elements are specifying how the SMF should start, stop and refresh the service. For these tasks we use three exec_method elements as follow: \u003cexec_method name='start' type='method' exec='/opt/jws/runner start' timeout_seconds='60'\u003e \u003c/exec_method\u003e This is the start method, SMF will invoke what the exec attribute specifies when it want to start the service. \u003cexec_method name='stop' type='method' exec=':kill' timeout_seconds='60'\u003e \u003c/exec_method\u003e The SMF will terminate the process it started for the service using a kill signal. By default it uses the SIGTERM but we can specify our own signal. For example we can use ‘kill -9’ or ‘kill -HUP’ or any other signal we find appropriate for our service termination. \u003cexec_method name=‘refresh’ type=‘method’ exe","date":"2011-01-31","objectID":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/:0:0","tags":["SMF","Solaris"],"title":"Writing Solaris Service Management Facility (SMF) service manifest","uri":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"In this article we will study how we can use the fmadm command to get the list of faulty components along with the detailed information about the fault. Before starting this article we should have a command console open and then we can proceed with using the fmadm command. The most basic form of using fmadm command is using its faulty subcommand as follow \\# fmadm faulty When we have no error in the system, this command will not show anything and exit normally but with a faulty component the output will be different, for example in the following sample we have a faulty ZFS pool because some of its underlying devices are missing. fmadm command output Starting from top we have: Identification record: This record consisting of the timestamp, a unique event ID, a message Id letting us know which knowledge repository article we can refer to for learning more about the problem and troubleshooting and finally the fault severity which can be Minor or Major. Fault Class: This field allows us to know what is the device hierarchy causing this fault Affects: tells us which component of our system is affected and how. In this instance some devices are missing and therefore the fault manager takes the Zpool out of service. Problem in: shows more details about the problem root. In this case the device id. Description: this field refer us to the a knowledge base article discussing this type of faults Response: Shows what action(s) were executed by fault manager to repair the problem. Impact: describe the effect of the fault on the overall system stability and the component itself Action: a quick tip on the next step administrator should follow to shoot the problem. This step is fully described in the knowledge base article we were referred in the description field. Following figure shows the output for proceeding with the suggested action. fmadm taking action As we can see the same article we were referred to, is mentioned here again. We can see that two of the three devices have failed and fpool had no replica for each of those failed device to replace them automatically. If we had a mirrored pool and one of the three devices were out of service, the system could automatically take corrective actions and replace continue working in a degraded status until we replace the faulty device. The fault management framework is a plug-able framework consisting of diagnosis engines and subsystem agents. Agents and diagnosis engine contains the logic for assessing the problem, performing corrective actions if possible and filing the relevant fault record into the fault database. To see a list of agents and engines plugged into the fault management framework we can use config subcommand of the fmadm command. Following figure shows the output for this command. fmadm configuration As we can see in the figure, there are two engines deployed with OpenSolaris, eft and the zfs-diagnosis. The eft, standing for Eversholt Fault Diagnosis Language, is responsible for assessing and analyzing hardware faults while the zfs-diagnosis is a ZFS specific engine which analyzes and diagnoses ZFS problems. The fmadm is a powerful utility we which can perform much more than what we discussed. Here we can see few other tasks we can perform using the fmadm. We can use the repaired subcommand of the fmadm utility to notify the FMD about a fault being repaired so it changes the component status and allows it to get enabled and utilized. For example to notify the FMD about repairing the missing underlying device of the ZFS pool we can use the following command. \\# fmadm repaired zfs://pool=fpool/vdev=7f8fb1c77433c183 We can rotate the log files created by the FMD when we want to keep a log file in a specific status or when we want to have a fresh log using the rotate subcommand as shown below. \\# fmadm rotate errlog | fltlog The fltlog and errlog are two log files residing in the /var/fm/fmd/ directory storing all event information regarding faults and the errors causing them. To","date":"2011-01-12","objectID":"/2011/01/solaris-fault-administration-using-fmadm-command/:0:0","tags":["Solaris","ZFS"],"title":"Solaris fault administration using fmadm command","uri":"/2011/01/solaris-fault-administration-using-fmadm-command/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Performance, performance, performance; this is what we hear in all software development and management sessions. ZFS provides few utility commands to monitor one or more pools’ performance. You should remember that we used fsstat command to monitor the UFS performance metrics. We can use iostat subcommand of the zpool command to monitor the performance metrics of ZFS pools. The iostat subcommand provides some options and arguments which we can see in its syntax shown below: iostat \\[-v\\] \\[pool\\] ... \\[interval \\[count\\]\\] The -v option will show detailed performance metrics about the pools it monitor including the underlying devices. We can pass as many pools as we want to monitor or we can omit passing a pool name so the command shows performance metrics of all commands. The interval and count specifies how many times we want the sampling to happen what is the interval between each subsequent sampling. For example we can use the following command to view detailed performance metrics of fpool for 100 times in 5 seconds interval we can use the following command. \\# zpool iostat -v fpool 5 100 The output for this command is shown in the following figure. Monitor ZFS pool performance The first row shows the entire pool capacity stats including how much space were used upon the sampling and how much was available. The second row shows how many reads and writes operations performed during the interval time and finally the last column shows the band width used for reading from this pools and writing into the pool. The zpool iostat retrieve some of its information from the read-only attributes of the requested pools and the system metadata and calculate some other outputs by collecting sample information on each interval. ","date":"2010-12-30","objectID":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/:0:0","tags":["Solaris","ZFS"],"title":"Monitoring ZFS pools performance using zpool iostat","uri":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. We are going to add a logical interface to e1000g1 interface with a 10.0.2.24 as its static IP address. Before doing so let’s see what network interface we have using the ifconfig command. 3180_04_12 Now to add the logical interface we only need to execute the following command: \\# ifconfig e1000g1 addif 10.0.2.24/24 up Invoking this command performs the following tasks: Create a logical interface named e1000g1:1. The naming schema for logical interfaces conforms with interface_name:logical_interface_number which the number element can be from 1 to 4096. Assign 10.0.2.24/24 as its IP address, net mask and broadcast address. Now if we invoke ifconfing -a command the output should contain the logical interface status as well. The following figure shows a fragment of ifconfig -a command. 3180_04_13 Operating system does not retain this configuration over a system reboot and to make the configuration persistent we need to make some changes in the interface configuration file. For example to make the configuration we applied in this recipe persistent the content of /etc/opensolaris.e1000g1 should something similar to the following snippet. 10.0.2.23/24 addif 10.0.2.24/24 The first line as we discussed in recipe 3 of this chapter assign the given address to this interface and the second like adds the logical interface with the given address to this interface. To remove a logical interface we can simply un-plumb it using the ifconfig command as shown below. \\# ifconfig e1000g1:1 unplumb When we create a logical interface, OpenSolaris register that interface in the network and any packet received by the interface will be delivered to the same stack that handles the underlying physical interface. Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. ","date":"2010-12-09","objectID":"/2010/12/managing-logical-network-interfaces-in-solaris/:0:0","tags":["Solaris","Networking"],"title":"Managing Logical network interfaces in Solaris","uri":"/2010/12/managing-logical-network-interfaces-in-solaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Link aggregation or commonly known Ethernet bonding allows us to enhance the network availability and performance by combining multiple network interfaces together and form an aggregation of those interfaces which act as a single network interface with greatly enhanced availability and performance. When we aggregate two or more network interfaces, we are forming a new network interface on top of those physical interfaces combined in the link layer. We need to have at least two network interfaces in our machine to create a link aggregation. The interfaces must be unplumb-ed in order for us to use them in a link aggregation. Following command shows how to unplumb a network interface. \\# ifconfig e1000g1 down unplumb We should disable the NWAM because link aggregation and NWAM cannot co-exist. \\# svcadm disable network/physical:nwam The interfaces we want to use in a link aggregation must not be part of virtual interface; otherwise it will not be possible to create the aggregation. To ensure that an interface is not part of a virtual interface checks the output for the following command. \\# dladm show-link Following figure shows that my e1000g0 has a virtual interface on top of it so it cannot be used in an aggregation. 3180_04_14 To delete the virtual interface we can use the dladm command as follow \\# dladm delete-vlan vlan0 The link aggregation as the name suggests works in the link layer and therefore we will use dladm command to make the necessary configurations. We use create-aggr subcommand of dladm command with the following syntax to create aggregation links. dladm create-aggr \\[-l interface_name\\]\\* aggregation_name In this syntax we should have at least two occurrence of -l interface_name option followed by the aggregation name. Assuming that we have e1000g0 and e1000g1 in our disposal following commands configure an aggregation link on top of them. \\# dladm create-aggr -l e1000g0 -l e1000g1 aggr0 Now that the aggregation is created we can configure its IP allocation in the same way that we configure a physical or virtual network interface. Following command plumb the aggr0 interface, assign a static IP address to it and bring the interface up. \\# ifconfig aggr0 plumb 10.0.2.25/24 up Now we can use ifconfig command to see status of our new aggregated interface. \\# ifconfig aggr0 The result of the above command should be similar to the following figure. 3180_04_15 To get a list of all available network interfaces either virtual or physical we can use the dladm command as follow \\# dladm show-link And to get a list of aggregated interfaces we can use another subcommand of dladm as follow. \\# dladm show-aggr The output for previous dladm commands is shown in the following figure. 3180_04_16 We can change an aggregation link underlying interfaces by adding an interface to the aggregation or removing one from the aggregation using add-aggr and remove-aggr subcommands of dladm command. For example: \\# dladm add-aggr -l e1000g2 aggr0 \\# dladm remove-aggr -l e1000g1 aggr0 The aggregation we created will survive the reboot but our ifconfig configuration will not survive a reboot unless we persist it using the interface configuration files. To make the aggregation IP configuration persistent we just need to add create /etc/hostname.aggr0 file with the following content: 10.0.2.25/24 The interface configuration files are discussed in recipe 2 and 3 of this chapter in great details. Reviewing them is recommended. To delete an aggregation we can use delete-aggr subcommand of dladm command. For example to delete aggr0 we can use the following commands. \\# ifconfig aggr0 down unplumb \\# dladm delete-aggr aggr0 As you can see before we could delete the aggregation we should bring down its interface and unplumb it. In recipe 11 we discussed IPMP which allows us to have high availability by grouping network interfaces and when required automatically failing over the IP address of any failed interface to a healthy one. In this recipe we ","date":"2010-11-25","objectID":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/:0:0","tags":["Solaris","Networking"],"title":"Configuring Solaris Link Aggregation (Ethernet bonding)","uri":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will look at how the directory tree walker and the directory stream reader works. These two features are another couple of long requested features which was not included in the core java before Java 7. First, lets see what directory stream reader is, this API allows us to filter content of a directory on the file system and extract the file names that matches our filter criteria. The feature works for very large folders with thousands of files. For filtration we can use PathMatcher expression matching the file name or we can filter the directory content based on the different file attributes. for example based on the file permissions or the file size. Following sample code shows how to use the DirectoryStream along with filtering. For using the PathMatcher expression we can just use another overload of the newDirectoryStream method which accepts the PathMatcher expression instead of the filter. public class DirectoryStream2 { public static void main(String args\\[\\]) throws IOException { //Getting default file system and getting a path FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/usr/bin\"); //creating a directory streamer filter DirectoryStream.Filter filter = new DirectoryStream.Filter () { public boolean accept(Path file) throws IOException { long size = Attributes.readBasicFileAttributes(file).size(); String perm = PosixFilePermissions.toString(Attributes.readPosixFileAttributes(file).permissions()); if (size \u003e 8192L \u0026\u0026 perm.equalsIgnoreCase(\"rwxr-xr-x\")) { return true; } return false; } }; // creating a directory streamer with the newly developed filter DirectoryStream ds = p.newDirectoryStream(filter); Iterator it = ds.iterator(); while (it.hasNext()) { Path pp = it.next(); System.out.println(pp.getName()); } } } The above code is self explaining and I will not explain it any further than the in-line comments. Next subject of this entry is directory tree walking or basically file visitor API. This API allows us to walk over a file system tree and execute any operation we need over the files we found. The good news is that we can scan down to any depth we require using the provided API. With the directory tree walking API, the Java core allows us to register a vaster class with the directory tree walking API and then for each entry the API come across, either a file or a folder, it calls our visitor methods. So the first thing we need is a visitor to register it with the tree walker. Following snippet shows a simple visitor which only prints the file type using the Files.probeContentType() method. class visit extends SimpleFileVisitor { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) { try { System.out.println(Files.probeContentType(file)); } catch (IOException ex) { Logger.getLogger(visit.class.getName()).log(Level.SEVERE, null, ex); } return super.visitFile(file, attrs); } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) { return super.postVisitDirectory(dir, exc); } @Override public FileVisitResult preVisitDirectory(Path dir) { return super.preVisitDirectory(dir); } @Override public FileVisitResult preVisitDirectoryFailed(Path dir, IOException exc) { return super.preVisitDirectoryFailed(dir, exc); } @Override public FileVisitResult visitFileFailed(Path file, IOException exc) { return super.visitFileFailed(file, exc); } } As you can see we extended the SimpleFileVisitor and we have visitor methods for all possible cases. Now that we have the visitor class, the rest of the code is straight forward. following sample shows how to walk over _/home/_masoud directory down to two levels. public class FileVisitor { public static void main(String args\\[\\]) { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud\"); visit v = new visit(); Files.walkFileTree(p, EnumSet.allOf(FileVisitOption.class), 2, v); } } You can grab the latest version of Java 7 aka Dolphin from here and from the same page you can grab the late","date":"2010-08-23","objectID":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 6: Filtering directory content and walking over a file tree","uri":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"For long time Java developers used in-house developed solutions to monitor the file system for changes. Some developed general purpose libraries to ease the task of others who deal with the same requirement. Commercial and free/ open source libraries like http://jnotify.sourceforge.net/, http://jpathwatch.wordpress.com/ and http://www.teamdev.com/jxfilewatcher among others. Java 7 comes with NIO.2 or JSR 203 which provides native file system watch service. The watch service provided in Java 7 uses the underlying file system functionalities to watch the file system for changes, so if we are running on Windows, MacOS or Linux… we are sure that the watch service is not imposing polling overhead on our application because the underlying OS and file system provides the required functionalities to allow Java to register for receiving notification on file system changes. If the underlying file system does not provide the watch-ability, which I doubt it for any mainstream file system, Java will fall back to some rudimentary polling mechanism to keep the code working but the performance will degrade. From the mentioned libraries the jpathwatch API is the same as the Java 7 APIs to make it easier to migrate an IO based application from older version of Java to Java 7 when its time arrives. The whole story starts with WatchService which we register our interest in watching a path using it. The WatchService itself is an interface with several implementatins for different file system and operating systems. We have four class to work with when we are developing a system with file system watch capability. A Watchable: A watchable is an object of a class implementing the Watchable interface. In our case this is the Path class which is the one of the central classes in the NIO.2 A set of event types: We use it to specify which types of events we are interested in. For example whether we want to receive creation, deletion, … events. In our case we will use StandardWatchEventKind which implements the WatchEvent.Kind. An event modifier: An event modifier that qualifies how a Watchable is registered with a WatchService. In our case we will deal with nothing specific up to now as there is no implementation of this interface included in the JDK distribution. The Wacher: This is the watcher who watch some watchable. In our case the watcher watches the File System for changes. The abstract class is java.nio.file.WatchService but we will be using the FileSystem object to create a watcher for the File System. Now that we know the basics, let’s see how a complete sample will look like and then we will break down the sample into different parts and discuss them one by one. import java.io.IOException; import java.nio.file.FileSystem; import java.nio.file.FileSystems; import java.nio.file.Path; import java.nio.file.StandardWatchEventKind; import java.nio.file.WatchEvent; import java.nio.file.WatchKey; import java.nio.file.WatchService; import java.util.List; import java.util.logging.Level; import java.util.logging.Logger; public class WatchSer { public static void main(String args\\[\\]) throws InterruptedException { try { FileSystem fs = FileSystems.getDefault(); WatchService ws = null; try { ws = fs.newWatchService(); } catch (IOException ex) { Logger.getLogger(WatchSer.class.getName()).log(Level.SEVERE, null, ex); } Path path = fs.getPath(\"/home/masoud/Pictures\"); path.register(ws, StandardWatchEventKind.ENTRY_CREATE, StandardWatchEventKind.ENTRY_MODIFY, StandardWatchEventKind.OVERFLOW, StandardWatchEventKind.ENTRY_DELETE); WatchKey k = ws.take(); List\u003e events = k.pollEvents(); for (WatchEvent object : events) { if (object.kind() == StandardWatchEventKind.ENTRY_MODIFY) { System.out.println(\"Modify: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_DELETE) { System.out.println(\"Delete: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_CREATE) { System.out.println(\"Created: \" + object.co","date":"2010-08-10","objectID":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 5: Watch Service and Change Notification","uri":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What is a file system File systems make it possible to store and retrieve files and containing data into storages like hard disks, optical disks and other types of storages. OpenSolaris support both legacy file systems like UNIX File System (UFS) and its own file system called Zettabyte File System (ZFS). Following figure show how UFS and other legacy file systems work. As you can see we can partition each storage device into one or more volumes with different file systems. For example one hard disk may have an NTFS volume, a UFS volume and an EXT4 volume. In contrast with legacy file systems, ZFS uses concept of resource pooling. In ZFS one or more devices, volumes or files can be combined to form a pool of storages, hence called zpool. Then file systems are built on top of this pool. For example we can create a file system capable of storing a 32 TB file on top of a zpool created on using 16 hard disks, 2 TB each. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:1:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What OpenSolaris supported file systems are OpenSolaris as a pioneer OS for enterprises supports several file systems in addition to pushing its flagship and unique file system named ZFS. Each supported file system is either supported because of backward compatibility or to provide the administrators with a wider range of choices when it come to formatting the storages. The UNIX file system (UFS) is the common file system on UNIX like operating systems. The Personal Computer File System (PCFS) supports FAT12, FAT16 and FAT32 formatted disks. The High Sierra File System (HSFS) allows us to access files on High Sierra or ISO 9660 format CD-ROM disks. The Temporary File Storage Facility (TMPFS) provides a temporary file system in RAM or SWAP space, we can access the file system as a normal file system but in the background it is stored in memory instead of persisted storage. The integration of Storage and Archive Manager and Quick File System (SAM-QFS) is provided so we can have access to a clustered archiving solution in OpenSolaris. ZFS is the flagship file system of OpenSolaris. The file system is a combination of file system and logical volume manager. Some of the ZFS highly steamed features are discussed below. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:2:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"ZFS unique features ZFS is a unique file system which addresses all concerns in the enterprise data centers. We will discuss these features in this section. Storage pools ZFS file systems are built on top of storage pools which are built on top of a group of virtual devices. A virtual device can vary from an entire hard drive down to a file. Capacity ZFS is a 128-bit file system which can store 1.84 × 10^19^ times more data than current 64-bit systems. Copy-on-write transactional model No data get overwritten when updated and instead new data will be wrote in separate blocks and then the pointers pointing to old data will be changed to point to new blocks. Multiple write operation can be grouped together to enhance the performance. Snapshots ZFS does not overwrite the data when they are changed; this feature is used to keep snapshots of file system for later user. Clones A clone is a writeable snapshot which its block content changes when a block is changed in another clone. Deduplication Deduplication is a file system level compression method in which redundant data is eliminated. For example in a typical enterprise mail server one email attachment may find its way into hundreds of users mail boxes, Deduplication will store the actual file once and all other instance of the file are referenced to the same actual file. We use a console to study all commands discussed in this chapter. So before we can execute any of these commands we should be in terminal environment. To activate the terminal environment we can: Hit ALT+F2 to summon the run window and then type console to open a console window Press ALT+CTRL+F2 to switch the workspace to another terminal and use it to practice the command. 1.Using basic file system commands OpenSolaris supports multiple file systems but to access all of them we need to know how to use some basic commands. Following table includes these basic commands along with their descriptions and samples showing how we can use them. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:2:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will study some of the very basic file system commands in this recipe. The commands need to be invoked in the terminal window. To enter the terminal window Hit ALT+F2 to summon the run window and then type console to open a console window. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:3:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We have some basic commands which we should know to deal with the file system in a console. Some of these commands are introduced in the following table. Command Sample cd We can change the current directory using cd command. For example : cd /home/masoud: switch the current directory to /home/masoud cd ..: switch one level up from the current directory. ls We can use this command to get a list of directories and files. It works similar to ls in Linux or dir in MS-DOS. For example: ls: shows the list of all files and directories except hidden ones. ls -al: shows the list of all files and directories along with detailed permissions and creation dates. ls -al /home: shows content of the given path as explained above. mkdir We can create new directories using mkdir command anywhere we have write permission. For example: mkdir safe: will create a directory named safe inside the current directory. mkdir -p /home/masoud/safe/personal/cash: When using -p it creates all directories required to form the mentioned path. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:4:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. All commands introduced in this recipe rely on the metada stored for the file system to perform its job. For example the ls command checks the metadata available for the path in question to show content of the given path. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:5:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can get a short or long help for any command in OpenSolaris using the commands help messages or the provided man pages. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:6:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing the help message for a command We can pass --help parameter to almost any command in OpenSolaris to see a short help message including usage syntax and list of options along with their descriptions. For example: \\# mkdir \\--help ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:6:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing extensive manual of a command We can see complete and extensive manual pages for any command in OpenSolaris by passing the command name to man command. For example to view the man pages for mkdir we can use the following command: \\# man mkdir ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:6:2","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Some other commands to review Following list includes some other commands which we need to know. Command to copy files and directories: cp Command to delete file and directories: rm Command to move or rename files and directories: mv 2. Formatting disks In this task we will learn how we can format and label disks prior to creating a file system on them. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:6:3","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready When we attach a disk to an OpenSolaris machine OpenSolaris will assign a name to disk and its partitions or slices. All disks have assigned names under /dev/dsk and /dev/rdsk which lets us access the disk at block level in the first name and at bye level using the second name. The byte level access method is also called raw access method. The disks names are usually compatible with C#T#D#S# or C#T#D#P#. The C stands for controller and the number part is a hex number like 4 or F4. A SATA connector on the mother board is sample of a controller. The T which is optional stands for target, the D stands for disk and the number after it points to one of the disks attached to the controller and the digit after S or P point to the slice number or the partition number. If the partition is from Solaris type we call it slice and its identifier is S# while if the partition is non-Solaris it will be identified with the P# ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:7:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. We can use format and fdisk commands to format and label the disks prior to using them. We will discuss format command as it also includes fdisk. To enter into the format command shell we should invoke format command in a console which will open the format shell. The initial screen as we can see in the following figure shows currently attached disks and allows us to select which disk we want to operate on. I have two hard disks connected to SATA controller and one to IDE controller. We can enter 1 and the list of available commands appears which is as shown in the following figure: The partition, format and fdisk are most common operations which we usually use. Typing each one of them opens another set of options which we can choose to commence with the operation. Instead of typing the complete command name, we can just enter its first character. For example we can quit from format shell to operating system shell or get back to format shell from its subcommands using q instead of quit. If we select a brand new disk in the format menu, the format command informs us that the disk has no partition and we need to use fdisk command to create some partitions before proceeding. To use fdisk we just need to enter fdisk subcommand and create partitions as we need or accept the default partitioning which fdisk command suggest. The default partitioning will create one Solaris2 partition occupying the entire disk. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:8:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a Solaris2 partition We can create a Solaris 2 partition using the following steps in format shell: Enter disk command and then select 1 to operate on c9t0d0 Enter fdisk command and when it asked whether we want to use the default partition select y so it create one Solaris2 partition occupying the whole disk. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:8:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a FAT32 partition Now let's create a FAT32 partition in the second disk using the following steps in the format shell: Enter disk command and then select 2 to operate on c9t1d0 Enter fdisk command and when it asked whether we want to use the default partition select n so it will enter the fdisk subcommand shell. When asked for selection enter 1 to create a new partitioning When asked for the partition type enter C to create a FAT32 partition When asked for size, enter 100 to use the whole disk capacity Enter y to activate the partition Now that the partition creation is complete enter 5 to apply the changes and exit fdisk shell. Now quit the format shell by entering q command. After creating the partitions we need to apply a file system on them before actually using them. To apply the file system on Solaris slice we can use newfs command as follow. \\# newfs /dev/rdsk/c9t0d0s2 And to apply the file system on the FAT32 partition we can use the mkfs with some additional parameters as follow: \\# mkfs -F pcfs -o fat=32 /dev/rdsk/c9t1d0p0:c ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:8:2","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Formatting removable disks. We can use format and fdisk commands to partition a removable media like a memory card or a thumb drive. For more information and detailed instructions check http://docs.sun.com/app/docs/doc/817-5093/gafmj?a=view ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:8:3","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Growing a UFS We can use growfs command to change a UFS size provided that when creating the file system we use the -T option with newfs command. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:8:4","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we create a partition in a disk, the layout we specified will be stored in the partition table area of the hard disk located in the beginning of the disk. The partition table contains all information like partition type, and its size which we specify when we create the partition. A good place to learn more about partitioning in general is http://en.wikipedia.org/wiki/Disk_partitioning ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:9:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next recipe we will learn how we can mount and put the partitions we created here in use. 3. Mounting and unmounting file systems Before we can use a file system we should mount the slices or partitions we created in the operating system. When we mount a partition or a slice we are making it accessible trough a directory which it is mounted to and we unmount the file system we are just detaching the file system from OS. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:10:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In previous recipes we create two partitions and applied a file system on each one of them. In this recipe we will mount them to some directories and start using them. To mount a file system we need a mount point. A mount point is a directory which we use it to access the mounted file system. So let’s create the mount point before mounting the partitions. \\# mkdir /mnt/fat /mnt/sol This command creates two directories named fat and sol inside the /mnt directory. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:11:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To mount the Solaris2 partition we created in previous recipe we can use the mount command as follow: \\# mount /dev/dsk/c9t0d0s2 /mnt/sol This command mounts the Solaris2 slice to /mnt/sol and we can create files and directories inside it using its mount point. For example: \\# mkdir /mnt/sol/sample If we get the list of /mnt/sol/ we will see something like the following figure. Mounting the FAT32 partition is a bit trickier than Solaris2 partition as you can see in the command below which mounts the FAT32 partition we created before to /mnt/fat. \\# mount -F pcfs /dev/dsk/c9t1d0p1 /mnt/fat As you can see we are passing the file system using -F parameter to let the command know which file system is applied on the partition. Using the following command we can create a text file containing “Hello FAT32 Partition” inside the FAT32 partition. \\# echo Hello FAT32 Partition \\\u003e /mnt/fat/sample.txt We can check partition content using the ls command as we used for Solaris2 partition. When we no longer need the file system or we want to take it down for other operations like checking the file system or moving the drive we can use umount command to detach the file system from the operating system. The umount command operates both on the device name like /dev/dsk/c9t1d0p1 and on the mount point like /mnt/fat. For example to unmount the FAT32 partition we can use the following command \\# umount /mnt/fat We can not use the umount or mount command when the file system is busy, for example when an application like terminal is accessing the file system or just has a path inside that file system open. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:12:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we mount a partition, we are asking the operating system’s file system to allow us access the content of the mounted partition through the directory we specified as the mount point. Operating system forward any changes we made to that mount point into the underlying partition ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:13:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Mounted file systems using mount command stays mounted for the period of the current operating system session and will not persist the system reboot and after each reboot we will need to mount them again. We can define the mount points in the mount table inside the /etc/vfstab file to let OpenSolaris pick them an execute them. Each row of the mount table represents one mount command. The /etc/vfstab is a plain text file with the following format. device device mount FS fsck mount mount to mount to fsck point type pass at boot options An example row for the mount table to mount our FAT32 partition is as follow: /dev/dsk/c9t1d0p1 /dev/rdsk/c9t1d0p1 /mnt/fat pcfs 5 yes - Let's see description of each field in the sample row of mount table. The block device name to mount. The raw device name to be used by fsck to check the disk. The mount point to mount the device. The file system type to mount the device under it. Whether the file system should be checked at startup or not. Whether to mount the device at boot or not. Additional options for mounting. Options like enforcing a read-only mount, enabling logging and so on. We applied no option. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:14:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Using fuser to see who is accessing a file system We can use fuser command to check which process is accessing the file system. This command comes handy when we want to unmount a device and umount throw us an error saying the device is busy. For example to check which process is using the /mnt/fat we can use the following command. \\# fuser /dev/dsk/c9t1d0p1 The above command results in something similar to the following figure if we have, for example a terminal window at /mnt/fat we will discuss fuser and pfiles, which gives us more details about who is using what part of the file system, in more details in chapter. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:14:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Mount options Following table shows important options we can use with mount command or in the mount table. Options Description nologging Disable the metadata operation logging. noatime Disable recording files access time. quota Enable disk space usage quotas. forcedirectio Force I/O requests to bypass the file system cache and therefore the operation can be faster for application with internal caches like Oracle and MySQL databases ## See also More information about the mount and umount commands is available in their man pages. And for the mount table we can refer to its official document at http://docs.sun.com/app/docs/doc/805-7228/6j6q7uev3?a=view 4. Monitoring UFS Monitoring is an integral part of all enterprise level activities and software and hardware systems are not an exception. OpenSolaris provide several utilities for monitoring the file system activities. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:14:2","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Following three commands are provided to monitor and check status of the file system. The df command report file system disk space usage. The du command summarizes disk usage of each directory recursively including the size of included files. The fsstat command reports kernel file operation activity by the file system type or by the mount point. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:15:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To view the amount of disk usage and free space on each disk (mount point) we can use the following command: \\# df -hl The result of using the command is shown in the following figure. To view how much space each file and directory occupied we can use the du command. For example to see how much space is used by /export/home/masoud/Documents and its subdirectories and files we can use the following command. \\# du -ah /export/home/masoud/Documents/ The command result is something like the following figure. We can use -b parameter to make the command show the size in bytes instead of wrapping it based on the block size. Another useful parameter is -s which summarizes the space consumption for the directory. Another monitoring command which we will discuss is fsstate which can show live statistics about file system usage. For example to see default monitoring factors for / mount point we can use the following command. \\# fsstat / 5 200 This command shows the statistics for 200 times and gathers the sampling data each 5 second. A sample output is like the following figure. In the resulting statistics we can see how many new files are created in the sample period, how many calls for getting and setting attributes placed, how many look-up, read, write, etc operations are performed. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:16:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. Commands that collect and shows statistics about the file systems uses multiple sources for the statistics they show including the operating system internal messages, the file system read-only properties and calculation they make to provide the users with sensible information. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:17:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. There are plenty of options which we can use to customise the result of each monitoring command. To available options and their usages are included in the man pages and short help of each command. There is another monitoring command we can use for monitoring IO operations called iostat. More information about iostat is available in its man pages. 5. Backing up and restoring UFS In previous recipe we saw how we can create, mount and monitor UFS partitions and file systems. In this recipe we will learn how to backup, and restore a UFS. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:18:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The command we use to create a backup is ufsdump and ufsrestore. These two commands can create a dump of a UFS like /mnt/sol and then restore it when required. We should backup a file system when it is not mounted or mounted in read-only mode to ensure that during the dump operation no file has changed. Prior to starting this recipe I copied some files into the UFS slice to make the backup and restore operations more sensible. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:19:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The ufsdump create backup of the file system. The dump can be an incremental dump only containing files changed after the previous dump or it can be a full dump containing all files. We can dump the file system into tapes or we can dump it into a binary file understandable for restore command. Following syntax shows how we can use ufsdump command. ufsdump \\[options\\] \\[list of arguments\\] files_to_dump In this command schema: The options are a single string of one-letter ufsdump options, some of important options are listed in this recipe table. The arguments may be multiple strings whose association with the options is determined by order. That is, the first argument goes with the first option that takes an argument and so on. An example of arguments is the -f parameter argument which specifies where to dump into. The files_to_dump is required and must be the last argument on the command line. For example we can create a full dump of a Solaris2 file system located on /dev/dsk/c9t0d0s2 and store it the FAT32 partition we mounted in /mnt/fat using the following command. \\# ufsdump 0fu /mnt/fat/dumped_here /dev/dsk/c9t0d0s2 The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Following figure shows the sample output of this command. Important list of options is shown in the following table. Option Description 0-9 Specify the dump level. Level 0 means a full dump while other numbers means incremental dump after the previous level. For example if we have a dump level 1 created on Friday, a dump level 1 on Monday will dump all changes happened between Friday and Monday. f Specify the file which we want to dump the dump files into it. This option need a argument in the argument list u Update the dump record. Add an entry to the file /etc/dumpdates, for each file system successfully dumped. v Verify the dump after created to ensure its integrity. c Set the defaults for cartridge instead of the standard half-inch reel L Sets the tape label to string, instead of the default value which is none. This option needs a string argument in the arguments list. The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Now to restore the dump we have just created we can use the ufsrestore command. The ufsrestore command works in interactive and headless mode. In the interactive mode it walk us through the procedure step by step and let us view a dump file content and select what to restore. An example of using the ufsrestore in interactive more is shown in the following figure. As you can see in the figure we can use some common commands like ls, add, and extract in the ufsrestore shell to get the content list, add our selected files to list of extractees and then extract them into the current working directory using the extract command of the ufsrestore shell. The following table explains the options used with the ufsrestore command above. To use the ufsrestore in the headless mode we should use the following syntax. ufsrestore \\[options\\] \\[space separated arguments\\] what_to_restore In this schema we have: The dump_file argument is path to the file we want to restore a file system content using it, for example /mnt/fat/dump_fileImportant. The what_to_restore argument is a path pointing to a directory or a file inside the dump file. The important options available to the ufsrestore command are included in the following table. Option Description i Enter the interactive shell of the ufsrestore command f Path to the dump file. The option need the file name to be passed in the argument list x Extract the named files into the current directory. For example if the named file is directory, the ufsrestore will restore that directory and its contents into the current working directory. r Extract the dump content recursively into the current working directory. Current working directory is where we are executing the command from. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:20:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other alternatives to ufsdump and ufsrestore There are some file system independent commands which we can use to create backup of a file system and then restore it. Some of these commands are as follow. The ufsdump command has its advantages like creating incremental backup, support for reels and so on over these commands. The tar command is a no compression achiever which can archive a file system. The cpio command similar to tar but with capabilities like sending the archive stream to a remove machine and support for different archive formats. The pax command is a combination of tar and cpio capabilities. The dd command is a low level coping command. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:20:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating snapshot of a UFS We can use fssnap to create snapshot of a UFS. The created snapshot is read-only and won’t sustain a restart. We usually use these snapshots for creating backup of the mounted file systems. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:20:2","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. The ufsdump performs the dumping task by scanning the requested file system path and creating a list of its content and writing the list into the specified media. Then it goes through the list and read each file addressed in the list and store its content in the specified media. Similarly the ufsrestore read the list, creates the directory layout and write the file contents into the media specified as restore target. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:21:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In addition to man pages we can learn more the ufsdump and ufsrestore commands in their reference documents available at http://docs.sun.com/app/docs/doc/817-3814/6mjcp0r5h?a=view 6. Checking UFS for errors UFS like any other file system can get corrupted in the block level. We can find the UFS corruptions and to some extend recover from those corruptions or prevent them from happening. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:22:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will mostly focus on using fsck command to check a disk and repair the damage if possible. I assume that we have some UFS partition like /dev/rdsk/c9t0d0s2 in our disposal to study on. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:23:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The fsck command works on raw devices, we discussed raw and block devices in the Formatting and Labelling recipe, so we should use the raw name of a device to check for errors. As you remember from the Mounting and unmounting recipe of this chapter, OpenSolaris check the file systems for error on start-up if configured. That checking option kicks tart the fsck command when necessary. We should not run this command periodically nor should we execute in on a mounted file system. More importantly, we should not use this command on extra large disks as it may take days to complete. Following command checks the /dev/rdsk/c9t0d0s2 and interactively asks for our permission to perform sensitive operations. \\# fsck /dev/rdsk/c9t0d0s2 As you can see in the above command we passed the raw device instead of the block device name to this command. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:24:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. UFS uses several mechanism to prevent and recover from corruptions, such as logging which we discussed in Mounting and unmounting recipe. The fsck uses the output of these mechanism to check and if required try to fix the file system. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:25:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can try the man page and view the short help message to find more about this utility using the following commands. \\# man fsck \\# fsck \\--help 7. Specifying disk quotas on UFSs Like any other advanced file system supports space quota meaning that we can specify a space usage limitation for users on each file system. The disk usage limitation can be enforced for a single user or a group of users over any UFS like /mnt/sol. We can enforce the limitation over number of consumed blocks and Inodes. Each block, depending on its size, can contain 4 KB, 16 KB or any value specified during the file system creation. So, limiting the number of blocks limits the amount of data which can be stored in the file system. Each Inode contains a reference to a file or a directory in the file system. Limiting number of Inodes limits the number of files and directories which can be created. Before we get down to the business, we should understand basics that the disk quota operates on. Following three terms explains these basics. Soft limit: maximum amount of disk space that a user or a group of users can use. The disk usage by the user or the group can be exceeded for a certain amount of time. This amount of time is called Grace Period. Grace Period: A period of time which the soft limit may stay exceeded by a user or a group of users. The grace period can be specified in seconds, minutes, hours, days, weeks, or months. This period of time let the user to get below the soft limit. Hard limit: Specifies a hard limit for the user or group disk usage. This limitation cannot be exceeded in the Grace Period. For each quota we specify one line of information is recorded in the quotas file with the following structure. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:26:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Each file system needs to have a file named quotas owned by root in the top level. We use edauota command to edit this file and include the quota for users and groups of of the system. By default OpenSolaris will use vi to open the quotas file for editing but vi may not be a convenient editor for new users. We can change the default editor of the system by changing the value of EDITOR environment variable by using the following command: \\# export EDITOR=gedit This command change the default editor to gedit which is the default OpenSolaris desktop editor. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:27:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. Assuming that we want to define quota over /mnt/sol for a a user named masoud we need the following steps: Create the quotas file in the root of /mnt/sol \\# touch /mnt/sol/quotas Changing the ownership of the file to root \\# chmod 0600 /mnt/sol/quotas Specifying the quota using edquota command using the following command. \\# edquota masoud When we execute the above command, gedit window opens with content similar to the following figure. To change the quota values, we just need to change the numbers and press CTRL+S to save the file and exit gedit. After specifying the quotas we should turn quotas on if we have not turned it on using the mount table. Following commands turn quotas on for the given file system. \\# quotaon /mnt/sol To turn off the quota for a file system we can use the following command. \\# quotaoff /mnt/sol Now we can get report on quotas using the quota and repquota commands. For example to check quota for masoud on current file system we can use the following command: \\# quota -v masoud Output for this command can be similar to the following figure. To enable the quotas for a file system permanently, persistent across reboot, we can use the mount table options. For example we can add the following line to mount table by editor /etv/vfstab to enable quota for the /mnt/sol /dev/dsk/c9t0d0s2 /dev/rdsk/c9t0d0s2 /mnt/sol ufs 5 yes rq We used the last column which specifies mount options to enable quota for this file system. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:28:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we enable the quotas for a file system, Operating system consult the quota definitions before storing anything in the file system to see whether the user has the right to write some data into the disk according to his disk usage or not. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:29:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Varieties of commands are available to get more out of quotas. We discuss some of these commends here. ","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:30:0","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other quotas reporting commands We can use repquota to view the quota details for a file system. For example to get detailed report on /mnt/sol file system quotas we can use the following command. \\# repquota /mnt/sol Or to view quota information for all users for all file systems we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e repquota -av\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e ### Specifying quotas for group of users or multiple users ### Specifying the grace period We can use edquota to specify the grace period for each user. To open the quota editor for grace period we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e edquota -t masoud\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e In the editor we just need to specify two numbers for the grace period of blocks and inodes. ## See also Looking at the quota, quotaon, repquota, quotacheck, quotaoff man pages can increase our understanding of quotas in general and these commands in particular. We can also check UFS management documentation located at http://docs.sun.com/app/docs/doc/817-0403/sysresquotas-97946?a=view # 8. Getting list of ZFS pools In the introduction section of this chapter we said that all ZFS file systems are build on top of storage pools. So, to work with ZFS, the first thing to lean is storage pools. In this recipe we will learn how to get information about available storage pools using zpool command. ## Getting ready In this recipe we will work on viewing the available zpools, you may find that the result of my commands are different than what you will see in your console, this difference can be caused by the fact that I have more than one pool created while you may only have the rpool which is created when installing OpenSolaris. ## How to do it.. Following command shows a list of available pools in the system along with their basic properties\\' values. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e The result for the executing the command can be similar to the following figure. ![](post-img/3180_02_14.png) The list subcommand can show one or more specific pool\\'s properties when we pass them as its arguments. For example to see information about rpool we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list rpool\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e Each pool has some more specific attributes which we can view or edit their values using zpool list command. For example the following command will show values for the given listed properties of rpool. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c","date":"2010-08-09","objectID":"/2010/08/learning-how-to-use-zfs-in-opensolaris/:30:1","tags":["Solaris","ZFS","File System"],"title":"Learning how to use zfs in OpenSolaris","uri":"/2010/08/learning-how-to-use-zfs-in-opensolaris/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"This is the 4th installment of my entries covering NIO.2. In this entry I will discuss more about what we can do with attributes and permissions. The NIO.2 lets us write the permissions and attributes of a file in addition to reading them. For example we can change the rwx permissions using the Attributes utility class and PosixFilePermission. You can execute the following sample code with almost no changes, the only thing you need to change is the path to our sample file and then you are ready to see how it works. import java.io.IOException; import java.nio.file.\\*; import java.nio.file.attribute.\\*; import java.util.\\*; public class Perm2 { public static void main(String args\\[\\]) throws IOException { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We should have enough permisison to change the owner otherwise it will // throw a FileSystemException: /home/masoud/a: Operation not permitted sort of thing Attributes.setOwner(p, up); //another way to read and write the owner value for a file is using FileOwnerAttributeView FileOwnerAttributeView fo = p.getFileAttributeView(FileOwnerAttributeView.class, LinkOption.NOFOLLOW_LINKS); fo.setOwner(up); //Now that we have changed the permissions lets see the permissions again: att = (Map) p.readAttributes(\"posix:permissions\", LinkOption.NOFOLLOW_LINKS); System.out.println(\"New Permissions:\" + \": \" + att.get(\"permissions\")); } } Let’s see what we have done starting from top: at the beginning we have just initialized the FileSystem object and got a file reference to /home/masoud/dl. Then we read all of this files basic and posix attributes using the attributes view notation. The following sample code shows this step FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); Next step we have use the PosixPermissions class to create a permission set using the OS permissions presentation and then applied these permissions on our file // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); In the next step we created a user principal for a user named masoud and changed the ownership of our file to this user. We have done this using both available methods. // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We s","date":"2010-07-13","objectID":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 4: Changing File System Attributes and Permissions","uri":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In two previous entries I covered Introducing NIO.2 (JSR 203) Part 1: What are new features? and Introducing NIO.2 (JSR 203) Part 2: The Basics In this entry I will discuss Attributes introduced in NIO.2. Using attributes we can read platform specific attributes of an element in the file system. For example to hide a file system in DOS file system or to check the last access date of a file in a UNIX machine. Using NIO.2 we can check which attributes are supported in the platform we are running on and then we can decide how to deal with the available attributes. Following sample code shows how we can detect the available attributes and then how to manipulate them. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); //checking available attributes: Set\u003cString\u003e supportedViews = fs.supportedFileAttributeViews(); //We always have at least one member in the set, the basic view. BasicFileAttributes ba = p.getFileAttributeView(BasicFileAttributeView.class, LinkOption.NOFOLLOW_LINKS).readAttributes(); //Printing some basic attributes System.out.println(p.toString() + \" last access: \" + ba.lastAccessTime()); System.out.println(p.toString() + \" last modified \" + ba.lastModifiedTime()); // As I know I am in NIX machine I access the unix attributes. // If I didnt I should have iterate over the set to determine which // attributes are supported if (supportedViews.contains(\"unix\")) { PosixFileAttributes pat = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); System.out.println(pat.group().getName()); System.out.println(pat.owner().getName()); } I placed plethora of comments on the code so reading and understanding it get easier. In the next snippet we will see how we can read permissions of file system element. The first step in checking permissions is using the checkAccess method as shown below. the method throw an exception if the permission is not present or it will execute with no exception if the permission is present. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); try { // A method to check the access permissin p.checkAccess(AccessMode.EXECUTE); } catch (IOException ex) { Logger.getLogger(perm.class.getName()).log(Level.SEVERE, null, ex); } // Extracting all permissions of a file and iterating over them. //I know that I am dealing with NIX fs so I go directly with that attributes // otherwise we should check which attributes are supported and then we can // use them. PosixFileAttributes patts = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); Set\u003cPosixFilePermission\u003e st = patts.permissions(); for (Iterator\u003cPosixFilePermission\u003e it = st.iterator(); it.hasNext();) { System.out.println(it.next().toString()); } // Using PosixFilePermissions to convert permissions to different representations System.out.println(PosixFilePermissions.toString(st)); As you can see in the code we can use the helper class to convert the permission set to a simpl OS represeted permission of the element. for example the set can be translated to rwx— if the file has owner read, write and execute permissions attached to it. The helper calss can convert the os represenation of the permission to the permissions set for later use in other nio classess or methods. In the next entry I will conver more on permissions and security by tackling the Access Control List (ACL) support in the nio.2 ","date":"2010-06-23","objectID":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 3: File System Attributes and Permissions support in NIO.2","uri":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will discuss the basic classes that we will work with them to have file system operations like copying a file, dealing with symbolic links, deleting a file, and so on. I will write a separate entry to introduce classes which are new to Java 7 for dealing with streams and file contents, watching service and directory tree walking. If you want to know what are new features in Java SE 7 for dealing with IO take a look at Introducing NIO.2 (JSR 203) Part 1: What are new features? Before NIO.2, dealing with file system was mainly done using the File class and no other base class was available. In NIO.2 it there are some new classes at our disposal to take advantage of their existence to do our job. FileSystems: Everything starts with this factory class. We use this class to get an instance of the FileSystem we want to work on. The nio.2 provides a SPI to developed support for new file systems. For example an in-memory file system, a ZIP file system and so on. Following two methods are most important methods in FileSystems class. The getDefault() returns the default file system available to the JVM. Usually the operating system default files system. The getFileSystem(URI uri) returns a file system from the set of available file system providers that match the given uir schema. Path: This is the abstract class which provides us with all File system functionalities we may need to perform over a file, a directory or a link. FileStore: This class represents the underplaying storage. For example /dev/sda2 in *NIX machines and I think c: in windows machines. We can access the storage attributes using FileStoreSpaceAttributes object. Available space, empty space and so on. Following two sample codes shows how to copy a file and then how to copy it. public class Main { public static void main(String\\[\\] args) { try { Path sampleFile = FileSystems.getDefault().getPath(\"/home/masoud/sample.txt\"); sampleFile.deleteIfExists(); sampleFile.createFile(); // create an empty file sampleFile.copyTo(FileSystems.getDefault().getPath(\"/home/masoud/sample2.txt\"), StandardCopyOption.COPY_ATTRIBUTES.REPLACE_EXISTING); // Creating a link Path dir = FileSystems.getDefault().getPath(\"/home/masoud/dir\"); dir.deleteIfExists(); dir.createSymbolicLink(sampleFile); } catch (IOException ex) { Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex); } } And the next sample shows how we can use the FileStore class. In this sample we get the underlying store for a file and examined its attributes. We can an iterator over all available storages using FileSystem.getFileStores() method and examine all of them in a loop. public class Main { public static void main(String\\[\\] args) throws IOException { long aMegabyte = 1024 \\* 1024; FileSystem fs = FileSystems.getDefault(); Path sampleFile = fs.getPath(\"/home/masoud/sample.txt\"); FileStore fstore = sampleFile.getFileStore(); FileStoreSpaceAttributes attrs = Attributes.readFileStoreSpaceAttributes(fstore); long total = attrs.totalSpace() / aMegabyte; long used = (attrs.totalSpace() - attrs.unallocatedSpace()) / aMegabyte; long avail = attrs.usableSpace() / aMegabyte; System.out.format(\"%-20s %12s %12s %12s%n\", \"Device\", \"Total Space(MiB)\", \"Used(MiB)\", \"Availabile(MiB)\"); System.out.format(\"%-20s %12d %12d %12d%n\", fstore, total, used, avail); } In next entry I will discuss how we can manage file attributes along with discussing the security features of the nio.2 file system. ","date":"2010-06-01","objectID":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 2: The Basics","uri":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"I will write a series of blog to discuss what are the new features introduced in NIO.2 (JSR 203). The NIO.2 implementation is a part of OpenJDK project and we can alreay use it by downloading a copy of OpenJDK binary. In the first entry I will just go through what are these new I/O features of Java 7, which help developer iron out better applications easier. Talking about File systems support and features which let us deal with file system we can name the following features: Platform friendly-ness of NIO.2: We can deal with all file systems in a unified model. File tree walk: We can walk on a file tree and examine each node using the built-in APIs, NIO.2 to let us know whether the current member is a file, a directory or a symbolic link. We can then perform any operation we want on that node. File Operations (Copy, Delete, Move): Basic operations are supported with plethora of options. The move operation can be performed in atomic way. Symbolic links support: count soft and hard links as well as managing them. Support for file attributes in NIO.2: Managing file systems attributes is fully supported for different file systems including DOS, POSIX… File system change notification: We can setup a watch service and receive notification when a change happens on the path we are watching. SPI for providing new file systems support, for example to support zip, zfs, btrfs, we can implement the provider interfaces and use the unified API to access that specific file system using our implementation Working with sockets and reding/ writing files we can name the following features: Multicasting is now supported in the DatagramChannel meaning that we can send and receive IP datagrams from a complete group. Both IPv4 and IPv6 are supported. Asynchronous I/O for sockets and Files: Now we can have Asynchronous read and write both on channles and files. It basically means we can have event driven I/O over both networks and files. Other improvement, features: Support for very large multi gigabyte buffers Some MXBeans are included to monitor IO specific buffers. Interoperability between Java 7 IO and previous versions using the Path API. I will post sample codes for each of these features in upcomming entries. So stay tuned if you want to learn more about NIO.2 ","date":"2010-05-20","objectID":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 1: What are new features?","uri":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/"}]