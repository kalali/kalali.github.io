[{"categories":["Architecture"],"content":"Deciding between software architecture attributes primacy is hard. I follow some practices that I will talk a little about them here.","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Last week I was talking to a former colleague, he is running a SaaS startup now, and our conversation went around SRE, production, and reliability. We talked a lot about architectural attributes that relate or impact the incidents, from detection to recovery, and on to prevention. I thought to put some of my thoughts around architecture into a blog post since that was the core of our conversation. The next post I will cover more details on architecture for fault tolerance. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:0:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Architectural attributes, the -ities I generally put different architectural attributes into two categories: Internal: for example operability, testability, supportability, upgradability, extensibility, etc. External: for example scalability, responsiveness, security, compliance, usability, availability, etc. ) The architectural attributes can be sliced and diced in different ways, I will probably write another slicing and decision making in another post. At the end of the day, any attribute and quality of a software system are going to impact the users. But some attributes are more concerns of customers and users of the system while another group is more concerning those building and maintaining it (developers, support, QA, etc.). There is always a push and pull between different stakeholders on the importance and investment that should go into these two categories. For example, usability, availability, and responsiveness might be the product owner’s focus while a technical stakeholder would also think, and advocate about investment in extensibility, supportability, and so on. Each stakeholder advocates for the attributes they better relate to. These pushes and pulls get more interesting when other stakeholders are advocating for attributes, not that clearly understood by others. For example compliance and legal matters (I generally put this into the external category). Keeping the balance Deciding on which attributes from different categories should be the core is very much context-dependent. For example, deciding between yield and harvest would result in deciding whether reliability or availability should have a higher degree of importance. I approach deciding about architectural attributes is based on a few practices: I Talk to all stakeholders to form a clear understanding of what the software system is going to do. Why is it being built, or rebuilt? How is it going to be used and who is going to be the user/s. I think about three attributes as the heart of a system based on the above. Any other attributes form around these over time, without diminishing them I associate a degree of importance with attributes contributing to architecture. These will be considered when the architecture is evolving or when trade-offs are being made for adding a new feature, etc. Iterate over these as an understanding of the system is building up and evolving the system. Deciding on the core attributes is like setting a strategy, decision being made around how the architecture could evolve is easier when you have some core attributes that cannot be diminished or ignored. Next to the core attributes, there are some considerations that deferring or ignoring has the highest cost of reconsideration. For example, testability and operateability are important enough to be next to the core attributes. If a software system is not testable and operatable it is going to rot over time and a re-write will ensue. Some examples to consider: For a regulated business attributes contributing to compliance are the top concerns. For example data integrity, audit-ability, access control, etc. Building anything, no matter how good, that is not compliant is unlikely to see the light of production. For a web-scale service the first items to consider are scalability, availability, and reliability, the rest of the attributes would form around these. Some systems may need very little attention to scalability but an extreme emphasis on safe operation, reliability, and upgradability. A medical device, a CNC machine, and so on. Deciding on any architectural attributes should be deferred as long as it is not causing indecision. When the same subject comes up multiple times without a conclusive way forward it means there is a lack of strategy, being for the product or the organization. I would put having a decision about the architectural attributes of a product to be part of the product and in a larger scope, the organization’s strategy. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:1:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Agile Team"],"content":"What I have observed with successful teams is the trust and belief in the team's mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams.","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Agile Team"],"content":"What I have learned about successful teams is the trust and the belief in the team’s mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams. When the group of individuals that are part of the said team cannot see a mission they trust, going through the famous steps of Forming, Storming, Norming, and Performing will be harder. The mission is what the team usually communicate and collaborate on. And effectively going through FSNP stages is possible with communication and collaboration. The communication and understanding that forms between team members is what makes the team reaching the performing stage and becoming what is called a high performing team. The concept of task oriented teams is floating around with different acronyms, one being the Quick Reaction Team, the miscellaneous teams, being project based team, fluid team, and so on. The whole concept behind it is a team that has no long history of having gone through the team formation and a long term strong mission to move toward to. It is usually working through a set of bugs and firefighting, or through disjoint projects that come into the pipeline. Setting up successful development teams is hard! It is not only because people are different but also because organizations vastly differ from one another and thus one recipe won’t work for everyone and everywhere. I tried setting up teams with a similar approach with some teams focused on long-term work that required more research and prototyping and a team dedicated to quicker gains for the system’s customers. The quicker gains include bugfix or features with smaller scopes and higher demands. The result was accumulated fatigue of never-ending context switching for the quick response team. Of course, there is always the gratifying feeling of what they have developed lands in so-called production but the feeling of accomplishment is not on par with the fatigue of the context switches. That was when I thought about rotating the team members between different teams with implications of time spent getting up to speed with the feature development and transfer of the knowledge context from the team member moving out of the feature development team and getting used to ways of working of the quick response team for the newly joined member. It is possible to have a pool of engineers working together on different areas of the same bounded context, as long as the members that are coming together have already gone through knowing one another as team. But to pull seemingly random engineers between different knowledge/context in a project based manner may result in developing a feature but it will not result in positive outcomes long term. ","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/:0:0","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Developer Experience"],"content":"When a new hire joins a team dealing with the development or sustaining of a large-scale application there are a few things that they would need to form a basic understanding of the system: Deeper understanding of what the business about Get a view of the system’s architecture for the software system and the deployment view. Get a bird’s eye view of the system’s dependencies. At least the immediate dependencies. History lessons, how the system came to be at the current state. Maybe ADRs would help with this. Learn about what are/were the push and pull resulted in the current state (Lack of maintenance, lack of test, long release cycles, well automated, etc.) Learn how to go through a development cycle of checking the code out, make a small fix/change, run the tests, go through the release cycle. In some organizations, these steps take months to complete. Or some of the above may not even be possible, because nobody knows the context of some decisions and some paths taken. It always pays off to make any onboarding as easy as it can get. Onboarding is a recurring cost like reading/changing. So if clean code matters, so should a clean onboarding. There are plenty of reasons for the first days of software developers starting a new job being less than ideal. From poor security onboarding to lack of a buddy to walk them through the day-to-day of work-life for the first few days. Details of that is a subject of another series of blog posts ","date":"2022-04-26","objectID":"/2022/04/onboarding-as-important-as-code-readablity/:0:0","tags":["Team","architecture","Onboarding","Documentation","Developer Experience"],"title":"Smooth Developer Onboarding is as Important as Code Readability","uri":"/2022/04/onboarding-as-important-as-code-readablity/"},{"categories":["Architecture"],"content":"During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the  architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it.","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Introduction During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it. Wikipedia has a good definition of software architecture that I quote below: Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations. The architecture of a software system is a metaphor, analogous to the architecture of a building. It functions as a blueprint for the system and the developing project, laying out the tasks necessary to be executed by the design teams. Software, data, IT, domain, and enterprise are going to have an architecture no matter if it is intentional, developed and nurtured and well documented. Or something that is grown out of what everyone involved in the system has done to arrive to without a record of why the architecture is what it is. Below I write about how and why documenting software architecture is not as successful as it should be and how can it possibly be improved. AD and ADR During the blog I refer to Architecture description as AD and Architecture Decision Record as ADR. I see adding the ADR as an ongoing effort while the AD is the overall architectural view of the system. The AD has higher level of abstraction compared to the ADRs which are focused for a particular decision. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture description infamy The primary reasons for the infamy of software architecture description, as far as I can say, is the failure in attracting different stakeholders to read or to develop architecture descriptions. There are plenty of archetypes of architectural descriptions targeting different groups of stakeholders. Here I will put the focus on the software architecture with the target audience being software engineers. Some reasons for architecture description usually is not the favorite topic of conversation are the followings: The architecture description is too generic and is not created for software developers consumption. The vocabulary is wrong, the addressed concerns may not be relevant and so on. It has too much irrelevant information that the software developers may not need. So the noise to signal ratio is too high for the document to be considered. The architecture description is not easy to access, and if it is; it is not easily readable because of the tooling that is used to create it. No uniform theme is used. Different architecture description within the same organisation are not following the same theme. The description is not up to date From the list above “keep it up to date” might be hard to imagine and the “theme” item may not be clear. So I will go into a bit of details for each. Keep architecture description up to date Keeping documentation up to date is hard, we all know it. And there are many advice on how to approach this. Some of these advice are applicable to any documentation. Keep the documentation short: Write as little and as targeted as possible. Avoid fluff, avoid write one document for all stakeholders approach. Have a responsible person: Each ADR and the AD itself should be owned by someone who is the sole responsible to keep track of and update them. If everyone is responsible nobody is. Make updating easy and streamlined: Keep the AD and ADR in the code repo, where the devs like to spend their time in. Make it easy to contribute to the AD/R: Anyone in the team/s should be able to open a PR or contribute to clarifying the AD/R Establish a uniform theme I know almost no software developer without a very carefully selected theme for the editors, terminal and IDE. So I’d expect the same emphasis of look and feel would apply on the documentation as well. When I say theme I am referring to the following aspects of the architecture documentation: Terminology: Refer to any one concept/artefact/etc. with one term throughout the org Framework: Use the same architecture model framework, if it is C4 Model or 4+1 Model or anything else, stay with the same model everywhere Tooling: There are 100s of diagraming tools, pick one and stay with it. The lighter it is the better chance of it being used. Stay away from heavy tools for day to day work. Choose a tool that works on all operating systems used in the organisation Common icons/glyphs: Develop common icons and glyphs for internal architectural concepts and artefacts; use standard icons, e.g. cloud vendor provided icons, or framework/tools provided icons and glyphs Use a standarar template: At application server organisation we had a architecture committee (AsArch) which had a OnePager template for architectural decisions/changes that needed to review. Everyone in the committee or any attendees knew what to expect to see in the architecture description. For each of the above line items I can write down a long blog post. But let me talk a little bit about the template as the container for rest of the items (a glyph, a diagram, a term or expression wont be used without it appearing on a page) ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:1","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture descriptor template Templates have poor reputation as anyone in larger enterprises associate them with a long document with unfamiliar terminology and vague questions. The architecture description OnePager template should not endup being vague or filled with questions. It is a description not a checklist, not a questionnaire. What about ADRs? One thing to mention before I go far with describing a template, I can say that template is not a farfetched concept when using ADRs. The ADRs follow a template, for example it can contain headers for context, assumptions, decision, consequences and status and the content for each header. I see ADR mostly suitable to document the on-going decisions. Such decisions, if they impact the bigger picture must result in an update in the one page. Think about the OnePager as a summary of all the ADRs. Usual heading in ADRs Some of the most common headings in an ADR are the following: Tittle: What is it that this decision talksa about Date: When was this decision made Context/Summary: Problem definition and solution context. This is setting the scene for rest of the description/decision to come Decision: What is decided in relation to the issue/context/question Status: proposed | rejected | accepted | deprecated |… Consequences: What changes in the system (Performance, testability, cohesion, isolation, etc.), what turn more complicated or easier. You may also see the following headings being mentioned: More heading in ADRs Deciders Assumptions Constraint Related Decisions But generally speaking, the template is a set of common sense headings encapsulating the discussion that has happened over a coffee, over an email, in a meeting, etc. in an easy to follow structure. The OnePager as a story Now, down to a OnePager. What I appreciated about the OnePager that we had was the clear headings and subheadings. There was no confusion about what is needed to be mentioned and in what order. Of course not all headers were necessary to have any content or to be present, but their presense in the OnePager was a guide so that the authors do not forget about adding some details. For example, if there was something to mention in relation to performance impact it would go under “Performance impacts” header. If there was an impact on the system security it would go under the security headring and so on. Reading one pager should feel like reading a story, same as reading a well written code imho, starting with an author and an overview, what is involved in the system and what is not (the scope) and down to details. It must be open to everyone in the organisation, anyone should be able to easily find it, read it, comment on it and could reach out to the author/s for any clarification. The OnePager, at any point in time, must reflect the current state of the approach taken to build the system. Depending on the scope and size of the system the architecture description may have a single a very high level representation of the system components or a more detailed approach. For a significant enough s system there wont be more than Context diagrams (if we assume C4 Model). For example mentioning the presence of a pipeline to deliver the code to a target deployment environment can be a component in the context and described in a single paragraph. Later on each of these high level components of the architecture will have their own OnePager going into the details. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:2","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Conclusion Write less Write where it can be accessed and changed. Stay consistent through the organisation Dont write a single document for all the role; write targeted documents for different roles Have clear ownership for the description ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:2:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":null,"content":"I work in software industry; and have been involved with software development since 2000. Delphi, .Net and J2SE back in the time, and now I am more focused on technical leadership, cloud, architecture, and getting organizations closer to realizing their potentials and goals with. My responsiblities are more in ensuring alignments, and with assisting organizations with tech strategy, architecture, adoption of new technologies and practices, planning and implementations when the area of impact is across organization boundaries. I like the thrill of innovating, building or rebuilding. I enjoy it because it is as giving existence to something that is not; being software engineering or small tinkering projects. I like outdoors, mountains, open land, I like water sports; and the prime of my activities is running, mountain running that is (I am not good at it). I like writing, authored a book and co-authored another a decade ago. Whenever I get the chance I read books ranging from sci-fi and fantasy to leadership and management; every now and then I binge a bunch of articles or blogs that I have bookmarked to go through. Recently I revived this blog (Removed about 110 posts that were outdated) to start writing again How to get in touch with me? If you wanted to talk to me about any of the blog post, or anything else for that matter, send me an email to my first name at kalali.blog. ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About me, Masoud Kalali","uri":"/about/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it.","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it. For example: Have you seen the plenty of use cases that every vendor in every segment of tech stack provide as part of the adoption or success story? How about the tutorial that shows how quickly and easily something as significant as a an observability solution can be set and be up and running? And many other medium that one way or another demo how easily a solution can be used/adopted. Existence of such case studies and tutorial is amazing contribution to the overall wealth of knowledge and understanding in the tech sphere, no doubt avbout it. Sometimes I see engineering team members speaking about adopting a new infrastructure component or a brand new CNCF project with estimations that are 1/20st of the realistic time that it would take to adopt such a solution into our infrastructure. Any of those can be an entry point for a team or organisation deciding to adopt something and then endup in the pink elephant. With the adoption takes longer and longer and at the end of the day the organisation endups with half baked integrations and a team or an individual who is in charge of maintaining something that is not ready for prime time but is in widespread use. If vendors and contributors that are writing quick tutorial can be more straight forward with the write-ups in relation to things like compliance, governance, automation, seamless integration, security, etc. that an organisation must take into account before thinking about adopting project/product XYZ. Over short term there might be fewer attempt at adopting new components, products and projects but in the long run there will be more success in the industry which hopefully make everyone happier. Consumer of tech products will be happier with bug free, and less breaches in the product they are using and the practitioners will be happier with better build and well thought IT ecosystem. On the other end of it, it may result in less experiments and thus less feedback to the product owners on how to improve or enhance different aspect of the product. From the rollout to the usability of it, it is a trade-off and a balance for sure. ","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/:0:0","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Platform Engineering"],"content":"Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Intro Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. Platform? If we divide the software development within an organization into two very broad categories, there is platform development, and platform products in one layer and and application development in another, which is usually consumes the platform. The applications are usually customer/end-user facing or directly facilitating a particular business use-case, for example a login routine or a batch job that runs every night to process schedule payments. Applications most often have a clear business goal and defining OKRs is easier and ROI can be observed within a shorter period of time. On the other hand, platform development which can be anything from shared components for different applications to release pipelines, development tools, or combination of many things together is not as straight forward. Platforms and platform teams are there to not only save initial cost of starting a new project or product but to save on the cost of maintenance and owning such product but introducing familiar concepts and ways of working across the vertical and horizontal integration points. Sometimes a platform/team is hard to justify, not easy to have any meaningful OKRs and usually have no immediate ROI. For example, If the platform team develops a pipeline and related libraries, the adoption of such pipeline takes time. After a reasonable number of applications have started using the said pipeline, product owner would slowly see an indirect ROI from: Better compliance baked into the pipeline Automation in release practices rather than throw over the wall and hand overs Uniform quality gates as pipeline would enforce some level conde analysis and apply some gates Faster time to production as it reduces the number of manual steps and communication costs Less code sediment as continues or even frequent deployment reduces the code that is sitting in the codebase without being exercised Less regressions because of frequent small releases etc. That is when the justification for existence of a platform become easier and the platform teams and products will become necessities. Why Platforms? Simply put platforms help with the following: Time saving as they prevent inventing the same thing again and again; e.g. a pipeline. Time saving as, when done right, they introduce a consistent way of working across diferent layers. Being applications, infrastructure or tooling. That will in turn makes transfer of knowledge and understanding easier Time saving as teams with expertise build re-usable components that other team can incorporate rather than every time having to acquire such expertise Governance, and compliance and other orthogonal aspects can be addressed via automation backing variety of self services. Let’s say that there an enterprise with the very well established process for software development, delivery, operation and support. This has been the case for the past couple of decades and individuals, teams, organisational units, and organisation as a whole developed a culture around the development process (for good or bad). The process can be something along the line of the following for simple features/functionalities. Imagine a handover between most steps. Long contemplations and many meetings of the domain architect/s Retirement gathering and certifying it with stakeholders Doing any risk assessment, compliance check in relation to data being processed and/or collected Planning (usually includes capacity planning and schedule as well as development timeline) Development and figuring o","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:0","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Setting up the expectations Being a product owner or an advocate for a platform requires more conversation and investigation about painpoints existing in the organization as a whole. Being the application/program development teams or the relationship between compliance/security/process divisions with the application development hierarchy. Setting clear expectations for external customers and goals for the platform team itself is an important task that the product owner, in combination with engineering teams (and management) should set. This expectation setting will make it clear to management on what timeline and outcome to expect for given organisational support, runway and budget. At the same time, a platform development team must make it clear to the management that having a platform is a tradeoff. For example if the current way of working is that each team entirely choose their guidelines and architectural patterns, a development platform may enforce certain principles and guidelines using automation as much as possible, so no exception and exemptions. An example can be use of certain libraries for logging, or certain pipeline for deployment, certain number of active engineers per application and so on. Platform adoption A development platform like any other product will need adoption and use to get the feedback cycle going. Let’s say that the platform teams understood the painpoints and the platform is developed to not only remedy the painpoints but also ensure the viability of the application ecosystem over the next decad. If the platform is not being used and has an established feedback cycle it will be hard for it to grow and cover the majority of use-cases or valid viewpoints. There are some general approach in spreading the adoption of the platform and starting a feedback cycle. Platform team owning applications One approach that I have employed, and seems to work for adoption of the platform, is ensuring the platform team owns some programs built on top of the platform. This service can be the example to showcase how platform works and how does it help with the pain-points that it exists to remedy and how does it propel the organisation into the new ways of working and tech landscape. Of course, platform owners developing applications has some drawbacks: The application/s may turn into a an unintended blueprint and others may follow something that is not fully baked yet. The platform team too far get into habit of optimising the platform for their own use/level of competence. The platform team get too far distracted by the application to pay attention to the platform itself. This may result in mental overload for the team and far too many context switching. Platform team helping an application team convert Another approach is locating a team that struggling the most and is vocal the most about the painpoints, of course within reason, and help them convert their service development to use the new platform. This would require multiple criteria: The service in question does not have too many dependencies that can impact the conversion The service team is willing to take the steps. The steps that the platform is advocating for. For example code review, use of pipeline, test automation and so on that is a service provided by the platform. The platform team has some understanding of the domain that the service team is driving. Combination of the above two One thing to consider in the combination or even in the second approach is to identify the players and influencers and get buy-in from them. In every organisation there are people who are sitting far behind the scene without any title with more influence on what different teams may adopt that any line managers. Convincing them will multiply the rate of adoption. Next In the next instalments of this series I will write more about each aspect of the development platform mentioned here. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:1","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Identity, something that we hear more often these days with the whole web 2.0 and social services and more and more web based public services growing around us. The identity notion is an integral part of a security system in distributed services. Developing effective software system require an effective security and access control system which java provides, not exactly in the way that it should be in 2011 but it does provide what is the bare bone necessity to develop applications and frameworks on top of it and benefit from its presence. The identity API is going to ease the interaction between the identity providers and those who consume the identity and trust the identity providers in addition to governing and managing the identity attributes. I was studying the JSR details and it seems to be covering everything required for the identity attributes governance and the required API for both ends of the usage including the client API the governing/producing API. The identity producing and consuming is not new and there are fair number of public identity producers like facebook, twitter, etc. and also products that system integrators can use like OpenAM as an open source product or gorilla Commercial software products like ORACLE identity management or IBM tivoli identity management software, etc. In a very simple set of words, the JSR 351: The Java Identity API will be as successful as it is going to be adopted. No adoption and it will endup dying some dark corner… Design a simple and elegant API and try to ship it with some free and easy to use service implementations and it may get some momentum, otherwise it will be a goner and people will stick with what they have. I like the new features that it is going to introduce in the decision making or authorization part but we should see how well it will be adopted by identity providers to develop the services that provides the interaction point between the JSR interface and their repositories. Pushing it as JSR wont really do that much without a wide adoption in the community. Look at how many implementation of the JSR 115 and JSR 196 exits to be plugged into application servers supporting the contract and you will get what I am referring to by community adoption. ","date":"2011-10-24","objectID":"/2011/10/some-thought-on-jsr-351-java-identity-api/:0:0","tags":["Java","JCP"],"title":"My thoughts on JSR 351, Java Identity API","uri":"/2011/10/some-thought-on-jsr-351-java-identity-api/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Well, as many of us already know Oracle submitted the JSR for Java EE 7 which is sort of an umbrella JSR for many update in already existing specifications, new versions of some JSRs and some completely new JSRs which will be part of the grand Java EE 7 - JSR 342. One of these JSRs is the JSR 343 which introduces a new version of JMS into the platform as an evolution of its previous version, JSR-914, and will unify the JMS usage with what added to the platform in the past 8 years. The following represent some very simple usecases of JMS in the enterprise while complex multiphase transactional usecases are not unusual when MDBs and XA data sources are involved. JMS API architecture JMS itself is for asynchronous communication and widely used to communicate some execution instructions from one node or point to another or a set of other points. For example long running queries can be queued using a JMS queue to get processed by another point in the system while the query client is not blocked for the query result. Or it can be used to communicate a common set of instructions to many interested parties which may or may not be around the communication happens, durable subscriptions and persisted topics. For example when clients need to get an ordered set of update messages to update a localcache when they get online after some times. Each client will get its own copy of messages it should receive when getting online. JMS API provides enough functionalities to realize most of our design out of the specification and the minor features and functionalities not included in the JSR while required by some designs are covered by the vendor specific pack of enhancement and tweaks provided in the broker level and through the vendor specific API. You may ask if the current JMS API provides all we need, why a new JSR should be put on the table, the answer mainly relies on the theme for Java EE 7 which is making Java EE more cloud friendly and sort of cloud enabled by nature rather than by product. The details of JMS 2.0 spec goals are listed at the JSR homepage but a brief list can be seen as follow: Community requested features and enhancements. Make the JSR more cloud friendly based on how Java EE 7 will define “to be cloud friendly” Cleanup of some ambiguity in the relation of JMS with other Java EE specs. Make the API easier to use, more annotations and more generics will be involved for the least of the things or maybe reducing number of boxes and lines in the aove figure could help many to start with the API faster. Make necessary changes to benefit from the JSR-299 or Contexts and Dependency Injection to easier and more unified use of API. In the follow up posts I will iterate over each one of these bullet points in more details. I am member of the JMS 2.0 expert group but this post or any other post in my personal blog does not reflect the expert group opinion or the opinion of my employer on the subject unless you could not see this paragraph at the end of the post :-). ","date":"2011-05-17","objectID":"/2011/05/jms2-jsr/:0:0","tags":["Java","JCP","JMS"],"title":"Brief overview of JSR 343: JavaTM Message Service 2.0","uri":"/2011/05/jms2-jsr/"},{"categories":["SysAdm","Old Blog Migrated Content"],"content":"SMF services are basically daemons staying in background and waiting for the requests which they should server, when the request come the daemon wake ups, serve the request and then wait for the next request to come. The services are building using software development platforms and languages but they have one common aspect which we are going to discuss here. The service manifests which describe the service for the SMF and let the SMF manage and understand the service life cycle. To write a service manifest we should be familiar with XML and the service manifest schema located at /usr/share/lib/xml/dtd/service_bundle.dtd.1. This file specifies what elements can be used for describing a service for the SMF. Next thing we need is a text editor and preferable an XML aware text editor. The Gnome gedit can do the task for us. The service manifest file composed of 6 important elements which are listed below: The service declaration: specifies the service name, type and instantiation model Zero or more dependency declaration elements: Specifies the service dependencies Lifecycle methods: Specifies the start, stop and refresh methods Property groups: Which property groups the service description has. Stability element: how stable the service interface is considering version changes Template element: more human readable information for the service. To describe a service, first thing we need to do is identifying the service itself. Following snippet shows how we can declare a service named jws. \u003cservice name='network/jws’ type='service' version='1'\u003e \u003csingle_instance/\u003e The first line specifies the service name, version and its type. The service name attribute forms the FMRI of the service which for this instance will be svc:/network/jws. In the second line we are telling SMF that it should only instantiate one instance of this service which will be svc:/network/jws:default. We can use the create_default_instance element to manipulate automatic creation of the default instance. All of the elements which we are going to mention in the following sections of this article are immediate child elements of the service element which itself is a child element of the service_bundle element. The next important element is dependency declaration element. We can have one or more of this element in our service manifest. \u003cdependency name='net-physica' grouping='require_all ' restart_on='none' type='service'\u003e \u003cservice_fmri value='svc:/network/physical'/\u003e \u003c/dependency\u003e Here we are telling that our service depends on the svc:/network/physical service and this service needs to be online before our service can start. Some of the values for the grouping attribute are as follow: The require_all which represent that all services marked with this grouping must be online before our service came online The require_any which represents that any of the services in this grouping suffice and our service can become online if one of them is online The optional_all presence of the services marked with this grouping is optional for our service. Our service works with or without them. The exclude_all: specifies the services which may have conflict with our service and we cannot become online in presence of them The next important elements are specifying how the SMF should start, stop and refresh the service. For these tasks we use three exec_method elements as follow: \u003cexec_method name='start' type='method' exec='/opt/jws/runner start' timeout_seconds='60'\u003e \u003c/exec_method\u003e This is the start method, SMF will invoke what the exec attribute specifies when it want to start the service. \u003cexec_method name='stop' type='method' exec=':kill' timeout_seconds='60'\u003e \u003c/exec_method\u003e The SMF will terminate the process it started for the service using a kill signal. By default it uses the SIGTERM but we can specify our own signal. For example we can use ‘kill -9’ or ‘kill -HUP’ or any other signal we find appropriate for our service termination. \u003cexec_method name=‘refresh’ type=‘method’ exe","date":"2011-01-31","objectID":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/:0:0","tags":["SMF","Solaris"],"title":"Writing Solaris Service Management Facility (SMF) service manifest","uri":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"In this article we will study how we can use the fmadm command to get the list of faulty components along with the detailed information about the fault. Before starting this article we should have a command console open and then we can proceed with using the fmadm command. The most basic form of using fmadm command is using its faulty subcommand as follow \\# fmadm faulty When we have no error in the system, this command will not show anything and exit normally but with a faulty component the output will be different, for example in the following sample we have a faulty ZFS pool because some of its underlying devices are missing. fmadm command output Starting from top we have: Identification record: This record consisting of the timestamp, a unique event ID, a message Id letting us know which knowledge repository article we can refer to for learning more about the problem and troubleshooting and finally the fault severity which can be Minor or Major. Fault Class: This field allows us to know what is the device hierarchy causing this fault Affects: tells us which component of our system is affected and how. In this instance some devices are missing and therefore the fault manager takes the Zpool out of service. Problem in: shows more details about the problem root. In this case the device id. Description: this field refer us to the a knowledge base article discussing this type of faults Response: Shows what action(s) were executed by fault manager to repair the problem. Impact: describe the effect of the fault on the overall system stability and the component itself Action: a quick tip on the next step administrator should follow to shoot the problem. This step is fully described in the knowledge base article we were referred in the description field. Following figure shows the output for proceeding with the suggested action. fmadm taking action As we can see the same article we were referred to, is mentioned here again. We can see that two of the three devices have failed and fpool had no replica for each of those failed device to replace them automatically. If we had a mirrored pool and one of the three devices were out of service, the system could automatically take corrective actions and replace continue working in a degraded status until we replace the faulty device. The fault management framework is a plug-able framework consisting of diagnosis engines and subsystem agents. Agents and diagnosis engine contains the logic for assessing the problem, performing corrective actions if possible and filing the relevant fault record into the fault database. To see a list of agents and engines plugged into the fault management framework we can use config subcommand of the fmadm command. Following figure shows the output for this command. fmadm configuration As we can see in the figure, there are two engines deployed with OpenSolaris, eft and the zfs-diagnosis. The eft, standing for Eversholt Fault Diagnosis Language, is responsible for assessing and analyzing hardware faults while the zfs-diagnosis is a ZFS specific engine which analyzes and diagnoses ZFS problems. The fmadm is a powerful utility we which can perform much more than what we discussed. Here we can see few other tasks we can perform using the fmadm. We can use the repaired subcommand of the fmadm utility to notify the FMD about a fault being repaired so it changes the component status and allows it to get enabled and utilized. For example to notify the FMD about repairing the missing underlying device of the ZFS pool we can use the following command. \\# fmadm repaired zfs://pool=fpool/vdev=7f8fb1c77433c183 We can rotate the log files created by the FMD when we want to keep a log file in a specific status or when we want to have a fresh log using the rotate subcommand as shown below. \\# fmadm rotate errlog | fltlog The fltlog and errlog are two log files residing in the /var/fm/fmd/ directory storing all event information regarding faults and the errors causing them. To","date":"2011-01-12","objectID":"/2011/01/solaris-fault-administration-using-fmadm-command/:0:0","tags":["Solaris","ZFS"],"title":"Solaris fault administration using fmadm command","uri":"/2011/01/solaris-fault-administration-using-fmadm-command/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Performance, performance, performance; this is what we hear in all software development and management sessions. ZFS provides few utility commands to monitor one or more pools’ performance. You should remember that we used fsstat command to monitor the UFS performance metrics. We can use iostat subcommand of the zpool command to monitor the performance metrics of ZFS pools. The iostat subcommand provides some options and arguments which we can see in its syntax shown below: iostat \\[-v\\] \\[pool\\] ... \\[interval \\[count\\]\\] The -v option will show detailed performance metrics about the pools it monitor including the underlying devices. We can pass as many pools as we want to monitor or we can omit passing a pool name so the command shows performance metrics of all commands. The interval and count specifies how many times we want the sampling to happen what is the interval between each subsequent sampling. For example we can use the following command to view detailed performance metrics of fpool for 100 times in 5 seconds interval we can use the following command. \\# zpool iostat -v fpool 5 100 The output for this command is shown in the following figure. Monitor ZFS pool performance The first row shows the entire pool capacity stats including how much space were used upon the sampling and how much was available. The second row shows how many reads and writes operations performed during the interval time and finally the last column shows the band width used for reading from this pools and writing into the pool. The zpool iostat retrieve some of its information from the read-only attributes of the requested pools and the system metadata and calculate some other outputs by collecting sample information on each interval. ","date":"2010-12-30","objectID":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/:0:0","tags":["Solaris","ZFS"],"title":"Monitoring ZFS pools performance using zpool iostat","uri":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. We are going to add a logical interface to e1000g1 interface with a 10.0.2.24 as its static IP address. Before doing so let’s see what network interface we have using the ifconfig command. 3180_04_12 Now to add the logical interface we only need to execute the following command: \\# ifconfig e1000g1 addif 10.0.2.24/24 up Invoking this command performs the following tasks: Create a logical interface named e1000g1:1. The naming schema for logical interfaces conforms with interface_name:logical_interface_number which the number element can be from 1 to 4096. Assign 10.0.2.24/24 as its IP address, net mask and broadcast address. Now if we invoke ifconfing -a command the output should contain the logical interface status as well. The following figure shows a fragment of ifconfig -a command. 3180_04_13 Operating system does not retain this configuration over a system reboot and to make the configuration persistent we need to make some changes in the interface configuration file. For example to make the configuration we applied in this recipe persistent the content of /etc/opensolaris.e1000g1 should something similar to the following snippet. 10.0.2.23/24 addif 10.0.2.24/24 The first line as we discussed in recipe 3 of this chapter assign the given address to this interface and the second like adds the logical interface with the given address to this interface. To remove a logical interface we can simply un-plumb it using the ifconfig command as shown below. \\# ifconfig e1000g1:1 unplumb When we create a logical interface, OpenSolaris register that interface in the network and any packet received by the interface will be delivered to the same stack that handles the underlying physical interface. Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. ","date":"2010-12-09","objectID":"/2010/12/managing-logical-network-interfaces-in-solaris/:0:0","tags":["Solaris","Networking"],"title":"Managing Logical network interfaces in Solaris","uri":"/2010/12/managing-logical-network-interfaces-in-solaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Link aggregation or commonly known Ethernet bonding allows us to enhance the network availability and performance by combining multiple network interfaces together and form an aggregation of those interfaces which act as a single network interface with greatly enhanced availability and performance. When we aggregate two or more network interfaces, we are forming a new network interface on top of those physical interfaces combined in the link layer. We need to have at least two network interfaces in our machine to create a link aggregation. The interfaces must be unplumb-ed in order for us to use them in a link aggregation. Following command shows how to unplumb a network interface. \\# ifconfig e1000g1 down unplumb We should disable the NWAM because link aggregation and NWAM cannot co-exist. \\# svcadm disable network/physical:nwam The interfaces we want to use in a link aggregation must not be part of virtual interface; otherwise it will not be possible to create the aggregation. To ensure that an interface is not part of a virtual interface checks the output for the following command. \\# dladm show-link Following figure shows that my e1000g0 has a virtual interface on top of it so it cannot be used in an aggregation. 3180_04_14 To delete the virtual interface we can use the dladm command as follow \\# dladm delete-vlan vlan0 The link aggregation as the name suggests works in the link layer and therefore we will use dladm command to make the necessary configurations. We use create-aggr subcommand of dladm command with the following syntax to create aggregation links. dladm create-aggr \\[-l interface_name\\]\\* aggregation_name In this syntax we should have at least two occurrence of -l interface_name option followed by the aggregation name. Assuming that we have e1000g0 and e1000g1 in our disposal following commands configure an aggregation link on top of them. \\# dladm create-aggr -l e1000g0 -l e1000g1 aggr0 Now that the aggregation is created we can configure its IP allocation in the same way that we configure a physical or virtual network interface. Following command plumb the aggr0 interface, assign a static IP address to it and bring the interface up. \\# ifconfig aggr0 plumb 10.0.2.25/24 up Now we can use ifconfig command to see status of our new aggregated interface. \\# ifconfig aggr0 The result of the above command should be similar to the following figure. 3180_04_15 To get a list of all available network interfaces either virtual or physical we can use the dladm command as follow \\# dladm show-link And to get a list of aggregated interfaces we can use another subcommand of dladm as follow. \\# dladm show-aggr The output for previous dladm commands is shown in the following figure. 3180_04_16 We can change an aggregation link underlying interfaces by adding an interface to the aggregation or removing one from the aggregation using add-aggr and remove-aggr subcommands of dladm command. For example: \\# dladm add-aggr -l e1000g2 aggr0 \\# dladm remove-aggr -l e1000g1 aggr0 The aggregation we created will survive the reboot but our ifconfig configuration will not survive a reboot unless we persist it using the interface configuration files. To make the aggregation IP configuration persistent we just need to add create /etc/hostname.aggr0 file with the following content: 10.0.2.25/24 The interface configuration files are discussed in recipe 2 and 3 of this chapter in great details. Reviewing them is recommended. To delete an aggregation we can use delete-aggr subcommand of dladm command. For example to delete aggr0 we can use the following commands. \\# ifconfig aggr0 down unplumb \\# dladm delete-aggr aggr0 As you can see before we could delete the aggregation we should bring down its interface and unplumb it. In recipe 11 we discussed IPMP which allows us to have high availability by grouping network interfaces and when required automatically failing over the IP address of any failed interface to a healthy one. In this recipe we ","date":"2010-11-25","objectID":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/:0:0","tags":["Solaris","Networking"],"title":"Configuring Solaris Link Aggregation (Ethernet bonding)","uri":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will look at how the directory tree walker and the directory stream reader works. These two features are another couple of long requested features which was not included in the core java before Java 7. First, lets see what directory stream reader is, this API allows us to filter content of a directory on the file system and extract the file names that matches our filter criteria. The feature works for very large folders with thousands of files. For filtration we can use PathMatcher expression matching the file name or we can filter the directory content based on the different file attributes. for example based on the file permissions or the file size. Following sample code shows how to use the DirectoryStream along with filtering. For using the PathMatcher expression we can just use another overload of the newDirectoryStream method which accepts the PathMatcher expression instead of the filter. public class DirectoryStream2 { public static void main(String args\\[\\]) throws IOException { //Getting default file system and getting a path FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/usr/bin\"); //creating a directory streamer filter DirectoryStream.Filter filter = new DirectoryStream.Filter () { public boolean accept(Path file) throws IOException { long size = Attributes.readBasicFileAttributes(file).size(); String perm = PosixFilePermissions.toString(Attributes.readPosixFileAttributes(file).permissions()); if (size \u003e 8192L \u0026\u0026 perm.equalsIgnoreCase(\"rwxr-xr-x\")) { return true; } return false; } }; // creating a directory streamer with the newly developed filter DirectoryStream ds = p.newDirectoryStream(filter); Iterator it = ds.iterator(); while (it.hasNext()) { Path pp = it.next(); System.out.println(pp.getName()); } } } The above code is self explaining and I will not explain it any further than the in-line comments. Next subject of this entry is directory tree walking or basically file visitor API. This API allows us to walk over a file system tree and execute any operation we need over the files we found. The good news is that we can scan down to any depth we require using the provided API. With the directory tree walking API, the Java core allows us to register a vaster class with the directory tree walking API and then for each entry the API come across, either a file or a folder, it calls our visitor methods. So the first thing we need is a visitor to register it with the tree walker. Following snippet shows a simple visitor which only prints the file type using the Files.probeContentType() method. class visit extends SimpleFileVisitor { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) { try { System.out.println(Files.probeContentType(file)); } catch (IOException ex) { Logger.getLogger(visit.class.getName()).log(Level.SEVERE, null, ex); } return super.visitFile(file, attrs); } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) { return super.postVisitDirectory(dir, exc); } @Override public FileVisitResult preVisitDirectory(Path dir) { return super.preVisitDirectory(dir); } @Override public FileVisitResult preVisitDirectoryFailed(Path dir, IOException exc) { return super.preVisitDirectoryFailed(dir, exc); } @Override public FileVisitResult visitFileFailed(Path file, IOException exc) { return super.visitFileFailed(file, exc); } } As you can see we extended the SimpleFileVisitor and we have visitor methods for all possible cases. Now that we have the visitor class, the rest of the code is straight forward. following sample shows how to walk over _/home/_masoud directory down to two levels. public class FileVisitor { public static void main(String args\\[\\]) { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud\"); visit v = new visit(); Files.walkFileTree(p, EnumSet.allOf(FileVisitOption.class), 2, v); } } You can grab the latest version of Java 7 aka Dolphin from here and from the same page you can grab the late","date":"2010-08-23","objectID":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 6: Filtering directory content and walking over a file tree","uri":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"For long time Java developers used in-house developed solutions to monitor the file system for changes. Some developed general purpose libraries to ease the task of others who deal with the same requirement. Commercial and free/ open source libraries like http://jnotify.sourceforge.net/, http://jpathwatch.wordpress.com/ and http://www.teamdev.com/jxfilewatcher among others. Java 7 comes with NIO.2 or JSR 203 which provides native file system watch service. The watch service provided in Java 7 uses the underlying file system functionalities to watch the file system for changes, so if we are running on Windows, MacOS or Linux… we are sure that the watch service is not imposing polling overhead on our application because the underlying OS and file system provides the required functionalities to allow Java to register for receiving notification on file system changes. If the underlying file system does not provide the watch-ability, which I doubt it for any mainstream file system, Java will fall back to some rudimentary polling mechanism to keep the code working but the performance will degrade. From the mentioned libraries the jpathwatch API is the same as the Java 7 APIs to make it easier to migrate an IO based application from older version of Java to Java 7 when its time arrives. The whole story starts with WatchService which we register our interest in watching a path using it. The WatchService itself is an interface with several implementatins for different file system and operating systems. We have four class to work with when we are developing a system with file system watch capability. A Watchable: A watchable is an object of a class implementing the Watchable interface. In our case this is the Path class which is the one of the central classes in the NIO.2 A set of event types: We use it to specify which types of events we are interested in. For example whether we want to receive creation, deletion, … events. In our case we will use StandardWatchEventKind which implements the WatchEvent.Kind. An event modifier: An event modifier that qualifies how a Watchable is registered with a WatchService. In our case we will deal with nothing specific up to now as there is no implementation of this interface included in the JDK distribution. The Wacher: This is the watcher who watch some watchable. In our case the watcher watches the File System for changes. The abstract class is java.nio.file.WatchService but we will be using the FileSystem object to create a watcher for the File System. Now that we know the basics, let’s see how a complete sample will look like and then we will break down the sample into different parts and discuss them one by one. import java.io.IOException; import java.nio.file.FileSystem; import java.nio.file.FileSystems; import java.nio.file.Path; import java.nio.file.StandardWatchEventKind; import java.nio.file.WatchEvent; import java.nio.file.WatchKey; import java.nio.file.WatchService; import java.util.List; import java.util.logging.Level; import java.util.logging.Logger; public class WatchSer { public static void main(String args\\[\\]) throws InterruptedException { try { FileSystem fs = FileSystems.getDefault(); WatchService ws = null; try { ws = fs.newWatchService(); } catch (IOException ex) { Logger.getLogger(WatchSer.class.getName()).log(Level.SEVERE, null, ex); } Path path = fs.getPath(\"/home/masoud/Pictures\"); path.register(ws, StandardWatchEventKind.ENTRY_CREATE, StandardWatchEventKind.ENTRY_MODIFY, StandardWatchEventKind.OVERFLOW, StandardWatchEventKind.ENTRY_DELETE); WatchKey k = ws.take(); List\u003e events = k.pollEvents(); for (WatchEvent object : events) { if (object.kind() == StandardWatchEventKind.ENTRY_MODIFY) { System.out.println(\"Modify: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_DELETE) { System.out.println(\"Delete: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_CREATE) { System.out.println(\"Created: \" + object.co","date":"2010-08-10","objectID":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 5: Watch Service and Change Notification","uri":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What is a file system File systems make it possible to store and retrieve files and containing data into storages like hard disks, optical disks and other types of storages. OpenSolaris support both legacy file systems like UNIX File System (UFS) and its own file system called Zettabyte File System (ZFS). Following figure show how UFS and other legacy file systems work. As you can see we can partition each storage device into one or more volumes with different file systems. For example one hard disk may have an NTFS volume, a UFS volume and an EXT4 volume. In contrast with legacy file systems, ZFS uses concept of resource pooling. In ZFS one or more devices, volumes or files can be combined to form a pool of storages, hence called zpool. Then file systems are built on top of this pool. For example we can create a file system capable of storing a 32 TB file on top of a zpool created on using 16 hard disks, 2 TB each. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:1:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What OpenSolaris supported file systems are OpenSolaris as a pioneer OS for enterprises supports several file systems in addition to pushing its flagship and unique file system named ZFS. Each supported file system is either supported because of backward compatibility or to provide the administrators with a wider range of choices when it come to formatting the storages. The UNIX file system (UFS) is the common file system on UNIX like operating systems. The Personal Computer File System (PCFS) supports FAT12, FAT16 and FAT32 formatted disks. The High Sierra File System (HSFS) allows us to access files on High Sierra or ISO 9660 format CD-ROM disks. The Temporary File Storage Facility (TMPFS) provides a temporary file system in RAM or SWAP space, we can access the file system as a normal file system but in the background it is stored in memory instead of persisted storage. The integration of Storage and Archive Manager and Quick File System (SAM-QFS) is provided so we can have access to a clustered archiving solution in OpenSolaris. ZFS is the flagship file system of OpenSolaris. The file system is a combination of file system and logical volume manager. Some of the ZFS highly steamed features are discussed below. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:2:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"ZFS unique features ZFS is a unique file system which addresses all concerns in the enterprise data centers. We will discuss these features in this section. Storage pools ZFS file systems are built on top of storage pools which are built on top of a group of virtual devices. A virtual device can vary from an entire hard drive down to a file. Capacity ZFS is a 128-bit file system which can store 1.84 × 10^19^ times more data than current 64-bit systems. Copy-on-write transactional model No data get overwritten when updated and instead new data will be wrote in separate blocks and then the pointers pointing to old data will be changed to point to new blocks. Multiple write operation can be grouped together to enhance the performance. Snapshots ZFS does not overwrite the data when they are changed; this feature is used to keep snapshots of file system for later user. Clones A clone is a writeable snapshot which its block content changes when a block is changed in another clone. Deduplication Deduplication is a file system level compression method in which redundant data is eliminated. For example in a typical enterprise mail server one email attachment may find its way into hundreds of users mail boxes, Deduplication will store the actual file once and all other instance of the file are referenced to the same actual file. We use a console to study all commands discussed in this chapter. So before we can execute any of these commands we should be in terminal environment. To activate the terminal environment we can: Hit ALT+F2 to summon the run window and then type console to open a console window Press ALT+CTRL+F2 to switch the workspace to another terminal and use it to practice the command. 1.Using basic file system commands OpenSolaris supports multiple file systems but to access all of them we need to know how to use some basic commands. Following table includes these basic commands along with their descriptions and samples showing how we can use them. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:2:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will study some of the very basic file system commands in this recipe. The commands need to be invoked in the terminal window. To enter the terminal window Hit ALT+F2 to summon the run window and then type console to open a console window. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:3:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We have some basic commands which we should know to deal with the file system in a console. Some of these commands are introduced in the following table. Command Sample cd We can change the current directory using cd command. For example : cd /home/masoud: switch the current directory to /home/masoud cd ..: switch one level up from the current directory. ls We can use this command to get a list of directories and files. It works similar to ls in Linux or dir in MS-DOS. For example: ls: shows the list of all files and directories except hidden ones. ls -al: shows the list of all files and directories along with detailed permissions and creation dates. ls -al /home: shows content of the given path as explained above. mkdir We can create new directories using mkdir command anywhere we have write permission. For example: mkdir safe: will create a directory named safe inside the current directory. mkdir -p /home/masoud/safe/personal/cash: When using -p it creates all directories required to form the mentioned path. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:4:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. All commands introduced in this recipe rely on the metada stored for the file system to perform its job. For example the ls command checks the metadata available for the path in question to show content of the given path. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:5:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can get a short or long help for any command in OpenSolaris using the commands help messages or the provided man pages. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing the help message for a command We can pass --help parameter to almost any command in OpenSolaris to see a short help message including usage syntax and list of options along with their descriptions. For example: \\# mkdir \\--help ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing extensive manual of a command We can see complete and extensive manual pages for any command in OpenSolaris by passing the command name to man command. For example to view the man pages for mkdir we can use the following command: \\# man mkdir ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Some other commands to review Following list includes some other commands which we need to know. Command to copy files and directories: cp Command to delete file and directories: rm Command to move or rename files and directories: mv 2. Formatting disks In this task we will learn how we can format and label disks prior to creating a file system on them. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:3","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready When we attach a disk to an OpenSolaris machine OpenSolaris will assign a name to disk and its partitions or slices. All disks have assigned names under /dev/dsk and /dev/rdsk which lets us access the disk at block level in the first name and at bye level using the second name. The byte level access method is also called raw access method. The disks names are usually compatible with C#T#D#S# or C#T#D#P#. The C stands for controller and the number part is a hex number like 4 or F4. A SATA connector on the mother board is sample of a controller. The T which is optional stands for target, the D stands for disk and the number after it points to one of the disks attached to the controller and the digit after S or P point to the slice number or the partition number. If the partition is from Solaris type we call it slice and its identifier is S# while if the partition is non-Solaris it will be identified with the P# ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:7:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. We can use format and fdisk commands to format and label the disks prior to using them. We will discuss format command as it also includes fdisk. To enter into the format command shell we should invoke format command in a console which will open the format shell. The initial screen as we can see in the following figure shows currently attached disks and allows us to select which disk we want to operate on. I have two hard disks connected to SATA controller and one to IDE controller. We can enter 1 and the list of available commands appears which is as shown in the following figure: The partition, format and fdisk are most common operations which we usually use. Typing each one of them opens another set of options which we can choose to commence with the operation. Instead of typing the complete command name, we can just enter its first character. For example we can quit from format shell to operating system shell or get back to format shell from its subcommands using q instead of quit. If we select a brand new disk in the format menu, the format command informs us that the disk has no partition and we need to use fdisk command to create some partitions before proceeding. To use fdisk we just need to enter fdisk subcommand and create partitions as we need or accept the default partitioning which fdisk command suggest. The default partitioning will create one Solaris2 partition occupying the entire disk. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a Solaris2 partition We can create a Solaris 2 partition using the following steps in format shell: Enter disk command and then select 1 to operate on c9t0d0 Enter fdisk command and when it asked whether we want to use the default partition select y so it create one Solaris2 partition occupying the whole disk. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a FAT32 partition Now let's create a FAT32 partition in the second disk using the following steps in the format shell: Enter disk command and then select 2 to operate on c9t1d0 Enter fdisk command and when it asked whether we want to use the default partition select n so it will enter the fdisk subcommand shell. When asked for selection enter 1 to create a new partitioning When asked for the partition type enter C to create a FAT32 partition When asked for size, enter 100 to use the whole disk capacity Enter y to activate the partition Now that the partition creation is complete enter 5 to apply the changes and exit fdisk shell. Now quit the format shell by entering q command. After creating the partitions we need to apply a file system on them before actually using them. To apply the file system on Solaris slice we can use newfs command as follow. \\# newfs /dev/rdsk/c9t0d0s2 And to apply the file system on the FAT32 partition we can use the mkfs with some additional parameters as follow: \\# mkfs -F pcfs -o fat=32 /dev/rdsk/c9t1d0p0:c ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Formatting removable disks. We can use format and fdisk commands to partition a removable media like a memory card or a thumb drive. For more information and detailed instructions check http://docs.sun.com/app/docs/doc/817-5093/gafmj?a=view ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:3","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Growing a UFS We can use growfs command to change a UFS size provided that when creating the file system we use the -T option with newfs command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:4","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we create a partition in a disk, the layout we specified will be stored in the partition table area of the hard disk located in the beginning of the disk. The partition table contains all information like partition type, and its size which we specify when we create the partition. A good place to learn more about partitioning in general is http://en.wikipedia.org/wiki/Disk_partitioning ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:9:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next recipe we will learn how we can mount and put the partitions we created here in use. 3. Mounting and unmounting file systems Before we can use a file system we should mount the slices or partitions we created in the operating system. When we mount a partition or a slice we are making it accessible trough a directory which it is mounted to and we unmount the file system we are just detaching the file system from OS. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:10:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In previous recipes we create two partitions and applied a file system on each one of them. In this recipe we will mount them to some directories and start using them. To mount a file system we need a mount point. A mount point is a directory which we use it to access the mounted file system. So let’s create the mount point before mounting the partitions. \\# mkdir /mnt/fat /mnt/sol This command creates two directories named fat and sol inside the /mnt directory. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:11:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To mount the Solaris2 partition we created in previous recipe we can use the mount command as follow: \\# mount /dev/dsk/c9t0d0s2 /mnt/sol This command mounts the Solaris2 slice to /mnt/sol and we can create files and directories inside it using its mount point. For example: \\# mkdir /mnt/sol/sample If we get the list of /mnt/sol/ we will see something like the following figure. Mounting the FAT32 partition is a bit trickier than Solaris2 partition as you can see in the command below which mounts the FAT32 partition we created before to /mnt/fat. \\# mount -F pcfs /dev/dsk/c9t1d0p1 /mnt/fat As you can see we are passing the file system using -F parameter to let the command know which file system is applied on the partition. Using the following command we can create a text file containing “Hello FAT32 Partition” inside the FAT32 partition. \\# echo Hello FAT32 Partition \\\u003e /mnt/fat/sample.txt We can check partition content using the ls command as we used for Solaris2 partition. When we no longer need the file system or we want to take it down for other operations like checking the file system or moving the drive we can use umount command to detach the file system from the operating system. The umount command operates both on the device name like /dev/dsk/c9t1d0p1 and on the mount point like /mnt/fat. For example to unmount the FAT32 partition we can use the following command \\# umount /mnt/fat We can not use the umount or mount command when the file system is busy, for example when an application like terminal is accessing the file system or just has a path inside that file system open. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:12:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we mount a partition, we are asking the operating system’s file system to allow us access the content of the mounted partition through the directory we specified as the mount point. Operating system forward any changes we made to that mount point into the underlying partition ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:13:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Mounted file systems using mount command stays mounted for the period of the current operating system session and will not persist the system reboot and after each reboot we will need to mount them again. We can define the mount points in the mount table inside the /etc/vfstab file to let OpenSolaris pick them an execute them. Each row of the mount table represents one mount command. The /etc/vfstab is a plain text file with the following format. device device mount FS fsck mount mount to mount to fsck point type pass at boot options An example row for the mount table to mount our FAT32 partition is as follow: /dev/dsk/c9t1d0p1 /dev/rdsk/c9t1d0p1 /mnt/fat pcfs 5 yes - Let's see description of each field in the sample row of mount table. The block device name to mount. The raw device name to be used by fsck to check the disk. The mount point to mount the device. The file system type to mount the device under it. Whether the file system should be checked at startup or not. Whether to mount the device at boot or not. Additional options for mounting. Options like enforcing a read-only mount, enabling logging and so on. We applied no option. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Using fuser to see who is accessing a file system We can use fuser command to check which process is accessing the file system. This command comes handy when we want to unmount a device and umount throw us an error saying the device is busy. For example to check which process is using the /mnt/fat we can use the following command. \\# fuser /dev/dsk/c9t1d0p1 The above command results in something similar to the following figure if we have, for example a terminal window at /mnt/fat we will discuss fuser and pfiles, which gives us more details about who is using what part of the file system, in more details in chapter. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Mount options Following table shows important options we can use with mount command or in the mount table. Options Description nologging Disable the metadata operation logging. noatime Disable recording files access time. quota Enable disk space usage quotas. forcedirectio Force I/O requests to bypass the file system cache and therefore the operation can be faster for application with internal caches like Oracle and MySQL databases ## See also More information about the mount and umount commands is available in their man pages. And for the mount table we can refer to its official document at http://docs.sun.com/app/docs/doc/805-7228/6j6q7uev3?a=view 4. Monitoring UFS Monitoring is an integral part of all enterprise level activities and software and hardware systems are not an exception. OpenSolaris provide several utilities for monitoring the file system activities. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Following three commands are provided to monitor and check status of the file system. The df command report file system disk space usage. The du command summarizes disk usage of each directory recursively including the size of included files. The fsstat command reports kernel file operation activity by the file system type or by the mount point. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:15:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To view the amount of disk usage and free space on each disk (mount point) we can use the following command: \\# df -hl The result of using the command is shown in the following figure. To view how much space each file and directory occupied we can use the du command. For example to see how much space is used by /export/home/masoud/Documents and its subdirectories and files we can use the following command. \\# du -ah /export/home/masoud/Documents/ The command result is something like the following figure. We can use -b parameter to make the command show the size in bytes instead of wrapping it based on the block size. Another useful parameter is -s which summarizes the space consumption for the directory. Another monitoring command which we will discuss is fsstate which can show live statistics about file system usage. For example to see default monitoring factors for / mount point we can use the following command. \\# fsstat / 5 200 This command shows the statistics for 200 times and gathers the sampling data each 5 second. A sample output is like the following figure. In the resulting statistics we can see how many new files are created in the sample period, how many calls for getting and setting attributes placed, how many look-up, read, write, etc operations are performed. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:16:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. Commands that collect and shows statistics about the file systems uses multiple sources for the statistics they show including the operating system internal messages, the file system read-only properties and calculation they make to provide the users with sensible information. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:17:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. There are plenty of options which we can use to customise the result of each monitoring command. To available options and their usages are included in the man pages and short help of each command. There is another monitoring command we can use for monitoring IO operations called iostat. More information about iostat is available in its man pages. 5. Backing up and restoring UFS In previous recipe we saw how we can create, mount and monitor UFS partitions and file systems. In this recipe we will learn how to backup, and restore a UFS. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:18:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The command we use to create a backup is ufsdump and ufsrestore. These two commands can create a dump of a UFS like /mnt/sol and then restore it when required. We should backup a file system when it is not mounted or mounted in read-only mode to ensure that during the dump operation no file has changed. Prior to starting this recipe I copied some files into the UFS slice to make the backup and restore operations more sensible. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:19:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The ufsdump create backup of the file system. The dump can be an incremental dump only containing files changed after the previous dump or it can be a full dump containing all files. We can dump the file system into tapes or we can dump it into a binary file understandable for restore command. Following syntax shows how we can use ufsdump command. ufsdump \\[options\\] \\[list of arguments\\] files_to_dump In this command schema: The options are a single string of one-letter ufsdump options, some of important options are listed in this recipe table. The arguments may be multiple strings whose association with the options is determined by order. That is, the first argument goes with the first option that takes an argument and so on. An example of arguments is the -f parameter argument which specifies where to dump into. The files_to_dump is required and must be the last argument on the command line. For example we can create a full dump of a Solaris2 file system located on /dev/dsk/c9t0d0s2 and store it the FAT32 partition we mounted in /mnt/fat using the following command. \\# ufsdump 0fu /mnt/fat/dumped_here /dev/dsk/c9t0d0s2 The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Following figure shows the sample output of this command. Important list of options is shown in the following table. Option Description 0-9 Specify the dump level. Level 0 means a full dump while other numbers means incremental dump after the previous level. For example if we have a dump level 1 created on Friday, a dump level 1 on Monday will dump all changes happened between Friday and Monday. f Specify the file which we want to dump the dump files into it. This option need a argument in the argument list u Update the dump record. Add an entry to the file /etc/dumpdates, for each file system successfully dumped. v Verify the dump after created to ensure its integrity. c Set the defaults for cartridge instead of the standard half-inch reel L Sets the tape label to string, instead of the default value which is none. This option needs a string argument in the arguments list. The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Now to restore the dump we have just created we can use the ufsrestore command. The ufsrestore command works in interactive and headless mode. In the interactive mode it walk us through the procedure step by step and let us view a dump file content and select what to restore. An example of using the ufsrestore in interactive more is shown in the following figure. As you can see in the figure we can use some common commands like ls, add, and extract in the ufsrestore shell to get the content list, add our selected files to list of extractees and then extract them into the current working directory using the extract command of the ufsrestore shell. The following table explains the options used with the ufsrestore command above. To use the ufsrestore in the headless mode we should use the following syntax. ufsrestore \\[options\\] \\[space separated arguments\\] what_to_restore In this schema we have: The dump_file argument is path to the file we want to restore a file system content using it, for example /mnt/fat/dump_fileImportant. The what_to_restore argument is a path pointing to a directory or a file inside the dump file. The important options available to the ufsrestore command are included in the following table. Option Description i Enter the interactive shell of the ufsrestore command f Path to the dump file. The option need the file name to be passed in the argument list x Extract the named files into the current directory. For example if the named file is directory, the ufsrestore will restore that directory and its contents into the current working directory. r Extract the dump content recursively into the current working directory. Current working directory is where we are executing the command from. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other alternatives to ufsdump and ufsrestore There are some file system independent commands which we can use to create backup of a file system and then restore it. Some of these commands are as follow. The ufsdump command has its advantages like creating incremental backup, support for reels and so on over these commands. The tar command is a no compression achiever which can archive a file system. The cpio command similar to tar but with capabilities like sending the archive stream to a remove machine and support for different archive formats. The pax command is a combination of tar and cpio capabilities. The dd command is a low level coping command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating snapshot of a UFS We can use fssnap to create snapshot of a UFS. The created snapshot is read-only and won’t sustain a restart. We usually use these snapshots for creating backup of the mounted file systems. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. The ufsdump performs the dumping task by scanning the requested file system path and creating a list of its content and writing the list into the specified media. Then it goes through the list and read each file addressed in the list and store its content in the specified media. Similarly the ufsrestore read the list, creates the directory layout and write the file contents into the media specified as restore target. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:21:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In addition to man pages we can learn more the ufsdump and ufsrestore commands in their reference documents available at http://docs.sun.com/app/docs/doc/817-3814/6mjcp0r5h?a=view 6. Checking UFS for errors UFS like any other file system can get corrupted in the block level. We can find the UFS corruptions and to some extend recover from those corruptions or prevent them from happening. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:22:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will mostly focus on using fsck command to check a disk and repair the damage if possible. I assume that we have some UFS partition like /dev/rdsk/c9t0d0s2 in our disposal to study on. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:23:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The fsck command works on raw devices, we discussed raw and block devices in the Formatting and Labelling recipe, so we should use the raw name of a device to check for errors. As you remember from the Mounting and unmounting recipe of this chapter, OpenSolaris check the file systems for error on start-up if configured. That checking option kicks tart the fsck command when necessary. We should not run this command periodically nor should we execute in on a mounted file system. More importantly, we should not use this command on extra large disks as it may take days to complete. Following command checks the /dev/rdsk/c9t0d0s2 and interactively asks for our permission to perform sensitive operations. \\# fsck /dev/rdsk/c9t0d0s2 As you can see in the above command we passed the raw device instead of the block device name to this command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:24:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. UFS uses several mechanism to prevent and recover from corruptions, such as logging which we discussed in Mounting and unmounting recipe. The fsck uses the output of these mechanism to check and if required try to fix the file system. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:25:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can try the man page and view the short help message to find more about this utility using the following commands. \\# man fsck \\# fsck \\--help 7. Specifying disk quotas on UFSs Like any other advanced file system supports space quota meaning that we can specify a space usage limitation for users on each file system. The disk usage limitation can be enforced for a single user or a group of users over any UFS like /mnt/sol. We can enforce the limitation over number of consumed blocks and Inodes. Each block, depending on its size, can contain 4 KB, 16 KB or any value specified during the file system creation. So, limiting the number of blocks limits the amount of data which can be stored in the file system. Each Inode contains a reference to a file or a directory in the file system. Limiting number of Inodes limits the number of files and directories which can be created. Before we get down to the business, we should understand basics that the disk quota operates on. Following three terms explains these basics. Soft limit: maximum amount of disk space that a user or a group of users can use. The disk usage by the user or the group can be exceeded for a certain amount of time. This amount of time is called Grace Period. Grace Period: A period of time which the soft limit may stay exceeded by a user or a group of users. The grace period can be specified in seconds, minutes, hours, days, weeks, or months. This period of time let the user to get below the soft limit. Hard limit: Specifies a hard limit for the user or group disk usage. This limitation cannot be exceeded in the Grace Period. For each quota we specify one line of information is recorded in the quotas file with the following structure. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:26:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Each file system needs to have a file named quotas owned by root in the top level. We use edauota command to edit this file and include the quota for users and groups of of the system. By default OpenSolaris will use vi to open the quotas file for editing but vi may not be a convenient editor for new users. We can change the default editor of the system by changing the value of EDITOR environment variable by using the following command: \\# export EDITOR=gedit This command change the default editor to gedit which is the default OpenSolaris desktop editor. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:27:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. Assuming that we want to define quota over /mnt/sol for a a user named masoud we need the following steps: Create the quotas file in the root of /mnt/sol \\# touch /mnt/sol/quotas Changing the ownership of the file to root \\# chmod 0600 /mnt/sol/quotas Specifying the quota using edquota command using the following command. \\# edquota masoud When we execute the above command, gedit window opens with content similar to the following figure. To change the quota values, we just need to change the numbers and press CTRL+S to save the file and exit gedit. After specifying the quotas we should turn quotas on if we have not turned it on using the mount table. Following commands turn quotas on for the given file system. \\# quotaon /mnt/sol To turn off the quota for a file system we can use the following command. \\# quotaoff /mnt/sol Now we can get report on quotas using the quota and repquota commands. For example to check quota for masoud on current file system we can use the following command: \\# quota -v masoud Output for this command can be similar to the following figure. To enable the quotas for a file system permanently, persistent across reboot, we can use the mount table options. For example we can add the following line to mount table by editor /etv/vfstab to enable quota for the /mnt/sol /dev/dsk/c9t0d0s2 /dev/rdsk/c9t0d0s2 /mnt/sol ufs 5 yes rq We used the last column which specifies mount options to enable quota for this file system. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:28:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we enable the quotas for a file system, Operating system consult the quota definitions before storing anything in the file system to see whether the user has the right to write some data into the disk according to his disk usage or not. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:29:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Varieties of commands are available to get more out of quotas. We discuss some of these commends here. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:30:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other quotas reporting commands We can use repquota to view the quota details for a file system. For example to get detailed report on /mnt/sol file system quotas we can use the following command. \\# repquota /mnt/sol Or to view quota information for all users for all file systems we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e repquota -av\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e ### Specifying quotas for group of users or multiple users ### Specifying the grace period We can use edquota to specify the grace period for each user. To open the quota editor for grace period we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e edquota -t masoud\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e In the editor we just need to specify two numbers for the grace period of blocks and inodes. ## See also Looking at the quota, quotaon, repquota, quotacheck, quotaoff man pages can increase our understanding of quotas in general and these commands in particular. We can also check UFS management documentation located at http://docs.sun.com/app/docs/doc/817-0403/sysresquotas-97946?a=view # 8. Getting list of ZFS pools In the introduction section of this chapter we said that all ZFS file systems are build on top of storage pools. So, to work with ZFS, the first thing to lean is storage pools. In this recipe we will learn how to get information about available storage pools using zpool command. ## Getting ready In this recipe we will work on viewing the available zpools, you may find that the result of my commands are different than what you will see in your console, this difference can be caused by the fact that I have more than one pool created while you may only have the rpool which is created when installing OpenSolaris. ## How to do it.. Following command shows a list of available pools in the system along with their basic properties\\' values. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e The result for the executing the command can be similar to the following figure. ![](post-img/3180_02_14.png) The list subcommand can show one or more specific pool\\'s properties when we pass them as its arguments. For example to see information about rpool we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list rpool\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e Each pool has some more specific attributes which we can view or edit their values using zpool list command. For example the following command will show values for the given listed properties of rpool. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:30:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"This is the 4th installment of my entries covering NIO.2. In this entry I will discuss more about what we can do with attributes and permissions. The NIO.2 lets us write the permissions and attributes of a file in addition to reading them. For example we can change the rwx permissions using the Attributes utility class and PosixFilePermission. You can execute the following sample code with almost no changes, the only thing you need to change is the path to our sample file and then you are ready to see how it works. import java.io.IOException; import java.nio.file.\\*; import java.nio.file.attribute.\\*; import java.util.\\*; public class Perm2 { public static void main(String args\\[\\]) throws IOException { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We should have enough permisison to change the owner otherwise it will // throw a FileSystemException: /home/masoud/a: Operation not permitted sort of thing Attributes.setOwner(p, up); //another way to read and write the owner value for a file is using FileOwnerAttributeView FileOwnerAttributeView fo = p.getFileAttributeView(FileOwnerAttributeView.class, LinkOption.NOFOLLOW_LINKS); fo.setOwner(up); //Now that we have changed the permissions lets see the permissions again: att = (Map) p.readAttributes(\"posix:permissions\", LinkOption.NOFOLLOW_LINKS); System.out.println(\"New Permissions:\" + \": \" + att.get(\"permissions\")); } } Let’s see what we have done starting from top: at the beginning we have just initialized the FileSystem object and got a file reference to /home/masoud/dl. Then we read all of this files basic and posix attributes using the attributes view notation. The following sample code shows this step FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); Next step we have use the PosixPermissions class to create a permission set using the OS permissions presentation and then applied these permissions on our file // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); In the next step we created a user principal for a user named masoud and changed the ownership of our file to this user. We have done this using both available methods. // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We s","date":"2010-07-13","objectID":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 4: Changing File System Attributes and Permissions","uri":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction In this chapter we are going to cover basic networking capabilities of OpenSolaris. While we will some of common utilities in the recipes, we will learn some more trivial ones here. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:0:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning netstat command The netstat command is well known for checking the active connections status in a system but it can provide a fair deal of other diagnostics. Following sample command shows some of the netstat use cases. List all ports: \\# netstat -a \\| more Inspecting an interface: \\# netstat -I e1000g0 Viewing the routing table: \\# netstat -rn ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:1:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning ping command We can use ping, which uses Internet Control Message Protocol (ICMP) packets, to check whether a target address is accessible or not. For example to check whether 10.0.2.24 is accessible for us, we can use \\# ping 10.0.2.24 When we check whether we have internet connection or not, it is better to ping an IP address instead of a domain name because pinging a domain require a Domain Name System (DNS) resolution which may not be possible when we have DNS failure. For example, we can use following command to check whether we have internet access or not. \\# ping 4.2.2.4 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:2:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning traceroute command The traceroute command allows us to see which hops a packet goes through before it reaches its destination. By examining the hops we can determine which one of our network components causes delay or even drops the packet or has no route to destination address. For example: \\# traceroute -n 4.2.2.4 \\# traceroute -n [www.google.com](http://www.google.com/) We usually use the -n parameter to prevent DNS resolution of intermediary hops to accelerate the overall process or because we do not have a DNS server configured for our network or the specific connection we are examining. 1. Listing network Interfaces In this recipe we will see how we can get a complete list of all plumbed network interfaces using the ifconfig command. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:3:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready As we are using a command line interface (CLI) utility to get the list of available interfaces we should execute it in the console environment. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:4:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Following command shows how we can get list of all available interfaces. \\# ifconfig -a Following figure shows the output for the above command. As we can see in the figure, the command lists all interfaces including the loopback interface and Ethernet ones. In addition the sample command we used lists both IPv4 and IPv6 enabled interfaces and separately show their addresses. OpenSolaris follows a default naming schema for network interfaces it detects in the system. Basically all Ethernet interfaces’ names starts with e100 or e1000 as the prefix and then g to gN as the distinguisher which distinguish each instance of the installed Ethernet card with another using an integer number. All wireless card names are prefixed with wpi followed by a number indicating the card instance number. Loopback interfaces are named with lo and then followed by a distinguisher number. There are more schemas for other cards like pcnN, bgeN, switch which we will discuss in next recipes. Variety of details might be shown in the flags area depending on the network interface driver. Some of most important flags are as follow: The UP flag indicates that interface sends and receives packets. The NOTRAILERS flag indicates that no trailer included at the end of the Ethernet frame. The RUNNING flag indicates that interface is recognized by the kernel. The MULTICAST flag indicates that the interface supports a multicast address. We can change an interface name to a more meaningful and human-readable name. Detailed instructions about renaming an interface is available at http://www.opensolaris.com/use/Migrating.pdf The ifconfig command can list the interfaces which are already plumbed (available for the kernel to use). If an interface is unplumbed, the command will not show it. We can use dladm command to list all plumbed and unplumbed interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:5:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The command reads the network configurations from network configuration files located at /etc/sysconfig/network-scripts/ and /etc/hostname.interface_name files or directly from memory when we do not have any of our interfaces configuration persisted. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:6:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… The ifconfig command is one of the most useful and handy commands when we deal with network interfaces and network configuration. A complete set of its parameters and options is accessible in the man pages or using the short help format which is as follow \\# ifconfig --help ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting information for an specific network interface If we know our interface name and we want to see its detailed information we can pass the interface name to ifconfig command. For example to see detailed information for e1000g0 we can use the following command. \\# ifconfig e1000g0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:1","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other commands for listing network interfaces We can use some other commands like dladm to list the available network interfaces. dladm can give more details about an interface including speed, state and so on. For example the following command shows the dladm command usage. The dladm shows both plumbed and unplum-ed interfaces in contrast with ifconfig which only shows plumbed interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:2","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 2 and recipe three of this chapter discuss how we can configure the network interface in OpenSolaris. Reading them is recommended to grasp better understanding of OpenSolaris networking. 2. Configuring a network interface to obtain IP from a DHCP Server In chapter 1 recipe we discussed how we can use Network Auto-Magic to configure automatically configure the network interfaces. Now in this recipe we will discuss how we can configure the interface manually without using the NWAM. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:8:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To configure a network interface manually we should disable the NWAM using the following commands. \\# svcadm disable network/physical:nwam Do not worry if you cannot understand meaning of the two commands. Simply accept that the svcadm disable the nwam service. In chapter recipe we will discuss svcadm in more details. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:9:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The ifconfig command we discussed in the previous recipe is our tool for configuring the network interfaces the way we want. Following command enable the Dynamic Host Configuration Protocol (DHCP) client layer for the e1000g0 network card. \\# ifconfig e1000g0 dhcp start Following figure shows how the new IP address is applied on e1000g0 interface. The command assumes that the interfaces are already plumbed, known to kernel. If not we should plumb the interface using the following command: \\# ifconfig e1000g0 plumb When we configure an interface manually we the configuration will not survive the reboot unless we make the configuration persistence. To make the configuration for e1000g0 persistent should create two files in the file system as follow: To ask the OS to configure the interface we should create /etc/hostname.e1000g0. The extension specify the interface we want the network/physical:default to configure the interface on boot-up. To ask network/physical:default service to use DHCP to assign an IP address to this interface we should create /etc/dhcp.e1000g0. We can create these files using the following command or using normal text editors like gedit. \\# touch /etc/hostname.e1000g0 /etc/dhcp.e1000g0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:10:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we stop the network/physical:nwam service, the OS check whether any configuration files for each one of the interfaces exists or not, if it exists OS will configure the interface accordingly and otherwise the interface will be left un-configured. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:11:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… Sometimes we want to get a new IP address for our interface or we just do not need the assigned IP anymore and we want to release it to the pool. Following command release the IP assigned to our interface back to the pool \\# ifconfig e1000g0 dhcp release The command formally send an information packet to DHCP server telling it that the IP address is no longer required and then stop the DHCP client. Another way which is not recommended is dropping the IP addressee without letting the server know that we are not using the address anymore. The DHCP server will eventually take the address back into the pool but it will take some times based on the DHCP server configuration \\# ifconfig e1000g0 dhcp drop ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:12:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next recipe we will see how we can configure an interface with a static IP instead of using DHCP. I recommend you review it to get more information about how network interfaces work in OpenSolaris. 3. Configuring network interfaces with static IP address Sometimes there is no DHCP server around for us to acquire an IP address from its pool or we just want to use a static IP address for our interface. Some use cases for static IP address can be the router, network gateway or our database server. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:13:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To configure a network interface manually we should disable the NWAM using the following commands. \\# svcadm disable network/physical:nwam Do not worry if you cannot understand meaning of the two commands. Simply accept that the svcadm disable the nwam service. In chapter recipe we will discuss svcadm in more details. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:14:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The ifconfig command we discussed in recipe 1 and recipe 2 of this chapter provides us with required options to assign a static IP address to a network interface. Following command assign 192.168.1.110 to e1000g0 with 255.255.255.0 as its subnet mask. \\# ifconfig e1000g0 inet 192.168.1.110/24 Before we could use the interface we should bring it up, to bring up the interface we can use the following command: \\# ifconfig e1000g0 inet up The IP_Address/Bit_count syntax specifies the IP address we want to assign to the interface and the number of bits associated with the subnet mask. In the subnet mask, 24 means that first 24 bits of the subnet mask should be 1 and the remaining 8 bits should be 1. Following figure shows how new IP address is assigned to e1000g0 interface. This configuration cannot survive a system reboot unless we persist it using the network interface declaration files. To ask the network/physical:default service to configure an interface using an static IP address we should create /etc/host_name.interface_name file and place the configuration information inside it. For example to configure the e1000g0 interface with the same configuration we used above we should create /etc/hostname.e1000g0 file and place the following line inside it. 192.168.1.110/24 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:15:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When OpenSolaris is booting, the network/physical:default check for any available file with a known interface as its extension it execute the ifconfig interface_name inet \u003ceach_line_of_the_file\u003e commands sequentially to configure the interface and finally it executes the ifconfig interface_name inet up to bring the interface online. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:16:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… To bring down an interface we can use the following command: \\# ifconfig interface_name inet down This command takes the interface offline. To remove the persisted configuration for an interface we should remove the content of its configuration file or completely remove the corresponding file from the system. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:17:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also First two recipes of this chapter discuss the network interfaces and using DHCP to configure an interface. We can read them to get more information about the network interfaces and configuring them. 6. Configuring FTP Server The FTP (File Transfer Protocol) is one of the widely used services to distribute files publicly. OpenSolaris comes with both FTP client, ftp command, and FTP server, ftpd daemon. In this recipe we setup an FTP server and use the ftp command to access the FTP server content. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:18:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use a console window or a virtual console to invoke necessary commands to enable the FTP server. So open the console window by executing gnome-terminal command in the Run Application (ALT+F2) window before getting down to business. We also need to have a ZFS pool available to us, in our sample commands I used fpool as the practicing pool while you can use any other pool you have previously created. Creating ZFS pools is discussed in recipe 9 of chapter 2. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:19:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… First we should create a file system for the FTP account and shared FTP content. We can do it using the following commands \\# zfs create fpool/ftp_fs \\# ftpconfig /fpool/ftp_fs So far, we shared a file system for our FTP users but we did not specify which users can access this shared content. The default configuration of OpenSolaris allows everyone to access the FTP server content using the anonymous access. Following command the FTP user details. \\# grep ftp /etc/passwd Now we can enable the FTP service using the following command. FTP server is managed by SMF as a service available at svc:/network/ftp. \\# svcadm enable ftp Following figure shows what we can expect to see when we execute each of the above commands. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:20:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Majority of setting up FTP server tasks is automated and the ftpconfig script is responsible for doing it. What happens when we invoke the command is summarized below: A user is created with ftp only access The script enriches the file system we specified with binary files required. For example the ls command is copied into the /fpool/ftp_fs/bin to ensure that FTP user can execute it to get the list of files. Later on it write down the configuration required for the FTP service so when we run the service, clients can access the /fpool/ftp_fs/pub trough the FTP server. Following figure shows content of the /fpool/ftp_fs/ which is created by ftpconfig script. As we can see in the figure, all directories are restricted for the root access except the pub directory which will contain the files we want to share over the network. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:21:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 10 of this chapter is recommended to see how we can setup Secure Shell (SSH) to ensure the data transfer confidentiality. 7. Configuring DHCP Server ==Done A Dynamic Host Configuration Protocol (DHCP) server leases IP address to clients connected to the network and has DHCP client enabled on their network interface. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:22:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can setup a start the DHCP server we need to install DHCP configuration packages. Detail information about installing packages in provided in recipe of chapter 1. But to save the time we can use the following command to install the packages. \\# pkg install SUNWdhcs After installing these packages we can continue with the next step. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:23:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… First thing to setup the DHCP server is creating the storage and initial settings for the DHCP server. Following command does the trick for us. \\# dhcpconfig -D -r SUNWfiles -p /fpool/dhcp_fs -a 10.0.2.254 -d domain.nme -h files -l 86400 In the above command we used several parameters and options, each one of these options are explained below. The -D specifies that we are setting up a new instance of the DHCP service. The -r SUNWfiles specifies the storage type. Here we are using plain-text storage while SUNWbinfiles and SUNWnisplus are available as well. The -p /fpool/dhcp_fs specifies the absolute path to where the configuration files should be stored. The -a 10.0.2.15 specifies the DNS server to use on the LAN. We can multiple comma separated addresses for DNS servers. The -d domain.nme specifies the network domain name. The -h files specifies where the host information should be stored. Other values are nisplus and dns. The -l 86400 specifies the lease time in seconds. Now that the initial configuration is created we should proceed to the next step and create a network. # dhcpconfig -N 10.0.2.0 -m 255.255.255.0 -t 10.0.2.1 Parameters we used in the above command are explained below. The -N 10.0.2.0 specifies the network address. The -m 255.255.255.0 specifies the network mask to use for the network The -t 10.0.2.1 specifies the default gateway All configurations that we created are stored in DHCP server configuration files. We can manage the configurations using the dhtadm command. For example to view all of the current DHCP server configuration assemblies we can use the following command. \\# dhtadm -P This command’s output is similar to the following figure. Each command we invoked previously is stored as a macro with a unique name in the DHCP configuration storage. Later on we will use these macros in subsequent commands. Now we need to create a network of addresses to lease. Following command adds the addresses we want to lease. \\# pntadm -C 10.0.2.0 If we need to reserve an address for a specific host or a specific interface in a host we should add the required configuration to the system to ensure that our host or interface receives the designated IP address. For example: \\# pntadm -A 10.0.2.22 -f MANUAL -i 01001BFC92BC10 -m 10.0.2.0 -y 10.0.2.0 In the above command we have: The -A 10.0.2.22 adds the IP address 10.0.2.22. The -f MANUAL sets the flag MANUAL in order to only assign this IP address to the MAC address specified. The -i 01001BFC92BC10 sets the MAC address for the host this entry assigned to it. The -m 10.0.2.0 specifies that this host is going to use the 192.168.0.0 macro. The –y asks the command to verify that the macro entered actually exists. The 10.0.2.0 Specifies the network the address is assigned to. Finally we should restart the DHCP server in order for all the changes to take effect. Following command restarts the corresponding service. \\# svcadm restart dhcp-server ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:24:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we setup the DHCP service, we store the related configuration in the storage of our choice. When we start the service, it reads the configuration from the storage and wait dormant until it receives a request for leasing an IP address. The service checks the configuration and if an IP was available for lease, it leases the IP to the client. Prior to leasing the IP, DHCP service checks all leasing conditions like leasing a specific IP address to a client to ensure that it leases the right address to a client. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:25:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can use the DHCP Manager GUI application to configure a DHCP server. The DHCP manager can migrate the DHCP storage from one format to another. To install the DHCP manager package we can use the following command. \\# pkg install SUNWdhcm Now we can invoke the DHCP manager using the following command which opens the DHCP Manager welcome page shown in the following figure. \\# dhcpmgr ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:26:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 5, 6 of this chapter recommended understanding IP assignment better. 8. Assigning multiple IP address to an interface Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:27:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We should have one network interface configured in our system in order to create additional logical interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:28:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We are going to add a logical interface to e1000g1 interface with a 10.0.2.24 as its static IP address. Before doing so let’s see what network interface we have using the ifconfig command. Now to add the logical interface we only need to execute the following command: \\# ifconfig e1000g1 addif 10.0.2.24/24 up Invoking this command performs the following tasks: Create a logical interface named e1000g1:1. The naming schema for logical interfaces conforms with interface_name:logical_interface_number which the number element can be from 1 to 4096. Assign 10.0.2.24/24 as its IP address, net mask and broadcast address. Bring up the interface. Now if we invoke ifconfing -a command the output should contain the logical interface status as well. The following figure shows a fragment of ifconfig -a command. Operating system does not retain this configuration over a system reboot and to make the configuration persistent we need to make some changes in the interface configuration file. For example to make the configuration we applied in this recipe persistent the content of /etc/opensolaris.e1000g1 should something similar to the following snippet. 10.0.2.23/24 addif 10.0.2.24/24 The first line as we discussed in recipe 3 of this chapter assign the given address to this interface and the second like adds the logical interface with the given address to this interface. To remove a logical interface we can simply un-plumb it using the ifconfig command as shown below. \\# ifconfig e1000g1:1 unplumb ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:29:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a logical interface, OpenSolaris register that interface in the network and any packet received by the interface will be delivered to the same stack that handles the underlying physical interface. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:30:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2 and 3 of this chapter is recommended as they discuss how to configure a network interface with DHCP or how to manually assign a static IP address to a network interface. The next recipe can be another source for learning about Virtual Network interfaces capability in OpenSolaris which allows us to add network interfaces to the system using an available physical interface. 9. Configuring virtual LAN interfaces Here in this recipe we will see how we can create multiple link layer virtual interfaces on top a physical network interface. Each one of the VLANs is accessible from outside and we can configure them to be used in our services and applications. For example, we can configure our DHCP server to use a VLAN or we can run GlassFish server on it. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:31:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can proceed with this recipe we should at least have one network interface installed in our system. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:32:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can create Virtual Interfaces using dladm command which is the utility we will use anytime we need to manage some data link layer entities. Following command shows how we can create a Virtual LAN interface (VLAN). \\# dladm create-vlan -l e1000g0 -v 1 vlan0 This command creates a VLAN on top of e1000g0 interface with 1 associated as its distinguisher tag. We will discuss distinguisher tags in more details in how it works section. Following command lists all available interfaces either configured or not. The shown interfaces include both virtual and physical interfaces. \\# dladm show-link Now that the vlan0 is created we should plumb it so operating system detects it. After plumbing we can configure it like any other interface. We can assign its IP address manually or we can configure it to acquire an IP address from a DHCP server. \\# ifconfig vlan0 plumb \\# ifconfig vlan0 inet 192.168.1.110/24 Following figure shows steps the step we described and the system response to each step. Although the result of dladm command invocations like creating a VLAN is persisted automatically but the ipconfig commands effect will not persist unless we use the interface configuration files as discussed in the recipe 2 and recipe 3 of this chapter. We can delete a VLAN using delete-vlan subcommand of dladm command as shown bellow. \\# dladm delete-vlan vlan0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:33:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a VLAN, operating system register its availability and we can see the interface using dladm command. Each VLAN has a distinguished numerical tag which we specified it using the -v option along with a unique name. The name we specified let us to manage and administrate the interface while the specified tag makes it possible for the OS to separate packets intended for this interface in the link layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:34:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 6,5 and 8 in this chapter is recommended to grasp better understanding of network interfaces and IP address allocation. 10. Using IP multipathing and link based failure detection OpenSolaris provides some features to enhance network reliability including IP Multipathing. In IP multipathing multiple network interfaces installed in the system forms a group of network interfaces and handles each other’s load when one of them fails. Each interface in the group can be active or standby, meaning that the interface is either handling the traffic or it is reserved to get in and handle the traffic when one of the active interfaces fails. When we configure a group and we look for network reliability and the reliability requires that system detect the failed interface and let another interface handles its load. Two failure detection mechanisms are supported by OpenSolaris which are as follow: The probe based, active, failure detection: In active failure detection model, OS constantly probe the interface by sending a probe packet to an exclusive address provided by the interface for failure detection. If system does not receives a reply for its probe packet the interface will be considered faulty. The link based, passive, failure detection: This model is supported by network interface driver and is the default model OpenSolaris uses. In this model the network interface driver set a flag for the network interface indicating that the interface failed. In this recipe we configure IPMP group with two active network interface members using the passive failure detection. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:35:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can create an IP multipathing group we should make sure that we met the following criteria. Each interface must have a unique MAC address. Each interface must be configured with a static IP address. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:36:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We will use ifconfig utility for our entire configuration as we are dealing with network layer. In our sample commands we will use e1000g0 and e1000g1 as the two interfaces which form the sm_group. Fowling figure shows fragment of ifconfig -a command prior to forming the sm_group. ![](post-img/=“media/image12.png” style=“width:5.5in;height:0.84722in” /\u003e insert image 3180_04_10.png As we can see there is nothing indicating that these interfaces are member of a group and as specified in the figure we two interfaces uses static IP addresses. Following command will make the group and joins the e1000g0 to the group. One interface can only join one group. \\# ifconfig e1000g0 group sm_group The first interface joining a group cause the group to be created and when last interface leave the group, system delete the group afterward. Following command joins the e1000g1 to the group. \\# ifconfig e1000g1 group sm_group Now if we check the status of our network interfaces we see a slight change in the e1000g0 and e1000g1 interface statuses as shown in the following figure. Like all network configuration we discussed in recipe and 3, the configuration we just made is not persistent over a reboot and we need to add some changes in the network interface configuration files to make the changes persistent on reboot. In recipe 2 and 3 we discussed how to configure static IP address for an interface and make that configuration persistent over a reboot using hostname.interface_name files stored in /etc/ directory. To make the grouping configuration persistent we just need to add the group command followed by group_name in the configuration line. For example to configure the e1000g0 we should have a /etc/hostname.e1000g0 file with the following content: 10.0.2.21/24 group sm_group To remove an interface from a group we just need to invoke grouping command with no group name. For example to remove e1000g0 from sm_group we can use the following command: \\# ifconfig e1000g0 group “” ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:37:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a multipathing group, OpenSolaris starts probing all interfaces by ICMP packets. When an interface fails to respond to the ICMP packet, the interface RUNNING flag clears and the IP address(es) assigned to the faulty interface get assigned to either a standby interface or to an interface with smallest number of IPs. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:38:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2, 3 and 11 in this chapter are recommended to learn more about networking in OpenSolaris. The IPMP-Tutorial Sneak Preview available at http://sun.systemnews.com/articles/144/1/Solaris/22782 is recommended for getting more insight into the IPMP. More detailed reference is available System Administration Guide: IP Services located at http://docs.sun.com/app/docs/doc/816-4554/ipmptm-1?a=view 11. Configuring Link Aggregation (Ethernet bonding) Link aggregation or commonly known Ethernet bonding allows us to enhance the network availability and performance by combining multiple network interfaces together and form an aggregation of those interfaces which act as a single network interface with greatly enhanced availability and performance. When we aggregate two or more network interfaces, we are forming a new network interface on top of those physical interfaces combined in the link layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:39:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We need to have at least two network interfaces in our machine to create a link aggregation. The interfaces must be unplumb-ed in order for us to use them in a link aggregation. Following command shows how to unplumb a network interface. \\# ifconfig e1000g1 down unplumb We should disable the NWAM because link aggregation and NWAM cannot co-exist. \\# svcadm disable network/physical:nwam The interfaces we want to use in a link aggregation must not be part of virtual interface; otherwise it will not be possible to create the aggregation. To ensure that an interface is not part of a virtual interface checks the output for the following command. \\# dladm show-link Following figure shows that my e1000g0 has a virtual interface on top of it so it cannot be used in an aggregation. To delete the virtual interface we can use the dladm command as follow \\# dladm delete-vlan vlan0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:40:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The link aggregation as the name suggests works in the link layer and therefore we will use dladm command to make the necessary configurations. We use create-aggr subcommand of dladm command with the following syntax to create aggregation links. dladm create-aggr \\[-l interface_name\\]\\* aggregation_name In this syntax we should have at least two occurrence of -l interface_name option followed by the aggregation name. Assuming that we have e1000g0 and e1000g1 in our disposal following commands configure an aggregation link on top of them. \\# dladm create-aggr -l e1000g0 -l e1000g1 aggr0 Now that the aggregation is created we can configure its IP allocation in the same way that we configure a physical or virtual network interface. Following command plumb the aggr0 interface, assign a static IP address to it and bring the interface up. \\# ifconfig aggr0 plumb 10.0.2.25/24 up Now we can use ifconfig command to see status of our new aggregated interface. \\# ifconfig aggr0 The result of the above command should be similar to the following figure. To get a list of all available network interfaces either virtual or physical we can use the dladm command as follow \\# dladm show-link And to get a list of aggregated interfaces we can use another subcommand of dladm as follow. \\# dladm show-aggr The output for previous dladm commands is shown in the following figure. We can change an aggregation link underlying interfaces by adding an interface to the aggregation or removing one from the aggregation using add-aggr and remove-aggr subcommands of dladm command. For example: \\# dladm add-aggr -l e1000g2 aggr0 \\# dladm remove-aggr -l e1000g1 aggr0 The aggregation we created will survive the reboot but our ifconfig configuration will not survive a reboot unless we persist it using the interface configuration files. To make the aggregation IP configuration persistent we just need to add create /etc/hostname.aggr0 file with the following content: 10.0.2.25/24 The interface configuration files are discussed in recipe 2 and 3 of this chapter in great details. Reviewing them is recommended. To delete an aggregation we can use delete-aggr subcommand of dladm command. For example to delete aggr0 we can use the following commands. \\# ifconfig aggr0 down unplumb \\# dladm delete-aggr aggr0 As you can see before we could delete the aggregation we should bring down its interface and unplumb it. In recipe 11 we discussed IPMP which allows us to have high availability by grouping network interfaces and when required automatically failing over the IP address of any failed interface to a healthy one. In this recipe we saw how we can join a group of interfaces together to have a better performance. By grouping a set of aggregations we can have the high availability that IPMP offers along with the performance boost that link aggregation offers. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:41:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The link aggregation works in layer 2 meaning that the aggregation groups the interfaces in layer 2 of network where network packets are dealt with. In this layer the network layer’s packets are extracted or created with the designated IP address of the aggregation and then are delivered to the lower, higher layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:42:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2 and 3 in addition to recipe 8 and 9 is recommended. 12. IP tunnelling An IP tunnel connects one machine to another over a network without the two computers being aware of the intermediate network. The most basic use of IP tunnels is to securely connect two networks through two endpoint interface over an unreliable network. This use case is called Virtual Private Network (VPN). In OpenSolaris the point to point connection is created using a specific type of interface called IP Tunnel. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:43:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We should have a network connection between the two computers we want to create the point to point connection. We can use ping command to check the connectivity between our computers before proceeding with the tunnel setup. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:44:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The IP Tunnel interfaces are configured using ifconfig command. Following sequence of commands creates a tunnel between 192.168.1.110 which is our local machine and 66.10.12.1 which is the server. We need to make the interface available to the kernel. \\# ifconfig ip.tun0 plumb Now we should configure the tunnel itself. A tunnel has a source and a destination address at minimum. \\# ifconfig ip.tun0 tsrc 192.168.1.110 tdst 66.10.12.1 thoplimit 60 And finally we should bring up the interface. \\# ifconfig ip.tun0 up In the second step of creating the tunnel we used several parameters which are described below. The ip.tun0 specifies the IP tunnel interface name. The interface name format is ip.tun# which the digit specifies a 0-4096. The tsrc 192.168.1.110 specifies the tunnel source or the client machine IP address. When we are behind firewall we need to adjust the firewall to let IP protocol 41 packet pass. The tdst 66.10.12.1 specifies the tunnel destination or server machine. The destination machine must have the service running so we could connect to it. The thoplimit 60 specifies the maximum number of intermediately hops allowed between the client and the server to form the connection. We could use one single command to create, configure and bring up the interface. ifconfig ip.tun0 plumb ip.tun0 tsrc 192.168.1.110 tdst 66.10.12.1 thoplimit 60 up We used similar syntax in recipe 11 as well. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:45:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In this recipe we only setup the tunnel between two machines, for two networks to connect over a tunnel we should define some routing elements. In chapter 5 recipe # we discuss how we can setup routing tables. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:46:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In two previous entries I covered Introducing NIO.2 (JSR 203) Part 1: What are new features? and Introducing NIO.2 (JSR 203) Part 2: The Basics In this entry I will discuss Attributes introduced in NIO.2. Using attributes we can read platform specific attributes of an element in the file system. For example to hide a file system in DOS file system or to check the last access date of a file in a UNIX machine. Using NIO.2 we can check which attributes are supported in the platform we are running on and then we can decide how to deal with the available attributes. Following sample code shows how we can detect the available attributes and then how to manipulate them. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); //checking available attributes: Set\u003cString\u003e supportedViews = fs.supportedFileAttributeViews(); //We always have at least one member in the set, the basic view. BasicFileAttributes ba = p.getFileAttributeView(BasicFileAttributeView.class, LinkOption.NOFOLLOW_LINKS).readAttributes(); //Printing some basic attributes System.out.println(p.toString() + \" last access: \" + ba.lastAccessTime()); System.out.println(p.toString() + \" last modified \" + ba.lastModifiedTime()); // As I know I am in NIX machine I access the unix attributes. // If I didnt I should have iterate over the set to determine which // attributes are supported if (supportedViews.contains(\"unix\")) { PosixFileAttributes pat = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); System.out.println(pat.group().getName()); System.out.println(pat.owner().getName()); } I placed plethora of comments on the code so reading and understanding it get easier. In the next snippet we will see how we can read permissions of file system element. The first step in checking permissions is using the checkAccess method as shown below. the method throw an exception if the permission is not present or it will execute with no exception if the permission is present. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); try { // A method to check the access permissin p.checkAccess(AccessMode.EXECUTE); } catch (IOException ex) { Logger.getLogger(perm.class.getName()).log(Level.SEVERE, null, ex); } // Extracting all permissions of a file and iterating over them. //I know that I am dealing with NIX fs so I go directly with that attributes // otherwise we should check which attributes are supported and then we can // use them. PosixFileAttributes patts = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); Set\u003cPosixFilePermission\u003e st = patts.permissions(); for (Iterator\u003cPosixFilePermission\u003e it = st.iterator(); it.hasNext();) { System.out.println(it.next().toString()); } // Using PosixFilePermissions to convert permissions to different representations System.out.println(PosixFilePermissions.toString(st)); As you can see in the code we can use the helper class to convert the permission set to a simpl OS represeted permission of the element. for example the set can be translated to rwx— if the file has owner read, write and execute permissions attached to it. The helper calss can convert the os represenation of the permission to the permissions set for later use in other nio classess or methods. In the next entry I will conver more on permissions and security by tackling the Access Control List (ACL) support in the nio.2 ","date":"2010-06-23","objectID":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 3: File System Attributes and Permissions support in NIO.2","uri":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will discuss the basic classes that we will work with them to have file system operations like copying a file, dealing with symbolic links, deleting a file, and so on. I will write a separate entry to introduce classes which are new to Java 7 for dealing with streams and file contents, watching service and directory tree walking. If you want to know what are new features in Java SE 7 for dealing with IO take a look at Introducing NIO.2 (JSR 203) Part 1: What are new features? Before NIO.2, dealing with file system was mainly done using the File class and no other base class was available. In NIO.2 it there are some new classes at our disposal to take advantage of their existence to do our job. FileSystems: Everything starts with this factory class. We use this class to get an instance of the FileSystem we want to work on. The nio.2 provides a SPI to developed support for new file systems. For example an in-memory file system, a ZIP file system and so on. Following two methods are most important methods in FileSystems class. The getDefault() returns the default file system available to the JVM. Usually the operating system default files system. The getFileSystem(URI uri) returns a file system from the set of available file system providers that match the given uir schema. Path: This is the abstract class which provides us with all File system functionalities we may need to perform over a file, a directory or a link. FileStore: This class represents the underplaying storage. For example /dev/sda2 in *NIX machines and I think c: in windows machines. We can access the storage attributes using FileStoreSpaceAttributes object. Available space, empty space and so on. Following two sample codes shows how to copy a file and then how to copy it. public class Main { public static void main(String\\[\\] args) { try { Path sampleFile = FileSystems.getDefault().getPath(\"/home/masoud/sample.txt\"); sampleFile.deleteIfExists(); sampleFile.createFile(); // create an empty file sampleFile.copyTo(FileSystems.getDefault().getPath(\"/home/masoud/sample2.txt\"), StandardCopyOption.COPY_ATTRIBUTES.REPLACE_EXISTING); // Creating a link Path dir = FileSystems.getDefault().getPath(\"/home/masoud/dir\"); dir.deleteIfExists(); dir.createSymbolicLink(sampleFile); } catch (IOException ex) { Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex); } } And the next sample shows how we can use the FileStore class. In this sample we get the underlying store for a file and examined its attributes. We can an iterator over all available storages using FileSystem.getFileStores() method and examine all of them in a loop. public class Main { public static void main(String\\[\\] args) throws IOException { long aMegabyte = 1024 \\* 1024; FileSystem fs = FileSystems.getDefault(); Path sampleFile = fs.getPath(\"/home/masoud/sample.txt\"); FileStore fstore = sampleFile.getFileStore(); FileStoreSpaceAttributes attrs = Attributes.readFileStoreSpaceAttributes(fstore); long total = attrs.totalSpace() / aMegabyte; long used = (attrs.totalSpace() - attrs.unallocatedSpace()) / aMegabyte; long avail = attrs.usableSpace() / aMegabyte; System.out.format(\"%-20s %12s %12s %12s%n\", \"Device\", \"Total Space(MiB)\", \"Used(MiB)\", \"Availabile(MiB)\"); System.out.format(\"%-20s %12d %12d %12d%n\", fstore, total, used, avail); } In next entry I will discuss how we can manage file attributes along with discussing the security features of the nio.2 file system. ","date":"2010-06-01","objectID":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 2: The Basics","uri":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"I will write a series of blog to discuss what are the new features introduced in NIO.2 (JSR 203). The NIO.2 implementation is a part of OpenJDK project and we can alreay use it by downloading a copy of OpenJDK binary. In the first entry I will just go through what are these new I/O features of Java 7, which help developer iron out better applications easier. Talking about File systems support and features which let us deal with file system we can name the following features: Platform friendly-ness of NIO.2: We can deal with all file systems in a unified model. File tree walk: We can walk on a file tree and examine each node using the built-in APIs, NIO.2 to let us know whether the current member is a file, a directory or a symbolic link. We can then perform any operation we want on that node. File Operations (Copy, Delete, Move): Basic operations are supported with plethora of options. The move operation can be performed in atomic way. Symbolic links support: count soft and hard links as well as managing them. Support for file attributes in NIO.2: Managing file systems attributes is fully supported for different file systems including DOS, POSIX… File system change notification: We can setup a watch service and receive notification when a change happens on the path we are watching. SPI for providing new file systems support, for example to support zip, zfs, btrfs, we can implement the provider interfaces and use the unified API to access that specific file system using our implementation Working with sockets and reding/ writing files we can name the following features: Multicasting is now supported in the DatagramChannel meaning that we can send and receive IP datagrams from a complete group. Both IPv4 and IPv6 are supported. Asynchronous I/O for sockets and Files: Now we can have Asynchronous read and write both on channles and files. It basically means we can have event driven I/O over both networks and files. Other improvement, features: Support for very large multi gigabyte buffers Some MXBeans are included to monitor IO specific buffers. Interoperability between Java 7 IO and previous versions using the Path API. I will post sample codes for each of these features in upcomming entries. So stay tuned if you want to learn more about NIO.2 ","date":"2010-05-20","objectID":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 1: What are new features?","uri":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction ZFS pools provide us with the underlying storage with which we can create files and directories inside it right after we create it. But OpenSolaris and ZFS provides more than that by introducing ZFS datasets… In this recipe we will work on top of a zfs pool named fpool and the default root pool named rpool. So before continuing on this recipe we should have a pool named fpool created. Creating ZFS pools is discussed in the recipe 9, Creating and Destroying ZFS pools, of chapter 2. 1. Getting list of ZFS datasets In this recipe we will discuss how we can get the list of ZFS datasets and also how to get detailed information about a dataset. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:0:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will work with list subcommand of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:1:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can use the list subcommand of the zfs command to get list of available datasets. Like zpool command we can use this command with a pool name or without a pool name to get list of all available datasets in the system. The simplified schema of the list subcommand is shown below: list \\[-rH\\] \\[list_of_pools\\] In this command: The -r option specifies that we want to see a recursive list of all child datasets in addition to root datasets. The -H specify that we do not want to see the header row of the printed information. The list_of_pool arguments specifies a space separated list of pools we want to see their datasets, passing no argument means we want all datasets of all pools to be included in the output. For example to view list of all datasets recursively in both rpool and fpool we can use the following command. \\# zfs list -r fpool rpool The output for the above command is similar to the following figure. As we can see in the figure, the child datasets are also listed in the output. One thing which we might notice is the differences between a ZFS file system name and its mount point, especially for the zpools. Each ZFS dataset has a mount point which can be different than its name and its location in the parent dataset. When we are using commands like zfs and zpool we will refer to the name of the dataset and when we ware accessing a file system dataset we use the mount point. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:2:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The command reads metadata related to the questioned pools and print them in the output. Metadata information of all ZFS objects are stored in /etc/zfs directory. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:3:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also We can get more information about datasets using their properties which is discussed in the last recipe of this chapter. 2. Creating and destroying ZFS File System We can create and destroy a ZFS file systems using create and destroy sub commands of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:4:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool. We can find how we can create ZFS pools in recipe 9 of chapter 2. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:5:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To create a ZFS file systems we should use the create subcommand of zfs command. The command syntax is shown below. create \\[-p\\] \\[-o property=value\\] ... \\\u003cfilesystem\\\u003e In this syntax: The -p option specifies that any non-existing parent addressed should be created. The -o can be used to pass ZFS properties to be assigned to the command. For example we can use it to enable the compression during dataset creation. We will discuss more about these properties in recipe number…. The file systems argument specifies the dataset path we want to create. Any of the non-existing parent file systems specified in this argument will be created recursively provided that we use -p option. Now to create file systems named parent/fset inside the fpool we can use the following command based on the above syntax. \\# zfs create -p -o compression=on fpool/parent/fset The command enables the compression for the file systems during the dataset creation time. The following image shows how this dataset appears in the pool. To destroy a dataset we can use the destroy subcommand of the zfs command. The command schema is as follow: destroy \\[-rRf\\] filesystem\\|volume\\|snapshot In this syntax: The -r option asks the command to destroy all descendents of the target dataset. The -R option asks the command to destroy all depended objects like clones and snapshots. We will discuss clones and snapshots in recipe number The -f option force unmount any mount point. The argument specifies type of the dataset we want to delete; either it is a volume, a file system or a snapshot. We will discuss volumes in next recipe. To delete the parent dataset with its entire child we can use the following command. \\# zfs destroy -rf fpool/parent The following figure shows the fpool datasets after we execute the command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:6:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS dataset its related records are inserted in ZFS metadata storages located at /etc/zfs and any further access to the file system will update these records. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:7:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Renaming a ZFS file system We can rename a ZFS dataset using the rename subcommand of the zfs command. For example to rename the parent file system we created previously we can use the following command \\# zfs rename fpool/parent fpool/parent2 ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:7:1","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next three recipes we discuss managing other types of datasets available in ZFS including volumes, snapshots. 3. Creating and Destroying ZFS volumes In this recipe we will see how we can create or destroy a ZFS volume using the create and destroy subcommand of the zfs command. ZFS volumes provide the software systems which require a block or character device to benefit from the storage pooling of ZFS pools and in the same time has the freedom of accessing the storage in block or character mode. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:8:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:9:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… OpenSolaris installation creates two ZFS volumes one for SWAP area and the other for storing different dumps created when the OS face a problem. To see the list of volumes we can use the following command. \\# zfs list -r rpool Thefollowing figure shows the result of executing the given command. As you can see none of the volumes has a mount point because they are block devices and not file systems unless we format them and mount them manually. To create a ZFS volume we can use the following syntax of the create subcommand of the zfs command. create \\[-ps\\] \\[-b blocksize\\] \\[-o property=value\\] ... -V \\\u003csize\\\u003e \\\u003cvolume\\\u003e In this syntax we have: The -p option similar to the -p option in creating dataset specifies that all non-existing parents addressed in the volume name should be created. The -s option specifies that we want to create a sparse volume. In contrast with normal volumes sparse volumes does not occupy any space after we created them and the volume size grows when we copy some data into it. The maximum size is specified using the -V parameter. The -b option is similar to using -o volblocksize option which let us determine the volume block size. The -V parameter specifies the volume capacity or size, if we use the -s parameter the size will not get allocated immediately but rather the volume grows when we copy some data into it. The volume argument specifies the volume name. For example to create a volume named fvol with a fixed size of 100 MB we can use the following command. \\# zfs create -v 100m fpool/fvol To create a sparse volume named svol with a maximum size of 500 MB we can use the following command. \\# zfs create -v 500m fpool/svol Following figure shows the fpool datasets after creating these two volumes. When we create a volume we can access it using its associated raw and character node names. The raw name for each ZFS volume consists with /dev/zvol/rdsk/pool_name/volume_name naming schema The character name for each ZFS volume consists with /dev/zvol/dsk/pool_name/volume_name naming schema To understand the naming schema better, the following figure shows how these volumes appear in the block and character devices list. Using sparse volumes is not a good move when we have sensitive applications utilizing the volume because the volume may not be able to grow when there might be no empty space left in the pool when the volume requires growing. Destroying a volume is similar to destroying a file system which we discussed in recipe 2. For example we can use the following command to destroy the fvol \\# zfs destroy -rf fpool/fvol ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:10:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS volume, all related metadata for the volume will be stored in the ZFS metadata storage located at /etc/zfs and the space we assigned to the volume will be reserved for the volume and will be subtracted from the underlying pool’s available space. The volume’s node will be created in the /dev/zvol subdirectories to allow both raw and block access level to the volume. Destroying a pool return back the allocated space, remove the associated records and also delete the volume node names created in the /dev/zvol subdirectories. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:11:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also You can take a look at recipe 2 and 4 which contains more information about different types of datasets supported by ZFS. 4. Creating and Destroying a ZFS snapshot In this recipe we will learn another ZFS dataset named snapshots. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:12:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named fset to continue with this recipe. Creating file systems dicussed in recipe 2 of this chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:13:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To create a snapshot we can use the snapshot subcommand of the zfs command. The command syntax is as follow: snapshot \\[-r\\] \\[-o property=value\\] ... \\\u003cfilesystem@snapname\\|volume@snapname\\\u003e In this syntax: The -r specifies that we want to create snapshot from all descendent datasets of the target dataset. The -o specifies a property we want to set during the snapshot creation. The filesystem@snapname | volume@snapname specifies the snapshot name. A snapshot is created from a file system or a volume and conforms to a naming schema similar to dataset_name@snapshot name. In the naming schema the first part is the dataset name for example fpool/fset followed by @ character and then the snapshot name like before_migration. To create a snapshot named initial from the fpool/fset we can use the following command \\# zfs snapshot \u003cfpool/fset@initial\u003e Following figure shows the snapshot status before anything changes in the fset and after we delete some files and add some other in their place. The main purpose of creating snapshot is to revert back to the point when we created the snapshot. To rollback the changes that a dataset have seen after creating a snapshot we can use rollback subcommand of the zfs command as follow. \\# zfs rollback \u003cfpool/fset@initial\u003e This command rollbacks the fset file system back to the state when we created the fpool/fset@initial snapshot. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:14:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… A snapshot as its name implies is an image of a file system or a volume in a specific point in time. The main reason behind creating snapshots is possibility to revert a file system or a volume back to a specific point in time. Utilising the copy-on-write and metadata blocks, a ZFS snapshot does not occupy any space after we create it, and the creating process is amazingly fast. After the snapshot is created, any change in the file system will cause the snapshot size to increase in order to reflect the changed blocks. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:15:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Renaming a snapshot ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:1","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Accessing snapshot data without reverting active dataset ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:2","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe number 3 and 5 are closely related to the snapshots and reading them can give you a better understanding of this recipe. 5. Creating and Destroying a ZFS clone In this recipe we will learn how to deal with ZFS clones. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:17:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we get down to the actual business of managing clones we should have an accessible pool and a dataset, either a ZFS volume or a ZFS file system, to study on. I assume that we have fpool and a file system named fset in our disposal to work with. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:18:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can use the clone subcommand of the zfs command to create a clone. The command syntax is as follow: zfs clone \\[-p\\] \\[-o property\\] ... source_snapshot target_dataset In this syntax: The -p option specifies that all parent datasets need to be created if not existing already The -o allows us to specify some options for the clone. List of all properties for a dataset is discussed in recipe number 12 The source_snapshot specifies which snapshot we want to base the clone on. The target_dataset specifies where we want the clone to be created. The target_dataset is of the same type as the source_snapshot is based on. To create a clone we need a snapshot so, lets create one. \\# zfs snapshot \u003cfpool/fset@initial\u003e Now we can create a clone based on the state which this snapshot represent from the underlaying file system which is fset. \\# zfs clone -p fpool/fset@initial fpool/initial_clone Now if we get the list of all datasets we can see the initial_clone as a file system in the fpool. A clone can not be created in any pool other than the pool which original dataset resides because of the copy-on-write nature of the ZFS and use of metadata blocks for referring to unchanged data blocks. Now that we have a clone to experiment on, we may need to replace the original dataset with the clone after we succeeded changing the dataset in the way we need. This is called promoting as we are promoting a clone to become the original. The following command shows syntax of the promote command which we can use to swap the clone place with the original dataset. promote clone_name In this syntax the clone_name argument specifies the clone we want to swap its place with the original dataset it is initialized with. To promote the initial_clone we can use the following command. \\# zfs promote fpool/initial_clone When the clone replaces the original dataset, any dependent of the dataset, which includes clones and snapshots, will cease its dependencies on the original dataset and point the dependency toward the newly promoted clone. Destroying a dataset which has dependencies like snapshots and clones is easy when we want to destroy the clones as well. We just need to use the -R option to destroy all dependencies including snapshots and clones. But if we only want to destroy an underlying dataset we should promote one of its clones to take its place and then destroy the dataset without harming dependent datasets. For example following set of commands promote the initial_clone and then destroy the fset file system. \\# zfs promote fpool/initial_clone \\# zfs destroy fpool/fset ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:19:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… A clone is a writeable copy of a dataset, a volume or a file system, which its initial data is the same as the original dataset it is created from. A clone can be initialized from a snapshot of the file system and its initial creation is as fast as creating a snapshot because of the copy-on-write nature of the ZFS file system. A clone size grows in time when we change the clone or the original dataset content. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:20:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 2, 3 and 4 are closely related to this recipe as they explain ZFS file systems, ZFS volumes and ZFS snapshots. Reviewing them can help understanding this recipe better. 6. Specifying ZFS mount point In order to use a file system, we should mount it to a mount point so we can read from the file system or write into it. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:21:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named fset to continue with this recipe. Creating file systems dicussed in recipe 2 of this chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:22:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Let’s create a file system and see where it is mounted by default. Following command creates fset in the fpool. \\# zfs create fpool/fset And following figure shows how it is mounted in the default location. Now let’s use -o option and its argument to change the mount point during file system creation. \\# zfs create -o mountpoint=/fpool/cu-mp fpool/file-set And following figure shows how our -o option works and how we can use the set subcommand to change the mountpoint property of a file system after we create the file system. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:23:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS file system, the ZFS assign a mount point to the file system we created and which complies the default naming schema which is pool_name/file_system/ though the file_system argument itself can have multiple levels. This mount point is stored as a part of the file system metadata in the /etc/zfs and when system is booting it uses this storage to layout the file system as we configured. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:24:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also We can use the mount table to mount a ZFS file system for integrating with legacy software. More information about legacy mode is available at http://docs.sun.com/app/docs/doc/819-5461/gaynd?a=view 7. Replicating a ZFS dataset In t recipe 5 of this chapter we saw how we can clone a dataset but that has its own limitation as it only works inside the mother pool containing the dataset. What if we need to move a dataset from one computer to another or from one pool to another. You may suggest using commands like tar, cpio, and dd which we discussed in recipe NO of chapter 1. Sure we can use them but those commands have their associated drawbacks which we discussed in the related recipe. To facilitate replicating a dataset between different pools or different machines ZFS provides a send and receive subcommands of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:25:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready For this recipe we assume that we have two pools named fpool and mpool in one machine, we also assume that we have another machine with ssh server running in a remote machine networked with our workstation. To learn more about ssh review recipe in chapter ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:26:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… In brief, the send and receive subcommands of the zfs command can do the following tasks for us. Sending a ZFS Snapshot means that we can read a ZFS snapshot and send it to a destination. The destination can be a binary file or the receive command. Receiving a ZFS Snapshot means that we receive a ZFS snapshot sent by the send command and writes it in the file system in the same order it was in the origination pool. Remote Replication of ZFS Data: we can use combination of piping zfs second command with ssh and zfs receive commands to send a snapshot from one machine, piping it with ssh executing zfs receive command in a remote machine to write the snapshot on the remote machine. The following command sends a snapshot of the fset file system from fpool to mpool/nset. So the content of the mpool/nset will be an exact clone of the fset when the snapshot was taken. \\# zfs send fpool/fset@snap1 \\| zfs receive mpool/nset The following figure shows more details about the operation. Now lets see a more complicated sample of using the send and receive commands by sending the snapshot over SSH to another machine. \\# zfs send fpool/fset@snap1 \\| ssh machine-02 zfs receive mpool/nset The command will pipe the send output to ssh command which itself is running the receive command. When using SSH client command, ssh, without passing any username it assumes that we want to authenticate using the same user we are loged-in in our local machine. For more details about using SSH review recipe in chapter But the send and receive commands has much more to offer, we can incrementally by sending a group of snapshots to the destination in the same order we have created them. For example following command will send two snapshots named sn1 and sn2 to create the destination file system identical to the state of original file system included in the sn2 which itself is an increment of sn1. \\# zfs send -i fpool/fset@sn1 fpool/fset@sn2 \\| ssh machine-02 zfs recv mpool/fset For this command to succeed, the fset should exist in the remote machine. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:27:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The send command sends the dumped dataset as a stream to the designated output which can be the default output (screen) or any other output target specified using the pipe operand. The recv command write the file system it receives into the specified dataset as it receives it. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:28:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To understand this recipe better reviewing recipe 4 of this chapter and recipe N of chapter is recommended. To learn more about send and receive commands we can refer to man pages and also the online document available at http://docs.sun.com/app/docs/doc/819-5461/gbchx?a=view. To see whish other additional software are available to replicate a ZFS dataset take a look at http://hub.opensolaris.org/bin/view/Community+Group+zfs/faq/#backupsoftware 8. Managing datasets’ properties In recipe8 in chapter one list of a zpool properties are listed, the list is quite short because of the ZPools nature but for the ZFS datasets we have tens of properties which are either specific to a dataset type (volume, file system) or are shared between both of them. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:29:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will work on read-only and writeable properties of ZFS datasets. I assume that we have a pool named fpool and a file system named fpool/com to demonstrate the related commands. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:30:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We said that tens of properties are at our disposal to use for our benefit but these properties has some values assigned to them when we create a dataset. These values come from three sources which are listed below. Default: Some properties have default values, like block size for the dataset, or compression status. Inherited: each dataset can inherit its parent properties’ values if we specify Assigned: We assign some properties’ values when we create a dataset like the dataset name. The following table shows important properties of ZFS datasets. Each a description of these properties is given in the http://docs.sun.com/app/docs/doc/817-2271/gazss?a=view . The table specifies whether the volume, file system, clone and snapshot have the property or not. Property File System Volume Clone Snapshot atime available checksum compresiion compressionratio mountpoint quota readonly type used copies volblocksize secondarycache usedbysnapshot Some attributes are solely available to file system and some for a volume. But snapshot and clones share a subset of the properties available for their base dataset. For example a clone made from a file system has the mount point property while a clone made from a volume does not. Some of these properties are read-only meaning that the system itself sets their values, for example available and used, which contain the amount of space available to the dataset and amount of space used from are read-only and only operating system can change them. We can use two zfs subcommands to set a value for a property or get the current value. Following commands show how we can do it. \\# zfs get available fpool/com \\# zfs set mountpoint=fpool/newmp fpool/com To view a complete list of all properties for a dataset along with its source, current value, and write-ablity we can use the all argument for the get command as we can see in the following figure. ZFS datasets inherits many of their properties from their parent datasets. For example if we enable compression in the fpool/comp and then create fpool/comp/inner, the compression algorithm for the fpool/comp/inner will be the same as the compression state of the parent dataset. Sometimes we do not want a property to get inherited by a parent dataset, for example we may specify a custom mount point for a file system when we create it and we do not required the descendent file systems to use the same mount point we can use inherit subcommand of the zfs command to prevent descendent datasets to inherit the custom mount point. \\# zfs create -o mountpoint=/fpool/customs/fs1 fpool/fs1 \\# zfs inherit mountpoint fpool/fs1 From now on, any descendent file system of fs1 will not use the mount point relative to /fpool/customs/fs1 and either uses the default mount point pattern discussed in recipe 6 or uses the mount point we specified. The combination of get and set commands helps us in developing shell scripts to automate our daily jobs. We will discuss more about shell script development in chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:31:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… All ZFS attributes and properties are stored in the ZFS metadata storage located at /etc/zfs. When we invoke a set subcommand or any of the dataset creation command we either insert new records into this storage or we edit the already existing information. When we use get subcommand or any other zfs subcommands, we read the metadata stored in this database. OpenSolaris uses the information stored here to layout the file system during operating system boot-up. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:32:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can define and later on use custom properties for a ZFS dataset specifying descriptive information for our administrators and users. For example we can create a custom property named used-for to specify what we are using a dataset for. All custom dataset are pre-fixed with : to let us know a property is custom or native to ZFS. To set a custom property we can use set command as shown below. \\# zfs set :used-for=backup fpool/comp To view the value of a property we can use the get subcommand as follow. \\#zfs get used-for fpool/comp Following figure shows how the custom properties appear in the output. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:33:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also For some example of using the set and get commands review recipe 9 and 10 which deals with deduplication and the compression in ZFS file systems. 9. ZFS De-Duplication Deduplication is a file system level space saving method in which redundant data is eliminated. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:34:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool. We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named dedup inside this pool to makes it possible for the sample commands to execute properly. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:35:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can enable deduplication during file system creation or afterward using the set command. Following snippet shows how we can enable it for an fpool/dedup file system during file system creation. \\# zfs create –o deduplocation=on fpool/dedup Following figure shows value of the deduplication property for the fpool/dedup. We can also use the set command to enable deduplication for a file system after we create it. Following command shows how we can turn off the deduplication in the fpool/dedup file system. \\# zfs set deduplication=off fpool/dedup ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:36:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Deduplication operates in the file level so the file system checks every file and decide whether the file already exists in the system or not if the file already exists, the file will not be stored and instead a reference to the already existing file will be used. For example in a typical enterprise mail server one email attachment may find its way into hundreds of users mail boxes, Deduplication will store the actual file once and all other instance of the file are referenced to the same actual file. When we enable deduplication for a file system either when we create the file system or afterward using the set subcommand, we are asking OpenSolaris to only keep one copy of each single file stored in the hard disk and use reference to that single copy when necessary. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:37:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also 1.To learn more about ZFS dataset properties and their usage review recipe number 8. 10. ZFS compression In this recipe we will see how we can enable the ZFS compression to save space. We usually enable the compression when we rarely need to read the dataset content or the reading operation does not need to be performed fast. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:38:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We can enable the compression on both file systems and volumes. Compression can be enabled during the dataset creation or we can enable it after we created the file system and we felt that we need more space rather than higher I/O speed. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:39:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Following command creates a ZFS file system named comp with compression enabled and a custom mount point instead of the default mount point. \\# zfs create -o compression=on -o mountpoint=/fpool/custom fpool/comp We can check the compression ratio of the file system using the get command as follow: \\# zfs get compressratio fpool/comp Following figure shows how the output for this command, however I copied some files into the dataset before checking the compression ration so you can see how it actually works. We can use the set command to enable the compression or change its algorithm on a dataset. For example to change the compression algorithm in the fpool/comp we can use the following command \\# zfs set compression=gzip-9 fpool/comp There are multiple values for the compression option which varies in compression and CPU usage factors. Following table shows these values along with their explanations. Value Description on Turn on the compression using the default algorithm (lzjb). gzip-N same algorithm and compression ration which gzip provides. The N can be 1 to 9 which specifies the compression level. While 1 means no compression 9 is the highest compression level with most CPU usage gzip The gzip algorithm with default compression level set to 6. lzjb default compression algorithm which is a performance friendly algorithm with an acceptable compression ratio off turn off the compression for the given pool. When we create a clone from a dataset with compression enabled, by default the clone will not be compressed unless we enable it either during creation or afterward using the set command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:40:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we enable the compression, our data get compressed before the get written into the disk using the algorithm we specified. When reading the data, each block of data go trough decompression and then upper level applications receives the uncompressed data. When we disable the compression, no further compression is applied on the data we write into file system but the data which are already compressed will stay the same. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:41:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about ZFS dataset properties and their usage review recipe number 8. 11. Defining quota for ZFS ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:42:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use the quota and reservation properties of ZFS file system to set a limit on the amount of space the file system can use and amount of space which the pool must have in reserve for the file system. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:43:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Assuming that we want to limit the fpool/fset to 1 GB we can use the following command. \\# zfs set quota=1g fpool/fset And to ensure that the fpool always keep at least 200 MB free space for the fset we can use the following command. \\# zfs set reservation=200m fpool/fset The two commands we used means that The overall size of the fset and all of its descendent datasets can not exceed the 1GB limit At each point at time, the fpool has at least 200 MB space ready to be used by the fset. This space will be allocated and counted as used space of the pool. Following figure shows how the reservation and quota properties’ value can affect the status of a zfs dataset. As we can see before we specify the quota, the whole 2 GB of the fpool is available for the fpool/fset while after we specify the quota only 1024 MB of that space is available for the fpool/fset. On the next step, before we specify the reservation, just 102 K of the pool was used while after specifying the reservation; 200 MB of the pool were gone for good. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:44:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Defining quota in ZFS is very different compared to defining it in UFS which is described in recipe 7 of chapter 2. In ZFS we define the quotas in file system level for the file system itself and not for users’ usage. In ZFS we specify how much space a file system can use in the pool, quotas of that file system, and how much space the pool must have reserved for a specific file system. To limit how much space a user can use we will assign a file system to the specific user to prevent it from using more than quoted space. To lean more about security and user access management review chapter ? recipe ? ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:45:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about quota take a look at recipe 7 of chapter2. For leaning limiting the amount of space a user can have take a look at recipe of chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:46:0","tags":["Solaris","ZFS","File System"],"title":"Managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"}]