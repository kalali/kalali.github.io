[{"categories":["Microservices"],"content":"Fragmentation of Java runtimes in a microservices-based architecture There are many many differences between the monolithic architecture and microservices-based architecture. One of them is the number of independently deployed bits of software, with a monolith there are usually many replicas of the same distribution running on many machines, virtualized or not. With the Microservices the number of independently deployed software systems grows as the different contexts raise… or an already existing context boundary grows enough to be split. So many independent teams and deployments will likely results in different versions of libraries, charts, runtimes, and so on being present in the production environment. Below is a snapshot of what we had in production at one point, it shows what percentage of microservices are running on what Java distribution and version. As I mentioned above, it may look fragmented, but this level of fragmentation is accepted for us and if we manage to keep it at this level over the lifetime of the service we within the accepted range. Almost 70% of the workload is running on Java 17 (current LTS) less than 5% running the previous LTS (Java 11) and some services are running the latest release (Non-LTS) which we accept in the ecosystem. ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:0","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"What causes fragmentation to begin with? Sometimes fragmentations are positive, like a technical debt that can be a good debt. The same goes for fragmentation. If the different segments are having a purpose, for example, a small early adaptor group, a majority of steady progressors, and a small group on the previous version; it can be considered ok. Most often there is freshness for previous, current, and next versions of software systems. And having the majority of your workload on the current TLS of any platform is a sign of good management. Bad fragmentation can be the result of a bigger organizational issue or a smaller team/service owner neglect. Some causes of fragmentations are: No strategic direction by the organization for dependency hygiene Missing architectural design and implementation for easy dependency management for different dependency layers No automation for detecting an update and/or creating a pull request for it The pull request usually means some tests are executed and there is a reasonable level of certainty that everything works fine with this new version Lack of prioritization by the team to accept such PRs due to other usually urgent! work Missing (sense of) clear ownership for services etc. We are mainly using a combination of the following to encourage the service owners adopting to the latest JDK versions: Use Renovate Bot to scan repositories and submit pull requests Use Prometheus to gather version information from every running pod and if any service lagging too far behind generate alerts Tie the above metrics to our DORA interpretation and generate dashboard and alerts based on that Gamification of deploying to production and uptaking renovate PRs to encourage staying up to date Announcement when there is somethign urgent and teams must accept the PRs Some description for each of the above elements follows: ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:1","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"Renovate Bot The Renovate bot can scan a variety of dependencies descriptors and submit a pull request if an update is available. The bot scans all the repositories, that are not labeled to opt-out, and submits an update for the JRE version. If the build is green and the service owner sees it fit they will merge the changes. There are plenty of examples on how to do dependency managemnt using renovete for different platforms and also how to extend the rules for anything that is not already supported. ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:2","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"Prometheus As the platform team, we encourage everyone to use a metrics-related library that we built. Among other things the library exposes some versions and builds information as a metric that is scraped by Prometheus. The metrics go and form a dashboard for visualization and exploration as well as providing possibilities to derive other analytics and alerts. Each framework has a way to expose this. For example for Spring boot it is described in this spring boot github issue. The idea is there, few classes need to be implemented and contribute to a metric. ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:3","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"DORA metrics We have an interpretation of DORA Metric and some custom tooling that does the measurement and publish the metrics about teams. We have more work to do about DORA metrics as measuring an accurate Change Failure Rate and Time to Restore Services by automation is more involved. Measuring some of the metrics can be done using the CI/CD metadata, the change tracking systems (for example ServiceNow or similar solutions), the package registry, source versioning solutions (if you have a specific workflow that can provide the information), the ApplicationInfoMetrics as described above and so on. But generally whatever source is being picked must be the absolute source of truth that rest of the organization would use (so audit trails, and other numbers match at the end of the day) ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:4","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"Gamification We developed an in-house system that gather metrics from different sources (change management, Prometheus, etc.) and publishes some statistics about teams’ achievement and who has more points depending on the type of achievements, etc. That is an encouraging way to keep things fun and yet relevant to our overall goal of staying up to date with dependencies and preventing stale, abandoned services in production. ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:5","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Microservices"],"content":"Announcements There is an announcement channel that everyone is tuned to. If there is something urgent that must be taken care of an announcement is sent out and teams are supposed to act. For example, a config issue in a sidecar, a bug in a library, etc. We are working to automatically creating incidents for these type of issues that needs a short reaction turnaround. ","date":"2022-07-03","objectID":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/:1:6","tags":["Microservices","DevOps"],"title":"Microservices Runtime Fragmentation","uri":"/2022/07/microservices-fragmentation-and-what-to-do-about-it/"},{"categories":["Kubernetes",""],"content":"From Monolith to Microservices Having redundancy in application space is pretty common. We have all seen them for decades in different ways. Hardware, software layer 4, or layer 7, etc. All of them what they do they direct traffic based on a given decision-making rule to a destination. Being weighted round-robin, resource/load-based metrics, etc. All they do is reliably direct traffic to a healthy node in the destination. In a very simple and basic view with VMs and data centers, if VMs fail the load balancer will direct loads to identical VMs, all nodes more or less can handle any of the incoming requests (ignoring stickiness, etc.) If one node disappears another one will take the burden, hopefully without causing a cascading failure. No matter what type, the load balancer will add and remove the nodes based on their health or other metrics, API calls from the pool and add them back. When there are active-active or active/standby data centers the whole switch can happen between data centers manually or automatically when one data center is down or operating at the limit of its capacity, etc. VMs on different clusters ","date":"2022-07-01","objectID":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/:1:0","tags":["Kubernetes","Reliability","Fault Tolerance"],"title":"Kubernetes Active Active Clusters and Application Fault Tolerance","uri":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/"},{"categories":["Kubernetes",""],"content":"Microservices and redundancy With Microservices it will change, as usually there is no single DNS name that can handle all the requests but somewhat different components of the system service different subsets of requests. Each of those monolith VMs is now several dozen separate services, being micro or mini. The microservices are usually deployed in a Kubernetes distribution, using namespaces for each service or group of services that can form a boundary. Services are redundant across clusters; usually, the clusters are identical replicas in sense of the service dependency graph. Some of the services are serving requests that are originating from outside of the Kubernetes cluster, call them ** services and some of the services are only serving requests that originated from inside the cluster call them Core services. The experience services are load-balanced across the two clusters using hardware or software sitting outside these clusters. The Core services in each cluster serve the internal requests in that cluster. A simple look at an active active cluster Services in each cluster have a health check, which includes their dependencies health check. The health check happens at intervals so an Experience service knows if the dependencies are healthy and it can operate or not, and it exposes that status as its own health check to the load balancer. ","date":"2022-07-01","objectID":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/:1:1","tags":["Kubernetes","Reliability","Fault Tolerance"],"title":"Kubernetes Active Active Clusters and Application Fault Tolerance","uri":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/"},{"categories":["Kubernetes",""],"content":"Microservices and cluster reliability The problem arises when some of the Core services are not implementing the health check or that a Core service that handles a fraction of these requests that arrive at the experience service is dying off in one cluster. The health check cascade will put the experience service in that cluster out of the origin pool and route all the requests to another cluster. As image below first, the Core service fails then the health check cascade results in the load that was handled by the west is now routed to the east. A simple look at an active active cluster This other cluster either needs to autoscale to bear the load or have service degradation until the Core service is restored. Some of the solutions for cross-cluster load balancing: Exposing every single service as using DNS. The load balancing happens outside the cluster as the load balancer, generally sitting outside the cluster. It is simple and well-known how to manage DNS records and load balancers in any big organization. The big disadvantage is the traffic routing cost (time and CPU) in and out of the clusters and the increased failure chance with the LB instance/s being the SPF. Using Skupper which provides a level 7 virtual application network (VAN) layered on top of the already existing network. Skupper uses Apache Qpid Router and performs cross-cluster load balancing among a few other things. It is simple and very lightweight to use. It needs no cluster administrative privileges to install and overall automation for a cross-cluster load balancing is pretty straightforward (plenty of examples in their website). On the flip side if skupper proxies a service, the applications cannot do network port negotiations for example. E.g. FTP. Skupper is a completely decentralized solution for cross-cluster load balancing. Each namespace owner can decide to use it or not. Use a Service Mesh federation. Service mesh is a component that can help with addressing quite an array of requirements, one being cross-cluster load balancing! The mesh can dynamically balance the load between the two clusters for any of the service constructs that are enrolled. The disadvantage is the complexity of the service mesh operation and management. Of course, the mesh provides many other functionalities which if the organization requires it may be a reasonable solution. Depending on the organizational setup and cluster access levels a service mesh can be a semi-centralized solution. CRDs are managed by namespace owners and overall operation by cluster admins Using Submariner that manages the network mesh in layer 3. If the clusters are already set this is going to be difficult to roll out. It needs cluster-admin access as well as the complexity of maintenance and management. Dealing with compatibility between different cluster versions/flavors may also become an extra maintenance cost. The solution is centralized and managed by cluster admins. All of these solutions: Provide a control plane Full host of monitoring capacity (from having their own console and GUI to exposing a Prometheus scrapable endpoint) Support, e.g. from a vendor like redhat Cross cluster load balancing and routing Provide encryption for the communication channel The solutions differ on: What other functionalities do they provide What level of complexities do they impose on the infrastructure what network layer implications do they have What level of complexities are exposed to application developers Which way to go depends on the context and the expected outcome(s). If none of the functionalities listed for Submariner and Istio are needed then something like Skupper is the definite answer. If the organization requires more QoS that are provided by e.g. Istio or LinkerD then the service mesh is the way to go. But generally speaking, using the simplest solution with a bit of thought about how will it scale in the future should be the first MVP and attempt in addressing a requirement. ","date":"2022-07-01","objectID":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/:1:2","tags":["Kubernetes","Reliability","Fault Tolerance"],"title":"Kubernetes Active Active Clusters and Application Fault Tolerance","uri":"/2022/07/kubernetes-active-active-clusters-and-application-fault-tolerance/"},{"categories":["Architecture"],"content":"Deciding between software architecture attributes primacy is hard. I follow some practices that I will talk a little about them here.","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Last week I was talking to a former colleague, he is running a SaaS startup now, and our conversation went around SRE, production, and reliability. We talked a lot about architectural attributes that relate or impact the incidents, from detection to recovery, and on to prevention. I thought to put some of my thoughts around architecture into a blog post since that was the core of our conversation. The next post I will cover more details on architecture for fault tolerance. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:0:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Architecture"],"content":"Architectural attributes, the -ities I generally put different architectural attributes into two categories: Internal: for example operability, testability, supportability, upgradability, extensibility, etc. External: for example scalability, responsiveness, security, compliance, usability, availability, etc. ) The architectural attributes can be sliced and diced in different ways, I will probably write another slicing and decision making in another post. At the end of the day, any attribute and quality of a software system are going to impact the users. But some attributes are more concerns of customers and users of the system while another group is more concerning those building and maintaining it (developers, support, QA, etc.). There is always a push and pull between different stakeholders on the importance and investment that should go into these two categories. For example, usability, availability, and responsiveness might be the product owner’s focus while a technical stakeholder would also think, and advocate about investment in extensibility, supportability, and so on. Each stakeholder advocates for the attributes they better relate to. These pushes and pulls get more interesting when other stakeholders are advocating for attributes, not that clearly understood by others. For example compliance and legal matters (I generally put this into the external category). Keeping the balance Deciding on which attributes from different categories should be the core is very much context-dependent. For example, deciding between yield and harvest would result in deciding whether reliability or availability should have a higher degree of importance. I approach deciding about architectural attributes is based on a few practices: I Talk to all stakeholders to form a clear understanding of what the software system is going to do. Why is it being built, or rebuilt? How is it going to be used and who is going to be the user/s. I think about three attributes as the heart of a system based on the above. Any other attributes form around these over time, without diminishing them I associate a degree of importance with attributes contributing to architecture. These will be considered when the architecture is evolving or when trade-offs are being made for adding a new feature, etc. Iterate over these as an understanding of the system is building up and evolving the system. Deciding on the core attributes is like setting a strategy, decision being made around how the architecture could evolve is easier when you have some core attributes that cannot be diminished or ignored. Next to the core attributes, there are some considerations that deferring or ignoring has the highest cost of reconsideration. For example, testability and operateability are important enough to be next to the core attributes. If a software system is not testable and operatable it is going to rot over time and a re-write will ensue. Some examples to consider: For a regulated business attributes contributing to compliance are the top concerns. For example data integrity, audit-ability, access control, etc. Building anything, no matter how good, that is not compliant is unlikely to see the light of production. For a web-scale service the first items to consider are scalability, availability, and reliability, the rest of the attributes would form around these. Some systems may need very little attention to scalability but an extreme emphasis on safe operation, reliability, and upgradability. A medical device, a CNC machine, and so on. Deciding on any architectural attributes should be deferred as long as it is not causing indecision. When the same subject comes up multiple times without a conclusive way forward it means there is a lack of strategy, being for the product or the organization. I would put having a decision about the architectural attributes of a product to be part of the product and in a larger scope, the organization’s strategy. ","date":"2022-06-21","objectID":"/2022/06/software-architecture-attributes-how-to-decide/:1:0","tags":["Architecture"],"title":"How to plan the architectural attributes?","uri":"/2022/06/software-architecture-attributes-how-to-decide/"},{"categories":["Agile Team"],"content":"What I have observed with successful teams is the trust and belief in the team's mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams.","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Agile Team"],"content":"What I have learned about successful teams is the trust and the belief in the team’s mission. Trust in the mission, baring some issues like ill-fitting team members, etc. is the single most important factor for success of an individual, a team or a team of teams. When the group of individuals that are part of the said team cannot see a mission they trust, going through the famous steps of Forming, Storming, Norming, and Performing will be harder. The mission is what the team usually communicate and collaborate on. And effectively going through FSNP stages is possible with communication and collaboration. The communication and understanding that forms between team members is what makes the team reaching the performing stage and becoming what is called a high performing team. The concept of task oriented teams is floating around with different acronyms, one being the Quick Reaction Team, the miscellaneous teams, being project based team, fluid team, and so on. The whole concept behind it is a team that has no long history of having gone through the team formation and a long term strong mission to move toward to. It is usually working through a set of bugs and firefighting, or through disjoint projects that come into the pipeline. Setting up successful development teams is hard! It is not only because people are different but also because organizations vastly differ from one another and thus one recipe won’t work for everyone and everywhere. I tried setting up teams with a similar approach with some teams focused on long-term work that required more research and prototyping and a team dedicated to quicker gains for the system’s customers. The quicker gains include bugfix or features with smaller scopes and higher demands. The result was accumulated fatigue of never-ending context switching for the quick response team. Of course, there is always the gratifying feeling of what they have developed lands in so-called production but the feeling of accomplishment is not on par with the fatigue of the context switches. That was when I thought about rotating the team members between different teams with implications of time spent getting up to speed with the feature development and transfer of the knowledge context from the team member moving out of the feature development team and getting used to ways of working of the quick response team for the newly joined member. It is possible to have a pool of engineers working together on different areas of the same bounded context, as long as the members that are coming together have already gone through knowing one another as team. But to pull seemingly random engineers between different knowledge/context in a project based manner may result in developing a feature but it will not result in positive outcomes long term. ","date":"2022-05-03","objectID":"/2022/05/on-the-lack-of-merit-of-task-based-teams/:0:0","tags":["Leadership","Team Building","Velocity","Management"],"title":"Why not task based teams?","uri":"/2022/05/on-the-lack-of-merit-of-task-based-teams/"},{"categories":["Developer Experience"],"content":"When a new hire joins a team dealing with the development or sustaining of a large-scale application there are a few things that they would need to form a basic understanding of the system: Deeper understanding of what the business about Get a view of the system’s architecture for the software system and the deployment view. Get a bird’s eye view of the system’s dependencies. At least the immediate dependencies. History lessons, how the system came to be at the current state. Maybe ADRs would help with this. Learn about what are/were the push and pull resulted in the current state (Lack of maintenance, lack of test, long release cycles, well automated, etc.) Learn how to go through a development cycle of checking the code out, make a small fix/change, run the tests, go through the release cycle. In some organizations, these steps take months to complete. Or some of the above may not even be possible, because nobody knows the context of some decisions and some paths taken. It always pays off to make any onboarding as easy as it can get. Onboarding is a recurring cost like reading/changing. So if clean code matters, so should a clean onboarding. There are plenty of reasons for the first days of software developers starting a new job being less than ideal. From poor security onboarding to lack of a buddy to walk them through the day-to-day of work-life for the first few days. Details of that is a subject of another series of blog posts ","date":"2022-04-26","objectID":"/2022/04/onboarding-as-important-as-code-readablity/:0:0","tags":["Team","architecture","Onboarding","Documentation","Developer Experience"],"title":"Smooth Developer Onboarding is as Important as Code Readability","uri":"/2022/04/onboarding-as-important-as-code-readablity/"},{"categories":["Architecture"],"content":"During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the  architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it.","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Introduction During the past 20 years I have seen trends come and go; one of the thing that has stayed around in one or another form is the term architecture and the architect role. Of course, there are plenty of overloads for the term and plenty of architect archetype like domain architect, enterprise architects, IT architect, network architect, software architect and you name it. Wikipedia has a good definition of software architecture that I quote below: Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations. The architecture of a software system is a metaphor, analogous to the architecture of a building. It functions as a blueprint for the system and the developing project, laying out the tasks necessary to be executed by the design teams. Software, data, IT, domain, and enterprise are going to have an architecture no matter if it is intentional, developed and nurtured and well documented. Or something that is grown out of what everyone involved in the system has done to arrive to without a record of why the architecture is what it is. Below I write about how and why documenting software architecture is not as successful as it should be and how can it possibly be improved. AD and ADR During the blog I refer to Architecture description as AD and Architecture Decision Record as ADR. I see adding the ADR as an ongoing effort while the AD is the overall architectural view of the system. The AD has higher level of abstraction compared to the ADRs which are focused for a particular decision. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture description infamy The primary reasons for the infamy of software architecture description, as far as I can say, is the failure in attracting different stakeholders to read or to develop architecture descriptions. There are plenty of archetypes of architectural descriptions targeting different groups of stakeholders. Here I will put the focus on the software architecture with the target audience being software engineers. Some reasons for architecture description usually is not the favorite topic of conversation are the followings: The architecture description is too generic and is not created for software developers consumption. The vocabulary is wrong, the addressed concerns may not be relevant and so on. It has too much irrelevant information that the software developers may not need. So the noise to signal ratio is too high for the document to be considered. The architecture description is not easy to access, and if it is; it is not easily readable because of the tooling that is used to create it. No uniform theme is used. Different architecture description within the same organisation are not following the same theme. The description is not up to date From the list above “keep it up to date” might be hard to imagine and the “theme” item may not be clear. So I will go into a bit of details for each. Keep architecture description up to date Keeping documentation up to date is hard, we all know it. And there are many advice on how to approach this. Some of these advice are applicable to any documentation. Keep the documentation short: Write as little and as targeted as possible. Avoid fluff, avoid write one document for all stakeholders approach. Have a responsible person: Each ADR and the AD itself should be owned by someone who is the sole responsible to keep track of and update them. If everyone is responsible nobody is. Make updating easy and streamlined: Keep the AD and ADR in the code repo, where the devs like to spend their time in. Make it easy to contribute to the AD/R: Anyone in the team/s should be able to open a PR or contribute to clarifying the AD/R Establish a uniform theme I know almost no software developer without a very carefully selected theme for the editors, terminal and IDE. So I’d expect the same emphasis of look and feel would apply on the documentation as well. When I say theme I am referring to the following aspects of the architecture documentation: Terminology: Refer to any one concept/artefact/etc. with one term throughout the org Framework: Use the same architecture model framework, if it is C4 Model or 4+1 Model or anything else, stay with the same model everywhere Tooling: There are 100s of diagraming tools, pick one and stay with it. The lighter it is the better chance of it being used. Stay away from heavy tools for day to day work. Choose a tool that works on all operating systems used in the organisation Common icons/glyphs: Develop common icons and glyphs for internal architectural concepts and artefacts; use standard icons, e.g. cloud vendor provided icons, or framework/tools provided icons and glyphs Use a standarar template: At application server organisation we had a architecture committee (AsArch) which had a OnePager template for architectural decisions/changes that needed to review. Everyone in the committee or any attendees knew what to expect to see in the architecture description. For each of the above line items I can write down a long blog post. But let me talk a little bit about the template as the container for rest of the items (a glyph, a diagram, a term or expression wont be used without it appearing on a page) ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:1","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Software architecture descriptor template Templates have poor reputation as anyone in larger enterprises associate them with a long document with unfamiliar terminology and vague questions. The architecture description OnePager template should not endup being vague or filled with questions. It is a description not a checklist, not a questionnaire. What about ADRs? One thing to mention before I go far with describing a template, I can say that template is not a farfetched concept when using ADRs. The ADRs follow a template, for example it can contain headers for context, assumptions, decision, consequences and status and the content for each header. I see ADR mostly suitable to document the on-going decisions. Such decisions, if they impact the bigger picture must result in an update in the one page. Think about the OnePager as a summary of all the ADRs. Usual heading in ADRs Some of the most common headings in an ADR are the following: Tittle: What is it that this decision talksa about Date: When was this decision made Context/Summary: Problem definition and solution context. This is setting the scene for rest of the description/decision to come Decision: What is decided in relation to the issue/context/question Status: proposed | rejected | accepted | deprecated |… Consequences: What changes in the system (Performance, testability, cohesion, isolation, etc.), what turn more complicated or easier. You may also see the following headings being mentioned: More heading in ADRs Deciders Assumptions Constraint Related Decisions But generally speaking, the template is a set of common sense headings encapsulating the discussion that has happened over a coffee, over an email, in a meeting, etc. in an easy to follow structure. The OnePager as a story Now, down to a OnePager. What I appreciated about the OnePager that we had was the clear headings and subheadings. There was no confusion about what is needed to be mentioned and in what order. Of course not all headers were necessary to have any content or to be present, but their presense in the OnePager was a guide so that the authors do not forget about adding some details. For example, if there was something to mention in relation to performance impact it would go under “Performance impacts” header. If there was an impact on the system security it would go under the security headring and so on. Reading one pager should feel like reading a story, same as reading a well written code imho, starting with an author and an overview, what is involved in the system and what is not (the scope) and down to details. It must be open to everyone in the organisation, anyone should be able to easily find it, read it, comment on it and could reach out to the author/s for any clarification. The OnePager, at any point in time, must reflect the current state of the approach taken to build the system. Depending on the scope and size of the system the architecture description may have a single a very high level representation of the system components or a more detailed approach. For a significant enough s system there wont be more than Context diagrams (if we assume C4 Model). For example mentioning the presence of a pipeline to deliver the code to a target deployment environment can be a component in the context and described in a single paragraph. Later on each of these high level components of the architecture will have their own OnePager going into the details. ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:1:2","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":["Architecture"],"content":"Conclusion Write less Write where it can be accessed and changed. Stay consistent through the organisation Dont write a single document for all the role; write targeted documents for different roles Have clear ownership for the description ","date":"2022-04-19","objectID":"/2022/04/few-tips-for-more-success-with-architecture-description/:2:0","tags":["Architecture","Documentation"],"title":"Few tips for more success with architecture description","uri":"/2022/04/few-tips-for-more-success-with-architecture-description/"},{"categories":null,"content":"I work in software industry; and have been involved with software development since 2000. Delphi, .Net and J2SE back in the time, and now I am more focused on technical leadership, cloud, architecture, and getting organizations closer to realizing their potentials and goals. My responsiblities are more in ensuring alignments, and with assisting organizations with tech strategy, architecture, adoption of new technologies and practices, planning and implementations when the area of impact is across organization boundaries. I like the thrill of innovating, building or rebuilding. It is as giving existence to something that is not; being software engineering or small tinkering projects. I like outdoors and most often I go for a long trail run to unwind. I authored a book and co-authored another a decade ago Glassfish Security and Developing RESTful Services with JAX-RS 2.0, WebSockets, and JSON . At some point I was editor of some Dzone’s Zones and put some writing into that portal Whenever I get the chance I read books ranging from sci-fi and fantasy to leadership and management; every now and then I binge a bunch of articles or blogs that I have bookmarked to go through. Recently I revived this blog (Removed about 110 posts that were outdated) to start writing again. You can get in touch with me mostly by Linkedin ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About me, Masoud Kalali","uri":"/about/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it.","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Product Adoption"],"content":"Amount of information about how a new component or software system can solve all the problem in a specific area is sometimes overboard. The hotter the topic or the framework the more information is scattered around the web for it. For example: Have you seen the plenty of use cases that every vendor in every segment of tech stack provide as part of the adoption or success story? How about the tutorial that shows how quickly and easily something as significant as a an observability solution can be set and be up and running? And many other medium that one way or another demo how easily a solution can be used/adopted. Existence of such case studies and tutorial is amazing contribution to the overall wealth of knowledge and understanding in the tech sphere, no doubt avbout it. Sometimes I see engineering team members speaking about adopting a new infrastructure component or a brand new CNCF project with estimations that are 1/20st of the realistic time that it would take to adopt such a solution into our infrastructure. Any of those can be an entry point for a team or organisation deciding to adopt something and then endup in the pink elephant. With the adoption takes longer and longer and at the end of the day the organisation endups with half baked integrations and a team or an individual who is in charge of maintaining something that is not ready for prime time but is in widespread use. If vendors and contributors that are writing quick tutorial can be more straight forward with the write-ups in relation to things like compliance, governance, automation, seamless integration, security, etc. that an organisation must take into account before thinking about adopting project/product XYZ. Over short term there might be fewer attempt at adopting new components, products and projects but in the long run there will be more success in the industry which hopefully make everyone happier. Consumer of tech products will be happier with bug free, and less breaches in the product they are using and the practitioners will be happier with better build and well thought IT ecosystem. On the other end of it, it may result in less experiments and thus less feedback to the product owners on how to improve or enhance different aspect of the product. From the rollout to the usability of it, it is a trade-off and a balance for sure. ","date":"2022-04-05","objectID":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/:0:0","tags":["Vendor","Case Study"],"title":"Impacts of a few in-depth write-ups and an abundance of getting started","uri":"/2022/04/impacts-of-a-few-in-depth-write-ups-and-an-abundance-of-getting-started/"},{"categories":["Platform Engineering"],"content":"Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Intro Building a new development platform, for example a new microservices oriented platform to replace an existing monolith software system, is a massive endeavour. I have been working on a development platform, the whole ecosystem from ways of working to pipeline in the past 3 years and the experience might help others. So I thought to write down some of the observation and experience before they fully turn into intrinsic/implicit knowledge and hard to pen. Platform? If we divide the software development within an organization into two very broad categories, there is platform development, and platform products in one layer and and application development in another, which is usually consumes the platform. The applications are usually customer/end-user facing or directly facilitating a particular business use-case, for example a login routine or a batch job that runs every night to process schedule payments. Applications most often have a clear business goal and defining OKRs is easier and ROI can be observed within a shorter period of time. On the other hand, platform development which can be anything from shared components for different applications to release pipelines, development tools, or combination of many things together is not as straight forward. Platforms and platform teams are there to not only save initial cost of starting a new project or product but to save on the cost of maintenance and owning such product but introducing familiar concepts and ways of working across the vertical and horizontal integration points. Sometimes a platform/team is hard to justify, not easy to have any meaningful OKRs and usually have no immediate ROI. For example, If the platform team develops a pipeline and related libraries, the adoption of such pipeline takes time. After a reasonable number of applications have started using the said pipeline, product owner would slowly see an indirect ROI from: Better compliance baked into the pipeline Automation in release practices rather than throw over the wall and hand overs Uniform quality gates as pipeline would enforce some level conde analysis and apply some gates Faster time to production as it reduces the number of manual steps and communication costs Less code sediment as continues or even frequent deployment reduces the code that is sitting in the codebase without being exercised Less regressions because of frequent small releases etc. That is when the justification for existence of a platform become easier and the platform teams and products will become necessities. Why Platforms? Simply put platforms help with the following: Time saving as they prevent inventing the same thing again and again; e.g. a pipeline. Time saving as, when done right, they introduce a consistent way of working across diferent layers. Being applications, infrastructure or tooling. That will in turn makes transfer of knowledge and understanding easier Time saving as teams with expertise build re-usable components that other team can incorporate rather than every time having to acquire such expertise Governance, and compliance and other orthogonal aspects can be addressed via automation backing variety of self services. Let’s say that there an enterprise with the very well established process for software development, delivery, operation and support. This has been the case for the past couple of decades and individuals, teams, organisational units, and organisation as a whole developed a culture around the development process (for good or bad). The process can be something along the line of the following for simple features/functionalities. Imagine a handover between most steps. Long contemplations and many meetings of the domain architect/s Retirement gathering and certifying it with stakeholders Doing any risk assessment, compliance check in relation to data being processed and/or collected Planning (usually includes capacity planning and schedule as well as development timeline) Development and figuring o","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:0","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Platform Engineering"],"content":"Setting up the expectations Being a product owner or an advocate for a platform requires more conversation and investigation about painpoints existing in the organization as a whole. Being the application/program development teams or the relationship between compliance/security/process divisions with the application development hierarchy. Setting clear expectations for external customers and goals for the platform team itself is an important task that the product owner, in combination with engineering teams (and management) should set. This expectation setting will make it clear to management on what timeline and outcome to expect for given organisational support, runway and budget. At the same time, a platform development team must make it clear to the management that having a platform is a tradeoff. For example if the current way of working is that each team entirely choose their guidelines and architectural patterns, a development platform may enforce certain principles and guidelines using automation as much as possible, so no exception and exemptions. An example can be use of certain libraries for logging, or certain pipeline for deployment, certain number of active engineers per application and so on. Platform adoption A development platform like any other product will need adoption and use to get the feedback cycle going. Let’s say that the platform teams understood the painpoints and the platform is developed to not only remedy the painpoints but also ensure the viability of the application ecosystem over the next decad. If the platform is not being used and has an established feedback cycle it will be hard for it to grow and cover the majority of use-cases or valid viewpoints. There are some general approach in spreading the adoption of the platform and starting a feedback cycle. Platform team owning applications One approach that I have employed, and seems to work for adoption of the platform, is ensuring the platform team owns some programs built on top of the platform. This service can be the example to showcase how platform works and how does it help with the pain-points that it exists to remedy and how does it propel the organisation into the new ways of working and tech landscape. Of course, platform owners developing applications has some drawbacks: The application/s may turn into a an unintended blueprint and others may follow something that is not fully baked yet. The platform team too far get into habit of optimising the platform for their own use/level of competence. The platform team get too far distracted by the application to pay attention to the platform itself. This may result in mental overload for the team and far too many context switching. Platform team helping an application team convert Another approach is locating a team that struggling the most and is vocal the most about the painpoints, of course within reason, and help them convert their service development to use the new platform. This would require multiple criteria: The service in question does not have too many dependencies that can impact the conversion The service team is willing to take the steps. The steps that the platform is advocating for. For example code review, use of pipeline, test automation and so on that is a service provided by the platform. The platform team has some understanding of the domain that the service team is driving. Combination of the above two One thing to consider in the combination or even in the second approach is to identify the players and influencers and get buy-in from them. In every organisation there are people who are sitting far behind the scene without any title with more influence on what different teams may adopt that any line managers. Convincing them will multiply the rate of adoption. Next In the next instalments of this series I will write more about each aspect of the development platform mentioned here. ","date":"2022-03-22","objectID":"/2022/03/effective-internal-platform-product-manager/:0:1","tags":["product owner","PM","platform engineering","microservices"],"title":"Building a development platform","uri":"/2022/03/effective-internal-platform-product-manager/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Identity, something that we hear more often these days with the whole web 2.0 and social services and more and more web based public services growing around us. The identity notion is an integral part of a security system in distributed services. Developing effective software system require an effective security and access control system which java provides, not exactly in the way that it should be in 2011 but it does provide what is the bare bone necessity to develop applications and frameworks on top of it and benefit from its presence. The identity API is going to ease the interaction between the identity providers and those who consume the identity and trust the identity providers in addition to governing and managing the identity attributes. I was studying the JSR details and it seems to be covering everything required for the identity attributes governance and the required API for both ends of the usage including the client API the governing/producing API. The identity producing and consuming is not new and there are fair number of public identity producers like facebook, twitter, etc. and also products that system integrators can use like OpenAM as an open source product or gorilla Commercial software products like ORACLE identity management or IBM tivoli identity management software, etc. In a very simple set of words, the JSR 351: The Java Identity API will be as successful as it is going to be adopted. No adoption and it will endup dying some dark corner… Design a simple and elegant API and try to ship it with some free and easy to use service implementations and it may get some momentum, otherwise it will be a goner and people will stick with what they have. I like the new features that it is going to introduce in the decision making or authorization part but we should see how well it will be adopted by identity providers to develop the services that provides the interaction point between the JSR interface and their repositories. Pushing it as JSR wont really do that much without a wide adoption in the community. Look at how many implementation of the JSR 115 and JSR 196 exits to be plugged into application servers supporting the contract and you will get what I am referring to by community adoption. ","date":"2011-10-24","objectID":"/2011/10/some-thought-on-jsr-351-java-identity-api/:0:0","tags":["Java","JCP"],"title":"My thoughts on JSR 351, Java Identity API","uri":"/2011/10/some-thought-on-jsr-351-java-identity-api/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"Well, as many of us already know Oracle submitted the JSR for Java EE 7 which is sort of an umbrella JSR for many update in already existing specifications, new versions of some JSRs and some completely new JSRs which will be part of the grand Java EE 7 - JSR 342. One of these JSRs is the JSR 343 which introduces a new version of JMS into the platform as an evolution of its previous version, JSR-914, and will unify the JMS usage with what added to the platform in the past 8 years. The following represent some very simple usecases of JMS in the enterprise while complex multiphase transactional usecases are not unusual when MDBs and XA data sources are involved. JMS API architecture JMS itself is for asynchronous communication and widely used to communicate some execution instructions from one node or point to another or a set of other points. For example long running queries can be queued using a JMS queue to get processed by another point in the system while the query client is not blocked for the query result. Or it can be used to communicate a common set of instructions to many interested parties which may or may not be around the communication happens, durable subscriptions and persisted topics. For example when clients need to get an ordered set of update messages to update a localcache when they get online after some times. Each client will get its own copy of messages it should receive when getting online. JMS API provides enough functionalities to realize most of our design out of the specification and the minor features and functionalities not included in the JSR while required by some designs are covered by the vendor specific pack of enhancement and tweaks provided in the broker level and through the vendor specific API. You may ask if the current JMS API provides all we need, why a new JSR should be put on the table, the answer mainly relies on the theme for Java EE 7 which is making Java EE more cloud friendly and sort of cloud enabled by nature rather than by product. The details of JMS 2.0 spec goals are listed at the JSR homepage but a brief list can be seen as follow: Community requested features and enhancements. Make the JSR more cloud friendly based on how Java EE 7 will define “to be cloud friendly” Cleanup of some ambiguity in the relation of JMS with other Java EE specs. Make the API easier to use, more annotations and more generics will be involved for the least of the things or maybe reducing number of boxes and lines in the aove figure could help many to start with the API faster. Make necessary changes to benefit from the JSR-299 or Contexts and Dependency Injection to easier and more unified use of API. In the follow up posts I will iterate over each one of these bullet points in more details. I am member of the JMS 2.0 expert group but this post or any other post in my personal blog does not reflect the expert group opinion or the opinion of my employer on the subject unless you could not see this paragraph at the end of the post :-). ","date":"2011-05-17","objectID":"/2011/05/jms2-jsr/:0:0","tags":["Java","JCP","JMS"],"title":"Brief overview of JSR 343: JavaTM Message Service 2.0","uri":"/2011/05/jms2-jsr/"},{"categories":["SysAdm","Old Blog Migrated Content"],"content":"SMF services are basically daemons staying in background and waiting for the requests which they should server, when the request come the daemon wake ups, serve the request and then wait for the next request to come. The services are building using software development platforms and languages but they have one common aspect which we are going to discuss here. The service manifests which describe the service for the SMF and let the SMF manage and understand the service life cycle. To write a service manifest we should be familiar with XML and the service manifest schema located at /usr/share/lib/xml/dtd/service_bundle.dtd.1. This file specifies what elements can be used for describing a service for the SMF. Next thing we need is a text editor and preferable an XML aware text editor. The Gnome gedit can do the task for us. The service manifest file composed of 6 important elements which are listed below: The service declaration: specifies the service name, type and instantiation model Zero or more dependency declaration elements: Specifies the service dependencies Lifecycle methods: Specifies the start, stop and refresh methods Property groups: Which property groups the service description has. Stability element: how stable the service interface is considering version changes Template element: more human readable information for the service. To describe a service, first thing we need to do is identifying the service itself. Following snippet shows how we can declare a service named jws. \u003cservice name='network/jws’ type='service' version='1'\u003e \u003csingle_instance/\u003e The first line specifies the service name, version and its type. The service name attribute forms the FMRI of the service which for this instance will be svc:/network/jws. In the second line we are telling SMF that it should only instantiate one instance of this service which will be svc:/network/jws:default. We can use the create_default_instance element to manipulate automatic creation of the default instance. All of the elements which we are going to mention in the following sections of this article are immediate child elements of the service element which itself is a child element of the service_bundle element. The next important element is dependency declaration element. We can have one or more of this element in our service manifest. \u003cdependency name='net-physica' grouping='require_all ' restart_on='none' type='service'\u003e \u003cservice_fmri value='svc:/network/physical'/\u003e \u003c/dependency\u003e Here we are telling that our service depends on the svc:/network/physical service and this service needs to be online before our service can start. Some of the values for the grouping attribute are as follow: The require_all which represent that all services marked with this grouping must be online before our service came online The require_any which represents that any of the services in this grouping suffice and our service can become online if one of them is online The optional_all presence of the services marked with this grouping is optional for our service. Our service works with or without them. The exclude_all: specifies the services which may have conflict with our service and we cannot become online in presence of them The next important elements are specifying how the SMF should start, stop and refresh the service. For these tasks we use three exec_method elements as follow: \u003cexec_method name='start' type='method' exec='/opt/jws/runner start' timeout_seconds='60'\u003e \u003c/exec_method\u003e This is the start method, SMF will invoke what the exec attribute specifies when it want to start the service. \u003cexec_method name='stop' type='method' exec=':kill' timeout_seconds='60'\u003e \u003c/exec_method\u003e The SMF will terminate the process it started for the service using a kill signal. By default it uses the SIGTERM but we can specify our own signal. For example we can use ‘kill -9’ or ‘kill -HUP’ or any other signal we find appropriate for our service termination. \u003cexec_method name=‘refresh’ type=‘method’ exe","date":"2011-01-31","objectID":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/:0:0","tags":["SMF","Solaris"],"title":"Writing Solaris Service Management Facility (SMF) service manifest","uri":"/2011/01/authoring-solaris-service-management-facility-smf-service-manifest/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"In this article we will study how we can use the fmadm command to get the list of faulty components along with the detailed information about the fault. Before starting this article we should have a command console open and then we can proceed with using the fmadm command. The most basic form of using fmadm command is using its faulty subcommand as follow \\# fmadm faulty When we have no error in the system, this command will not show anything and exit normally but with a faulty component the output will be different, for example in the following sample we have a faulty ZFS pool because some of its underlying devices are missing. fmadm command output Starting from top we have: Identification record: This record consisting of the timestamp, a unique event ID, a message Id letting us know which knowledge repository article we can refer to for learning more about the problem and troubleshooting and finally the fault severity which can be Minor or Major. Fault Class: This field allows us to know what is the device hierarchy causing this fault Affects: tells us which component of our system is affected and how. In this instance some devices are missing and therefore the fault manager takes the Zpool out of service. Problem in: shows more details about the problem root. In this case the device id. Description: this field refer us to the a knowledge base article discussing this type of faults Response: Shows what action(s) were executed by fault manager to repair the problem. Impact: describe the effect of the fault on the overall system stability and the component itself Action: a quick tip on the next step administrator should follow to shoot the problem. This step is fully described in the knowledge base article we were referred in the description field. Following figure shows the output for proceeding with the suggested action. fmadm taking action As we can see the same article we were referred to, is mentioned here again. We can see that two of the three devices have failed and fpool had no replica for each of those failed device to replace them automatically. If we had a mirrored pool and one of the three devices were out of service, the system could automatically take corrective actions and replace continue working in a degraded status until we replace the faulty device. The fault management framework is a plug-able framework consisting of diagnosis engines and subsystem agents. Agents and diagnosis engine contains the logic for assessing the problem, performing corrective actions if possible and filing the relevant fault record into the fault database. To see a list of agents and engines plugged into the fault management framework we can use config subcommand of the fmadm command. Following figure shows the output for this command. fmadm configuration As we can see in the figure, there are two engines deployed with OpenSolaris, eft and the zfs-diagnosis. The eft, standing for Eversholt Fault Diagnosis Language, is responsible for assessing and analyzing hardware faults while the zfs-diagnosis is a ZFS specific engine which analyzes and diagnoses ZFS problems. The fmadm is a powerful utility we which can perform much more than what we discussed. Here we can see few other tasks we can perform using the fmadm. We can use the repaired subcommand of the fmadm utility to notify the FMD about a fault being repaired so it changes the component status and allows it to get enabled and utilized. For example to notify the FMD about repairing the missing underlying device of the ZFS pool we can use the following command. \\# fmadm repaired zfs://pool=fpool/vdev=7f8fb1c77433c183 We can rotate the log files created by the FMD when we want to keep a log file in a specific status or when we want to have a fresh log using the rotate subcommand as shown below. \\# fmadm rotate errlog | fltlog The fltlog and errlog are two log files residing in the /var/fm/fmd/ directory storing all event information regarding faults and the errors causing them. To","date":"2011-01-12","objectID":"/2011/01/solaris-fault-administration-using-fmadm-command/:0:0","tags":["Solaris","ZFS"],"title":"Solaris fault administration using fmadm command","uri":"/2011/01/solaris-fault-administration-using-fmadm-command/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Performance, performance, performance; this is what we hear in all software development and management sessions. ZFS provides few utility commands to monitor one or more pools’ performance. You should remember that we used fsstat command to monitor the UFS performance metrics. We can use iostat subcommand of the zpool command to monitor the performance metrics of ZFS pools. The iostat subcommand provides some options and arguments which we can see in its syntax shown below: iostat \\[-v\\] \\[pool\\] ... \\[interval \\[count\\]\\] The -v option will show detailed performance metrics about the pools it monitor including the underlying devices. We can pass as many pools as we want to monitor or we can omit passing a pool name so the command shows performance metrics of all commands. The interval and count specifies how many times we want the sampling to happen what is the interval between each subsequent sampling. For example we can use the following command to view detailed performance metrics of fpool for 100 times in 5 seconds interval we can use the following command. \\# zpool iostat -v fpool 5 100 The output for this command is shown in the following figure. Monitor ZFS pool performance The first row shows the entire pool capacity stats including how much space were used upon the sampling and how much was available. The second row shows how many reads and writes operations performed during the interval time and finally the last column shows the band width used for reading from this pools and writing into the pool. The zpool iostat retrieve some of its information from the read-only attributes of the requested pools and the system metadata and calculate some other outputs by collecting sample information on each interval. ","date":"2010-12-30","objectID":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/:0:0","tags":["Solaris","ZFS"],"title":"Monitoring ZFS pools performance using zpool iostat","uri":"/2010/12/monitoring-zfs-pools-performance-using-zpool-iostat/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. We are going to add a logical interface to e1000g1 interface with a 10.0.2.24 as its static IP address. Before doing so let’s see what network interface we have using the ifconfig command. 3180_04_12 Now to add the logical interface we only need to execute the following command: \\# ifconfig e1000g1 addif 10.0.2.24/24 up Invoking this command performs the following tasks: Create a logical interface named e1000g1:1. The naming schema for logical interfaces conforms with interface_name:logical_interface_number which the number element can be from 1 to 4096. Assign 10.0.2.24/24 as its IP address, net mask and broadcast address. Now if we invoke ifconfing -a command the output should contain the logical interface status as well. The following figure shows a fragment of ifconfig -a command. 3180_04_13 Operating system does not retain this configuration over a system reboot and to make the configuration persistent we need to make some changes in the interface configuration file. For example to make the configuration we applied in this recipe persistent the content of /etc/opensolaris.e1000g1 should something similar to the following snippet. 10.0.2.23/24 addif 10.0.2.24/24 The first line as we discussed in recipe 3 of this chapter assign the given address to this interface and the second like adds the logical interface with the given address to this interface. To remove a logical interface we can simply un-plumb it using the ifconfig command as shown below. \\# ifconfig e1000g1:1 unplumb When we create a logical interface, OpenSolaris register that interface in the network and any packet received by the interface will be delivered to the same stack that handles the underlying physical interface. Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. We should have one network interface configured in our system in order to create additional logical interfaces. ","date":"2010-12-09","objectID":"/2010/12/managing-logical-network-interfaces-in-solaris/:0:0","tags":["Solaris","Networking"],"title":"Managing Logical network interfaces in Solaris","uri":"/2010/12/managing-logical-network-interfaces-in-solaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Link aggregation or commonly known Ethernet bonding allows us to enhance the network availability and performance by combining multiple network interfaces together and form an aggregation of those interface_names which act as a single network interface with greatly enhanced availability and performance. When we aggregate two or more network interfaces, we are forming a new network interface on top of those physical interfaces combined in the link layer. We need to have at least two network interfaces in our machine to create a link aggregation. The interfaces must be unplumb-ed in order for us to use them in a link aggregation. Following command shows how to unplumb a network interface. \\# ifconfig e1000g1 down unplumb We should disable the NWAM because link aggregation and NWAM cannot co-exist. \\# svcadm disable network/physical:nwam The interfaces we want to use in a link aggregation must not be part of virtual interface; otherwise it will not be possible to create the aggregation. To ensure that an interface is not part of a virtual interface checks the output for the following command. \\# dladm show-link Following figure shows that my e1000g0 has a virtual interface on top of it so it cannot be used in an aggregation. 3180_04_14 To delete the virtual interface we can use the dladm command as follow \\# dladm delete-vlan vlan0 The link aggregation as the name suggests works in the link layer and therefore we will use dladm command to make the necessary configurations. We use create-aggr subcommand of dladm command with the following syntax to create aggregation links. dladm create-aggr \\[-l interface_name\\]\\* aggregation_name In this syntax we should have at least two occurrence of -l interface_name option followed by the aggregation name. Assuming that we have e1000g0 and e1000g1 in our disposal following commands configure an aggregation link on top of them. \\# dladm create-aggr -l e1000g0 -l e1000g1 aggr0 Now that the aggregation is created we can configure its IP allocation in the same way that we configure a physical or virtual network interface. Following command plumb the aggr0 interface, assign a static IP address to it and bring the interface up. \\# ifconfig aggr0 plumb 10.0.2.25/24 up Now we can use ifconfig command to see status of our new aggregated interface. \\# ifconfig aggr0 The result of the above command should be similar to the following figure. 3180_04_15 To get a list of all available network interfaces either virtual or physical we can use the dladm command as follow \\# dladm show-link And to get a list of aggregated interfaces we can use another subcommand of dladm as follow. \\# dladm show-aggr The output for previous dladm commands is shown in the following figure. 3180_04_16 We can change an aggregation link underlying interfaces by adding an interface to the aggregation or removing one from the aggregation using add-aggr and remove-aggr subcommands of dladm command. For example: \\# dladm add-aggr -l e1000g2 aggr0 \\# dladm remove-aggr -l e1000g1 aggr0 The aggregation we created will survive the reboot but our ifconfig configuration will not survive a reboot unless we persist it using the interface configuration files. To make the aggregation IP configuration persistent we just need to add create /etc/hostname.aggr0 file with the following content: 10.0.2.25/24 The interface configuration files are discussed in recipe 2 and 3 of this chapter in great details. Reviewing them is recommended. To delete an aggregation we can use delete-aggr subcommand of dladm command. For example to delete aggr0 we can use the following commands. \\# ifconfig aggr0 down unplumb \\# dladm delete-aggr aggr0 As you can see before we could delete the aggregation we should bring down its interface and unplumb it. In recipe 11 we discussed IPMP which allows us to have high availability by grouping network interfaces and when required automatically failing over the IP address of any failed interface to a healthy one. In this recip","date":"2010-11-25","objectID":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/:0:0","tags":["Solaris","Networking"],"title":"Configuring Solaris Link Aggregation (Ethernet bonding)","uri":"/2010/11/configuring-solaris-link-aggregation-ethernet-bonding/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will look at how the directory tree walker and the directory stream reader works. These two features are another couple of long requested features which was not included in the core java before Java 7. First, lets see what directory stream reader is, this API allows us to filter content of a directory on the file system and extract the file names that matches our filter criteria. The feature works for very large folders with thousands of files. For filtration we can use PathMatcher expression matching the file name or we can filter the directory content based on the different file attributes. for example based on the file permissions or the file size. Following sample code shows how to use the DirectoryStream along with filtering. For using the PathMatcher expression we can just use another overload of the newDirectoryStream method which accepts the PathMatcher expression instead of the filter. public class DirectoryStream2 { public static void main(String args\\[\\]) throws IOException { //Getting default file system and getting a path FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/usr/bin\"); //creating a directory streamer filter DirectoryStream.Filter filter = new DirectoryStream.Filter () { public boolean accept(Path file) throws IOException { long size = Attributes.readBasicFileAttributes(file).size(); String perm = PosixFilePermissions.toString(Attributes.readPosixFileAttributes(file).permissions()); if (size \u003e 8192L \u0026\u0026 perm.equalsIgnoreCase(\"rwxr-xr-x\")) { return true; } return false; } }; // creating a directory streamer with the newly developed filter DirectoryStream ds = p.newDirectoryStream(filter); Iterator it = ds.iterator(); while (it.hasNext()) { Path pp = it.next(); System.out.println(pp.getName()); } } } The above code is self explaining and I will not explain it any further than the in-line comments. Next subject of this entry is directory tree walking or basically file visitor API. This API allows us to walk over a file system tree and execute any operation we need over the files we found. The good news is that we can scan down to any depth we require using the provided API. With the directory tree walking API, the Java core allows us to register a vaster class with the directory tree walking API and then for each entry the API come across, either a file or a folder, it calls our visitor methods. So the first thing we need is a visitor to register it with the tree walker. Following snippet shows a simple visitor which only prints the file type using the Files.probeContentType() method. class visit extends SimpleFileVisitor { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) { try { System.out.println(Files.probeContentType(file)); } catch (IOException ex) { Logger.getLogger(visit.class.getName()).log(Level.SEVERE, null, ex); } return super.visitFile(file, attrs); } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) { return super.postVisitDirectory(dir, exc); } @Override public FileVisitResult preVisitDirectory(Path dir) { return super.preVisitDirectory(dir); } @Override public FileVisitResult preVisitDirectoryFailed(Path dir, IOException exc) { return super.preVisitDirectoryFailed(dir, exc); } @Override public FileVisitResult visitFileFailed(Path file, IOException exc) { return super.visitFileFailed(file, exc); } } As you can see we extended the SimpleFileVisitor and we have visitor methods for all possible cases. Now that we have the visitor class, the rest of the code is straight forward. following sample shows how to walk over _/home/_masoud directory down to two levels. public class FileVisitor { public static void main(String args\\[\\]) { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud\"); visit v = new visit(); Files.walkFileTree(p, EnumSet.allOf(FileVisitOption.class), 2, v); } } You can grab the latest version of Java 7 aka Dolphin from here and from the same page you can grab the late","date":"2010-08-23","objectID":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 6: Filtering directory content and walking over a file tree","uri":"/2010/08/introducing-nio-2-jsr-203-part-6-filtering-directory-content-and-walking-over-a-file-tree/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"For long time Java developers used in-house developed solutions to monitor the file system for changes. Some developed general purpose libraries to ease the task of others who deal with the same requirement. Commercial and free/ open source libraries like http://jnotify.sourceforge.net/, http://jpathwatch.wordpress.com/ and http://www.teamdev.com/jxfilewatcher among others. Java 7 comes with NIO.2 or JSR 203 which provides native file system watch service. The watch service provided in Java 7 uses the underlying file system functionalities to watch the file system for changes, so if we are running on Windows, MacOS or Linux… we are sure that the watch service is not imposing polling overhead on our application because the underlying OS and file system provides the required functionalities to allow Java to register for receiving notification on file system changes. If the underlying file system does not provide the watch-ability, which I doubt it for any mainstream file system, Java will fall back to some rudimentary polling mechanism to keep the code working but the performance will degrade. From the mentioned libraries the jpathwatch API is the same as the Java 7 APIs to make it easier to migrate an IO based application from older version of Java to Java 7 when its time arrives. The whole story starts with WatchService which we register our interest in watching a path using it. The WatchService itself is an interface with several implementatins for different file system and operating systems. We have four class to work with when we are developing a system with file system watch capability. A Watchable: A watchable is an object of a class implementing the Watchable interface. In our case this is the Path class which is the one of the central classes in the NIO.2 A set of event types: We use it to specify which types of events we are interested in. For example whether we want to receive creation, deletion, … events. In our case we will use StandardWatchEventKind which implements the WatchEvent.Kind. An event modifier: An event modifier that qualifies how a Watchable is registered with a WatchService. In our case we will deal with nothing specific up to now as there is no implementation of this interface included in the JDK distribution. The Wacher: This is the watcher who watch some watchable. In our case the watcher watches the File System for changes. The abstract class is java.nio.file.WatchService but we will be using the FileSystem object to create a watcher for the File System. Now that we know the basics, let’s see how a complete sample will look like and then we will break down the sample into different parts and discuss them one by one. import java.io.IOException; import java.nio.file.FileSystem; import java.nio.file.FileSystems; import java.nio.file.Path; import java.nio.file.StandardWatchEventKind; import java.nio.file.WatchEvent; import java.nio.file.WatchKey; import java.nio.file.WatchService; import java.util.List; import java.util.logging.Level; import java.util.logging.Logger; public class WatchSer { public static void main(String args\\[\\]) throws InterruptedException { try { FileSystem fs = FileSystems.getDefault(); WatchService ws = null; try { ws = fs.newWatchService(); } catch (IOException ex) { Logger.getLogger(WatchSer.class.getName()).log(Level.SEVERE, null, ex); } Path path = fs.getPath(\"/home/masoud/Pictures\"); path.register(ws, StandardWatchEventKind.ENTRY_CREATE, StandardWatchEventKind.ENTRY_MODIFY, StandardWatchEventKind.OVERFLOW, StandardWatchEventKind.ENTRY_DELETE); WatchKey k = ws.take(); List\u003e events = k.pollEvents(); for (WatchEvent object : events) { if (object.kind() == StandardWatchEventKind.ENTRY_MODIFY) { System.out.println(\"Modify: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_DELETE) { System.out.println(\"Delete: \" + object.context().toString()); } if (object.kind() == StandardWatchEventKind.ENTRY_CREATE) { System.out.println(\"Created: \" + object.co","date":"2010-08-10","objectID":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 5: Watch Service and Change Notification","uri":"/2010/08/introducing-nio-2-jsr-203-part-5-watch-service-and-change-notification/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What is a file system File systems make it possible to store and retrieve files and containing data into storages like hard disks, optical disks and other types of storages. OpenSolaris support both legacy file systems like UNIX File System (UFS) and its own file system called Zettabyte File System (ZFS). Following figure show how UFS and other legacy file systems work. As you can see we can partition each storage device into one or more volumes with different file systems. For example one hard disk may have an NTFS volume, a UFS volume and an EXT4 volume. In contrast with legacy file systems, ZFS uses concept of resource pooling. In ZFS one or more devices, volumes or files can be combined to form a pool of storages, hence called zpool. Then file systems are built on top of this pool. For example we can create a file system capable of storing a 32 TB file on top of a zpool created on using 16 hard disks, 2 TB each. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:1:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"What OpenSolaris supported file systems are OpenSolaris as a pioneer OS for enterprises supports several file systems in addition to pushing its flagship and unique file system named ZFS. Each supported file system is either supported because of backward compatibility or to provide the administrators with a wider range of choices when it come to formatting the storages. The UNIX file system (UFS) is the common file system on UNIX like operating systems. The Personal Computer File System (PCFS) supports FAT12, FAT16 and FAT32 formatted disks. The High Sierra File System (HSFS) allows us to access files on High Sierra or ISO 9660 format CD-ROM disks. The Temporary File Storage Facility (TMPFS) provides a temporary file system in RAM or SWAP space, we can access the file system as a normal file system but in the background it is stored in memory instead of persisted storage. The integration of Storage and Archive Manager and Quick File System (SAM-QFS) is provided so we can have access to a clustered archiving solution in OpenSolaris. ZFS is the flagship file system of OpenSolaris. The file system is a combination of file system and logical volume manager. Some of the ZFS highly steamed features are discussed below. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:2:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"ZFS unique features ZFS is a unique file system which addresses all concerns in the enterprise data centers. We will discuss these features in this section. Storage pools ZFS file systems are built on top of storage pools which are built on top of a group of virtual devices. A virtual device can vary from an entire hard drive down to a file. Capacity ZFS is a 128-bit file system which can store 1.84 × 10^19^ times more data than current 64-bit systems. Copy-on-write transactional model No data get overwritten when updated and instead new data will be wrote in separate blocks and then the pointers pointing to old data will be changed to point to new blocks. Multiple write operation can be grouped together to enhance the performance. Snapshots ZFS does not overwrite the data when they are changed; this feature is used to keep snapshots of file system for later user. Clones A clone is a writeable snapshot which its block content changes when a block is changed in another clone. Deduplication Deduplication is a file system level compression method in which redundant data is eliminated. For example in a typical enterprise mail server one email attachment may find its way into hundreds of users mail boxes, Deduplication will store the actual file once and all other instance of the file are referenced to the same actual file. We use a console to study all commands discussed in this chapter. So before we can execute any of these commands we should be in terminal environment. To activate the terminal environment we can: Hit ALT+F2 to summon the run window and then type console to open a console window Press ALT+CTRL+F2 to switch the workspace to another terminal and use it to practice the command. 1.Using basic file system commands OpenSolaris supports multiple file systems but to access all of them we need to know how to use some basic commands. Following table includes these basic commands along with their descriptions and samples showing how we can use them. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:2:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will study some of the very basic file system commands in this recipe. The commands need to be invoked in the terminal window. To enter the terminal window Hit ALT+F2 to summon the run window and then type console to open a console window. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:3:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We have some basic commands which we should know to deal with the file system in a console. Some of these commands are introduced in the following table. Command Sample cd We can change the current directory using cd command. For example : cd /home/masoud: switch the current directory to /home/masoud cd ..: switch one level up from the current directory. ls We can use this command to get a list of directories and files. It works similar to ls in Linux or dir in MS-DOS. For example: ls: shows the list of all files and directories except hidden ones. ls -al: shows the list of all files and directories along with detailed permissions and creation dates. ls -al /home: shows content of the given path as explained above. mkdir We can create new directories using mkdir command anywhere we have write permission. For example: mkdir safe: will create a directory named safe inside the current directory. mkdir -p /home/masoud/safe/personal/cash: When using -p it creates all directories required to form the mentioned path. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:4:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. All commands introduced in this recipe rely on the metada stored for the file system to perform its job. For example the ls command checks the metadata available for the path in question to show content of the given path. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:5:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can get a short or long help for any command in OpenSolaris using the commands help messages or the provided man pages. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing the help message for a command We can pass --help parameter to almost any command in OpenSolaris to see a short help message including usage syntax and list of options along with their descriptions. For example: \\# mkdir \\--help ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Viewing extensive manual of a command We can see complete and extensive manual pages for any command in OpenSolaris by passing the command name to man command. For example to view the man pages for mkdir we can use the following command: \\# man mkdir ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Some other commands to review Following list includes some other commands which we need to know. Command to copy files and directories: cp Command to delete file and directories: rm Command to move or rename files and directories: mv 2. Formatting disks In this task we will learn how we can format and label disks prior to creating a file system on them. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:6:3","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready When we attach a disk to an OpenSolaris machine OpenSolaris will assign a name to disk and its partitions or slices. All disks have assigned names under /dev/dsk and /dev/rdsk which lets us access the disk at block level in the first name and at bye level using the second name. The byte level access method is also called raw access method. The disks names are usually compatible with C#T#D#S# or C#T#D#P#. The C stands for controller and the number part is a hex number like 4 or F4. A SATA connector on the mother board is sample of a controller. The T which is optional stands for target, the D stands for disk and the number after it points to one of the disks attached to the controller and the digit after S or P point to the slice number or the partition number. If the partition is from Solaris type we call it slice and its identifier is S# while if the partition is non-Solaris it will be identified with the P# ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:7:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. We can use format and fdisk commands to format and label the disks prior to using them. We will discuss format command as it also includes fdisk. To enter into the format command shell we should invoke format command in a console which will open the format shell. The initial screen as we can see in the following figure shows currently attached disks and allows us to select which disk we want to operate on. I have two hard disks connected to SATA controller and one to IDE controller. We can enter 1 and the list of available commands appears which is as shown in the following figure: The partition, format and fdisk are most common operations which we usually use. Typing each one of them opens another set of options which we can choose to commence with the operation. Instead of typing the complete command name, we can just enter its first character. For example we can quit from format shell to operating system shell or get back to format shell from its subcommands using q instead of quit. If we select a brand new disk in the format menu, the format command informs us that the disk has no partition and we need to use fdisk command to create some partitions before proceeding. To use fdisk we just need to enter fdisk subcommand and create partitions as we need or accept the default partitioning which fdisk command suggest. The default partitioning will create one Solaris2 partition occupying the entire disk. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a Solaris2 partition We can create a Solaris 2 partition using the following steps in format shell: Enter disk command and then select 1 to operate on c9t0d0 Enter fdisk command and when it asked whether we want to use the default partition select y so it create one Solaris2 partition occupying the whole disk. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating a FAT32 partition Now let's create a FAT32 partition in the second disk using the following steps in the format shell: Enter disk command and then select 2 to operate on c9t1d0 Enter fdisk command and when it asked whether we want to use the default partition select n so it will enter the fdisk subcommand shell. When asked for selection enter 1 to create a new partitioning When asked for the partition type enter C to create a FAT32 partition When asked for size, enter 100 to use the whole disk capacity Enter y to activate the partition Now that the partition creation is complete enter 5 to apply the changes and exit fdisk shell. Now quit the format shell by entering q command. After creating the partitions we need to apply a file system on them before actually using them. To apply the file system on Solaris slice we can use newfs command as follow. \\# newfs /dev/rdsk/c9t0d0s2 And to apply the file system on the FAT32 partition we can use the mkfs with some additional parameters as follow: \\# mkfs -F pcfs -o fat=32 /dev/rdsk/c9t1d0p0:c ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Formatting removable disks. We can use format and fdisk commands to partition a removable media like a memory card or a thumb drive. For more information and detailed instructions check http://docs.sun.com/app/docs/doc/817-5093/gafmj?a=view ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:3","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Growing a UFS We can use growfs command to change a UFS size provided that when creating the file system we use the -T option with newfs command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:8:4","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we create a partition in a disk, the layout we specified will be stored in the partition table area of the hard disk located in the beginning of the disk. The partition table contains all information like partition type, and its size which we specify when we create the partition. A good place to learn more about partitioning in general is http://en.wikipedia.org/wiki/Disk_partitioning ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:9:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next recipe we will learn how we can mount and put the partitions we created here in use. 3. Mounting and unmounting file systems Before we can use a file system we should mount the slices or partitions we created in the operating system. When we mount a partition or a slice we are making it accessible trough a directory which it is mounted to and we unmount the file system we are just detaching the file system from OS. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:10:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In previous recipes we create two partitions and applied a file system on each one of them. In this recipe we will mount them to some directories and start using them. To mount a file system we need a mount point. A mount point is a directory which we use it to access the mounted file system. So let’s create the mount point before mounting the partitions. \\# mkdir /mnt/fat /mnt/sol This command creates two directories named fat and sol inside the /mnt directory. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:11:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To mount the Solaris2 partition we created in previous recipe we can use the mount command as follow: \\# mount /dev/dsk/c9t0d0s2 /mnt/sol This command mounts the Solaris2 slice to /mnt/sol and we can create files and directories inside it using its mount point. For example: \\# mkdir /mnt/sol/sample If we get the list of /mnt/sol/ we will see something like the following figure. Mounting the FAT32 partition is a bit trickier than Solaris2 partition as you can see in the command below which mounts the FAT32 partition we created before to /mnt/fat. \\# mount -F pcfs /dev/dsk/c9t1d0p1 /mnt/fat As you can see we are passing the file system using -F parameter to let the command know which file system is applied on the partition. Using the following command we can create a text file containing “Hello FAT32 Partition” inside the FAT32 partition. \\# echo Hello FAT32 Partition \\\u003e /mnt/fat/sample.txt We can check partition content using the ls command as we used for Solaris2 partition. When we no longer need the file system or we want to take it down for other operations like checking the file system or moving the drive we can use umount command to detach the file system from the operating system. The umount command operates both on the device name like /dev/dsk/c9t1d0p1 and on the mount point like /mnt/fat. For example to unmount the FAT32 partition we can use the following command \\# umount /mnt/fat We can not use the umount or mount command when the file system is busy, for example when an application like terminal is accessing the file system or just has a path inside that file system open. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:12:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we mount a partition, we are asking the operating system’s file system to allow us access the content of the mounted partition through the directory we specified as the mount point. Operating system forward any changes we made to that mount point into the underlying partition ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:13:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Mounted file systems using mount command stays mounted for the period of the current operating system session and will not persist the system reboot and after each reboot we will need to mount them again. We can define the mount points in the mount table inside the /etc/vfstab file to let OpenSolaris pick them an execute them. Each row of the mount table represents one mount command. The /etc/vfstab is a plain text file with the following format. device device mount FS fsck mount mount to mount to fsck point type pass at boot options An example row for the mount table to mount our FAT32 partition is as follow: /dev/dsk/c9t1d0p1 /dev/rdsk/c9t1d0p1 /mnt/fat pcfs 5 yes - Let's see description of each field in the sample row of mount table. The block device name to mount. The raw device name to be used by fsck to check the disk. The mount point to mount the device. The file system type to mount the device under it. Whether the file system should be checked at startup or not. Whether to mount the device at boot or not. Additional options for mounting. Options like enforcing a read-only mount, enabling logging and so on. We applied no option. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Using fuser to see who is accessing a file system We can use fuser command to check which process is accessing the file system. This command comes handy when we want to unmount a device and umount throw us an error saying the device is busy. For example to check which process is using the /mnt/fat we can use the following command. \\# fuser /dev/dsk/c9t1d0p1 The above command results in something similar to the following figure if we have, for example a terminal window at /mnt/fat we will discuss fuser and pfiles, which gives us more details about who is using what part of the file system, in more details in chapter. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Mount options Following table shows important options we can use with mount command or in the mount table. Options Description nologging Disable the metadata operation logging. noatime Disable recording files access time. quota Enable disk space usage quotas. forcedirectio Force I/O requests to bypass the file system cache and therefore the operation can be faster for application with internal caches like Oracle and MySQL databases ## See also More information about the mount and umount commands is available in their man pages. And for the mount table we can refer to its official document at http://docs.sun.com/app/docs/doc/805-7228/6j6q7uev3?a=view 4. Monitoring UFS Monitoring is an integral part of all enterprise level activities and software and hardware systems are not an exception. OpenSolaris provide several utilities for monitoring the file system activities. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:14:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Following three commands are provided to monitor and check status of the file system. The df command report file system disk space usage. The du command summarizes disk usage of each directory recursively including the size of included files. The fsstat command reports kernel file operation activity by the file system type or by the mount point. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:15:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. To view the amount of disk usage and free space on each disk (mount point) we can use the following command: \\# df -hl The result of using the command is shown in the following figure. To view how much space each file and directory occupied we can use the du command. For example to see how much space is used by /export/home/masoud/Documents and its subdirectories and files we can use the following command. \\# du -ah /export/home/masoud/Documents/ The command result is something like the following figure. We can use -b parameter to make the command show the size in bytes instead of wrapping it based on the block size. Another useful parameter is -s which summarizes the space consumption for the directory. Another monitoring command which we will discuss is fsstate which can show live statistics about file system usage. For example to see default monitoring factors for / mount point we can use the following command. \\# fsstat / 5 200 This command shows the statistics for 200 times and gathers the sampling data each 5 second. A sample output is like the following figure. In the resulting statistics we can see how many new files are created in the sample period, how many calls for getting and setting attributes placed, how many look-up, read, write, etc operations are performed. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:16:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. Commands that collect and shows statistics about the file systems uses multiple sources for the statistics they show including the operating system internal messages, the file system read-only properties and calculation they make to provide the users with sensible information. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:17:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. There are plenty of options which we can use to customise the result of each monitoring command. To available options and their usages are included in the man pages and short help of each command. There is another monitoring command we can use for monitoring IO operations called iostat. More information about iostat is available in its man pages. 5. Backing up and restoring UFS In previous recipe we saw how we can create, mount and monitor UFS partitions and file systems. In this recipe we will learn how to backup, and restore a UFS. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:18:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The command we use to create a backup is ufsdump and ufsrestore. These two commands can create a dump of a UFS like /mnt/sol and then restore it when required. We should backup a file system when it is not mounted or mounted in read-only mode to ensure that during the dump operation no file has changed. Prior to starting this recipe I copied some files into the UFS slice to make the backup and restore operations more sensible. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:19:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The ufsdump create backup of the file system. The dump can be an incremental dump only containing files changed after the previous dump or it can be a full dump containing all files. We can dump the file system into tapes or we can dump it into a binary file understandable for restore command. Following syntax shows how we can use ufsdump command. ufsdump \\[options\\] \\[list of arguments\\] files_to_dump In this command schema: The options are a single string of one-letter ufsdump options, some of important options are listed in this recipe table. The arguments may be multiple strings whose association with the options is determined by order. That is, the first argument goes with the first option that takes an argument and so on. An example of arguments is the -f parameter argument which specifies where to dump into. The files_to_dump is required and must be the last argument on the command line. For example we can create a full dump of a Solaris2 file system located on /dev/dsk/c9t0d0s2 and store it the FAT32 partition we mounted in /mnt/fat using the following command. \\# ufsdump 0fu /mnt/fat/dumped_here /dev/dsk/c9t0d0s2 The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Following figure shows the sample output of this command. Important list of options is shown in the following table. Option Description 0-9 Specify the dump level. Level 0 means a full dump while other numbers means incremental dump after the previous level. For example if we have a dump level 1 created on Friday, a dump level 1 on Monday will dump all changes happened between Friday and Monday. f Specify the file which we want to dump the dump files into it. This option need a argument in the argument list u Update the dump record. Add an entry to the file /etc/dumpdates, for each file system successfully dumped. v Verify the dump after created to ensure its integrity. c Set the defaults for cartridge instead of the standard half-inch reel L Sets the tape label to string, instead of the default value which is none. This option needs a string argument in the arguments list. The ufsdump command creates the dumps inside the /dev/rmt/# when no dump file is specified using the f option. Now to restore the dump we have just created we can use the ufsrestore command. The ufsrestore command works in interactive and headless mode. In the interactive mode it walk us through the procedure step by step and let us view a dump file content and select what to restore. An example of using the ufsrestore in interactive more is shown in the following figure. As you can see in the figure we can use some common commands like ls, add, and extract in the ufsrestore shell to get the content list, add our selected files to list of extractees and then extract them into the current working directory using the extract command of the ufsrestore shell. The following table explains the options used with the ufsrestore command above. To use the ufsrestore in the headless mode we should use the following syntax. ufsrestore \\[options\\] \\[space separated arguments\\] what_to_restore In this schema we have: The dump_file argument is path to the file we want to restore a file system content using it, for example /mnt/fat/dump_fileImportant. The what_to_restore argument is a path pointing to a directory or a file inside the dump file. The important options available to the ufsrestore command are included in the following table. Option Description i Enter the interactive shell of the ufsrestore command f Path to the dump file. The option need the file name to be passed in the argument list x Extract the named files into the current directory. For example if the named file is directory, the ufsrestore will restore that directory and its contents into the current working directory. r Extract the dump content recursively into the current working directory. Current working directory is where we are executing the command from. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other alternatives to ufsdump and ufsrestore There are some file system independent commands which we can use to create backup of a file system and then restore it. Some of these commands are as follow. The ufsdump command has its advantages like creating incremental backup, support for reels and so on over these commands. The tar command is a no compression achiever which can archive a file system. The cpio command similar to tar but with capabilities like sending the archive stream to a remove machine and support for different archive formats. The pax command is a combination of tar and cpio capabilities. The dd command is a low level coping command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Creating snapshot of a UFS We can use fssnap to create snapshot of a UFS. The created snapshot is read-only and won’t sustain a restart. We usually use these snapshots for creating backup of the mounted file systems. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:20:2","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. The ufsdump performs the dumping task by scanning the requested file system path and creating a list of its content and writing the list into the specified media. Then it goes through the list and read each file addressed in the list and store its content in the specified media. Similarly the ufsrestore read the list, creates the directory layout and write the file contents into the media specified as restore target. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:21:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In addition to man pages we can learn more the ufsdump and ufsrestore commands in their reference documents available at http://docs.sun.com/app/docs/doc/817-3814/6mjcp0r5h?a=view 6. Checking UFS for errors UFS like any other file system can get corrupted in the block level. We can find the UFS corruptions and to some extend recover from those corruptions or prevent them from happening. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:22:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will mostly focus on using fsck command to check a disk and repair the damage if possible. I assume that we have some UFS partition like /dev/rdsk/c9t0d0s2 in our disposal to study on. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:23:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. The fsck command works on raw devices, we discussed raw and block devices in the Formatting and Labelling recipe, so we should use the raw name of a device to check for errors. As you remember from the Mounting and unmounting recipe of this chapter, OpenSolaris check the file systems for error on start-up if configured. That checking option kicks tart the fsck command when necessary. We should not run this command periodically nor should we execute in on a mounted file system. More importantly, we should not use this command on extra large disks as it may take days to complete. Following command checks the /dev/rdsk/c9t0d0s2 and interactively asks for our permission to perform sensitive operations. \\# fsck /dev/rdsk/c9t0d0s2 As you can see in the above command we passed the raw device instead of the block device name to this command. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:24:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. UFS uses several mechanism to prevent and recover from corruptions, such as logging which we discussed in Mounting and unmounting recipe. The fsck uses the output of these mechanism to check and if required try to fix the file system. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:25:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. We can try the man page and view the short help message to find more about this utility using the following commands. \\# man fsck \\# fsck \\--help 7. Specifying disk quotas on UFSs Like any other advanced file system supports space quota meaning that we can specify a space usage limitation for users on each file system. The disk usage limitation can be enforced for a single user or a group of users over any UFS like /mnt/sol. We can enforce the limitation over number of consumed blocks and Inodes. Each block, depending on its size, can contain 4 KB, 16 KB or any value specified during the file system creation. So, limiting the number of blocks limits the amount of data which can be stored in the file system. Each Inode contains a reference to a file or a directory in the file system. Limiting number of Inodes limits the number of files and directories which can be created. Before we get down to the business, we should understand basics that the disk quota operates on. Following three terms explains these basics. Soft limit: maximum amount of disk space that a user or a group of users can use. The disk usage by the user or the group can be exceeded for a certain amount of time. This amount of time is called Grace Period. Grace Period: A period of time which the soft limit may stay exceeded by a user or a group of users. The grace period can be specified in seconds, minutes, hours, days, weeks, or months. This period of time let the user to get below the soft limit. Hard limit: Specifies a hard limit for the user or group disk usage. This limitation cannot be exceeded in the Grace Period. For each quota we specify one line of information is recorded in the quotas file with the following structure. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:26:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Each file system needs to have a file named quotas owned by root in the top level. We use edauota command to edit this file and include the quota for users and groups of of the system. By default OpenSolaris will use vi to open the quotas file for editing but vi may not be a convenient editor for new users. We can change the default editor of the system by changing the value of EDITOR environment variable by using the following command: \\# export EDITOR=gedit This command change the default editor to gedit which is the default OpenSolaris desktop editor. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:27:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it.. Assuming that we want to define quota over /mnt/sol for a a user named masoud we need the following steps: Create the quotas file in the root of /mnt/sol \\# touch /mnt/sol/quotas Changing the ownership of the file to root \\# chmod 0600 /mnt/sol/quotas Specifying the quota using edquota command using the following command. \\# edquota masoud When we execute the above command, gedit window opens with content similar to the following figure. To change the quota values, we just need to change the numbers and press CTRL+S to save the file and exit gedit. After specifying the quotas we should turn quotas on if we have not turned it on using the mount table. Following commands turn quotas on for the given file system. \\# quotaon /mnt/sol To turn off the quota for a file system we can use the following command. \\# quotaoff /mnt/sol Now we can get report on quotas using the quota and repquota commands. For example to check quota for masoud on current file system we can use the following command: \\# quota -v masoud Output for this command can be similar to the following figure. To enable the quotas for a file system permanently, persistent across reboot, we can use the mount table options. For example we can add the following line to mount table by editor /etv/vfstab to enable quota for the /mnt/sol /dev/dsk/c9t0d0s2 /dev/rdsk/c9t0d0s2 /mnt/sol ufs 5 yes rq We used the last column which specifies mount options to enable quota for this file system. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:28:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works.. When we enable the quotas for a file system, Operating system consult the quota definitions before storing anything in the file system to see whether the user has the right to write some data into the disk according to his disk usage or not. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:29:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There's more.. Varieties of commands are available to get more out of quotas. We discuss some of these commends here. ","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:30:0","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other quotas reporting commands We can use repquota to view the quota details for a file system. For example to get detailed report on /mnt/sol file system quotas we can use the following command. \\# repquota /mnt/sol Or to view quota information for all users for all file systems we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e repquota -av\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e ### Specifying quotas for group of users or multiple users ### Specifying the grace period We can use edquota to specify the grace period for each user. To open the quota editor for grace period we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e edquota -t masoud\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e In the editor we just need to specify two numbers for the grace period of blocks and inodes. ## See also Looking at the quota, quotaon, repquota, quotacheck, quotaoff man pages can increase our understanding of quotas in general and these commands in particular. We can also check UFS management documentation located at http://docs.sun.com/app/docs/doc/817-0403/sysresquotas-97946?a=view # 8. Getting list of ZFS pools In the introduction section of this chapter we said that all ZFS file systems are build on top of storage pools. So, to work with ZFS, the first thing to lean is storage pools. In this recipe we will learn how to get information about available storage pools using zpool command. ## Getting ready In this recipe we will work on viewing the available zpools, you may find that the result of my commands are different than what you will see in your console, this difference can be caused by the fact that I have more than one pool created while you may only have the rpool which is created when installing OpenSolaris. ## How to do it.. Following command shows a list of available pools in the system along with their basic properties\\' values. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e The result for the executing the command can be similar to the following figure. ![](post-img/3180_02_14.png) The list subcommand can show one or more specific pool\\'s properties when we pass them as its arguments. For example to see information about rpool we can use the following command. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e \u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\\#\u003c/span\u003e zpool list rpool\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e \u003c/div\u003e \u003c/div\u003e Each pool has some more specific attributes which we can view or edit their values using zpool list command. For example the following command will show values for the given listed properties of rpool. \u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e \u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e \u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1 \u003c/span\u003e\u003c/code\u003e\u003c","date":"2010-08-09","objectID":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/:30:1","tags":["Solaris","ZFS","File System"],"title":"OpenSolaris File System Basics: Managing UFS and ZFS pools","uri":"/2010/08/opensolaris-file-system-basics-managing-ufs-and-zfs-pools/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"This is the 4th installment of my entries covering NIO.2. In this entry I will discuss more about what we can do with attributes and permissions. The NIO.2 lets us write the permissions and attributes of a file in addition to reading them. For example we can change the rwx permissions using the Attributes utility class and PosixFilePermission. You can execute the following sample code with almost no changes, the only thing you need to change is the path to our sample file and then you are ready to see how it works. import java.io.IOException; import java.nio.file.\\*; import java.nio.file.attribute.\\*; import java.util.\\*; public class Perm2 { public static void main(String args\\[\\]) throws IOException { FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We should have enough permisison to change the owner otherwise it will // throw a FileSystemException: /home/masoud/a: Operation not permitted sort of thing Attributes.setOwner(p, up); //another way to read and write the owner value for a file is using FileOwnerAttributeView FileOwnerAttributeView fo = p.getFileAttributeView(FileOwnerAttributeView.class, LinkOption.NOFOLLOW_LINKS); fo.setOwner(up); //Now that we have changed the permissions lets see the permissions again: att = (Map) p.readAttributes(\"posix:permissions\", LinkOption.NOFOLLOW_LINKS); System.out.println(\"New Permissions:\" + \": \" + att.get(\"permissions\")); } } Let’s see what we have done starting from top: at the beginning we have just initialized the FileSystem object and got a file reference to /home/masoud/dl. Then we read all of this files basic and posix attributes using the attributes view notation. The following sample code shows this step FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/dl\"); // Reading attributes using attributes views. // here we read both basic and posix attributes of a file Map att = (Map) p.readAttributes(\"posix:\\*, basic:\\*\", LinkOption.NOFOLLOW_LINKS); // printing the attributes to output Set en = att.keySet(); for (Iterator it = en.iterator(); it.hasNext();) { String string = it.next(); System.out.println(string + \": \" + att.get(string)); } System.out.println(\"-------------Attributes printing finished----------\"); Next step we have use the PosixPermissions class to create a permission set using the OS permissions presentation and then applied these permissions on our file // definind a new permission set for our file Set st2 = PosixFilePermissions.fromString(\"rwxrwxrwx\"); // Setting the attribute using Attributes utility class Attributes.setPosixFilePermissions(p, st2); In the next step we created a user principal for a user named masoud and changed the ownership of our file to this user. We have done this using both available methods. // looking up and creating a principal for the given user. If use does // not exists it will throws a UserPrincipalNotFoundException UserPrincipal up = fs.getUserPrincipalLookupService().lookupPrincipalByName(\"masoud\"); // Setting the owner to the owner we just looked up. // We s","date":"2010-07-13","objectID":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 4: Changing File System Attributes and Permissions","uri":"/2010/07/introducing-nio-2-jsr-203-part-4-changing-file-system-attributes-and-permissions/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction The Solaris Zones provide us with kernel level virtualization allowing us to create multiple virtualized environments on top of the host system kernel. Each one of these virtualized environment is isolated from host and other virtualized environments despite running on the same Kernel which is provided by the host or global zone. A typical example of using zones can be hosting different applications like the HTTP server, different application server instances for different applications, and the database server in separate zones. This separation and therefore isolation of different applications into different zones bring the following benefits. Possibility to control, cap, apply restrictions and share the resources like CPU, memory, disks, network interfaces, etc. Ease the monitoring and management of the all related applications by hosting them in one single machine. Proving each application administrator with enough space to configure its system security and apply the performance tweaking while controlling all of them from the global zone. Ensure that any performance or security issue with each one of the applications will not affect the other infrastructure members. Another use case for zones can be allocating resources of a powerful machine between different use cases or departments; each department has the freedom to install applications it requires while the underlying operating system will be managed by one single administration. 1. Creating a Zone In this recipe we will create a none-global zone and familiarize ourselves with different zone administration utilities including zonecfg and zoneadm. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:0:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use CLI utilities so we need to either switch to one f the virtual terminals or open a gnome-terminal. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:1:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can use the zonecfg command in interactive or shell mode to create a zone following snippet shows how we can create a zone in the interactive mode. The answers that we need to provide to the zonecfg command are highlight and we will discuss them afterward. After we enter the zonecfg shell it detects that the zone we want to configure does not exists and offers us to create the zone by invoking the create command. By invoking the create command we add the zone creation request to the batch of tasks we are stacking in the zonecfg shell. After invoking the create command we should specify the zone file system root path. In our case we entered /fpool/zones/fzone. The next thing that we should do in addition to specifying the zone path is adding a network interface to the zone because the zone creation and installation will need to access package repositories in order to install some packages into the zone. To add the network interface we should specify an IP address or a host name for the zone in addition to the physical network interface we want to create the zone virtual interface on top of it. Invoking end exit the current sub shell and invoking exit commits all changes we requested and exit the shell. Now we can check existence of our zone configuration using zoneadm command as shown in the following figure. In the figure 2 we can see a column named Brand, this column shows what is the zone type as we can have different types of zones including native, ipkg, cluster, solaris8, solaris9, etc. The Native brand uses the old packaging system, the ipkg uses the new IPS, and the cluster is the brand name for the Cluster zones. The solaris9 and solaris8 zones are provided to support running software compatible with Solaris 9 or Solaris 8 in Solaris 11. Now we have a zone configuration stored as an XML file in the zone management repository, what we need to do next is installing the zone which is actually realizing the configuration into a bootable, isolated Solaris installation on top of the running operating system. We can install the zone using the zoneadm command as shown below. The zone installation process requires Internet access or access to a local network package repository. In our case we will use the main repository available on Sun network. Following figure shows how we invoke the zoneadm to install the fzone. The system will download the necessary packages and install them to provide us with the minimum bootable zone. Now that the zone is installed we can boot into the zone using the boot subcommand of the zoneadm command as shown below. # zoneadm –z fzone boot Now that the zone is booted we can login to zone and configure the runtime parameters like default terminal, hostname for the zone, etc. Following command initialize the login sequence. # zlogin -C fzone During the boot sequence we should specify the following configuration attributes: The terminal type: The first step will be specifying the terminal, we can select from 6 different terminals including xterm which is my favourite. After specifying the terminal, system generates the SSH key pair and continues to the next step. The host name for the zone: After the key pair we should specify the hostname for the zone, by default the zone name will be the host name but we can change it to whatever we want in this step. To continue to the next step we should press Escape+2 buttons. Security service: After the host name we can select either we want to use Kerberos authentication or not, select no and press Escape+2 buttons. Name Service: Here we should specify the Name Service we want to use, select none and continue to the next step. The NFSv4 Domain Name: Here we should select the network file system domain name, let the system use the host derived domain name. The Time zone: In this step we specify which time zone we want our zone to use by specifying the continent and then the country. The Zone password: Here in the final step we should specify a ro","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:2:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we install Solaris we have one zone created by default which is called global zone. The global zone represents the entire operating system and can host as many as 8191 non-global zones. Each none global zone is a virtualized environment with its own host name and a virtualized network interface. Each zone has its own security boundaries and its storage and processing resources are assigned to it from the global zone. As each none-global zone acts as a complete an isolated instance of Solaris, it needs utilities, libraries and applications accessible to it. The model in which Solaris provides the zone with the libraries and utilities differentiates the zones into two categories as follow: The sparse-root zone: In this model the zone has read-only access to /lib, /sbin, /usr, /platform directories of the global zone and therefore any changes in those directories will be reflected in the zone. On the other hand we cannot install applications which need write access to these directories into a sparse-root zone. The benefit of this zone is ease of administration and small footprint while its drawback is limited capability in installing new packages into the zone. The whole zone: In this mode we have full control over the file system layout of the pool and the packages and application we want to install but this will cost us a little more space. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:3:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Make sure that you review other recipes of this chapter as this recipe was just the door to the Solais Zones world. In next recipe we will see how we can control resources allocation for a Zone or how we can create a Linux zone instead of a Native Solaris zone. Zone Networking In this recipe we are going to see what are networking options in a zone and how we can use these options to further configure a zone to fit our requirements. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:4:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we assume that we have a zone installed according to the previous recipe and then we will tweak its networking configuration. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:5:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Assuming that we have a zone created using the previous recipe we can use the zoneadm command to see the zone configuration as shown in the following figure. As you can see under the IP column, we have shared as the IP type of the fzone. Basically this means that one IP stack and physical network interface is shared between the global and the non-global zone using a virtual network interface built on top of the physical interface we specified during the zone creation. When sharing an IP stack between a global and a non-global zone, the global zone takes care of IP routing and if we share the physical interface between multiple zones, packets won’t leave the system and get routed in the global zone kernel. Following figure shows how the logical interface is created on top the e1000g1 for the fzone to have network access. The logical interface won’t appear in the output of the ifconfig command unless we boot the zone. The command to boot a zone is as follow: # zoneadm –z fzone boot When we use logical interfaces to share a physical interface between different zones, the interface configuration is not changeable from within the non-global zones which increase the overall security of system. We can add use virtual interface, VNIC, physical network interfaces for a zone, to do so we only need to reconfigure a zone’s networking or specify the physical or virtual interface during the zone creation. We can configure a non-global zone with exclusive network interface using the following steps. As you can see we removed the shared interface from the zone and give it the exclusive access to the e1000g0. To make the changes effective we need to restart the zone. To do so we can halt the zone using halt command inside the zone shell or we can use the following command from the global zone. # zoneadm -z fzone halt When we allocate an exclusive physical network interface to a zone we are asking the system to create a completely isolated IP stack for the zone and isolate all the traffic. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:6:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we use logical interfaces to share a physical interface between different zones, the interface configuration is not changeable from within the non-global zones which increase the overall security of system. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:7:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can create a virtual interface using the dladm command and then assign that interface to the zone or we can create an aggregation and exclusively assign the aggregation to a zone. 3. Managing a zone resources In this recipe we will learn how to allocate and control the resource that a zone can utilize. For example how much CPU cycles and how much memory a zone can use. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:8:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we get down seeing how we can apply the resource control we need to have some basic knowledge of two different CPU cycles allocator available in Solaris. The Fair Share Scheduler, FSS: The FSS ensure that all parties, zones, at least get what we allocate for them. For example if we have 4 zones running and all of them are very busy the FSS ensure that each zone receives its fair share of CPU power as allocated by the administrator. Meanwhile if we have one very busy zone and 2 idle zones, the FSS allows the busy zone to get more than its share of CPU cycle until another zone requires receiving its allocated share. The Time Sharing scheduler: This schedule is the default scheduler in Solaris and acts based on the time slices independent of whether the zone requires the allocated time slice or not. For example if we have three zones, one very busy and two idle zones, the TS will gives each of these zones the time slice specified regardless of it not doing anything with the CPU power and one zone starving for the power. Capping: The capping resource controlling feature allows us to place a cap on the amount of resource the zone can use regardless of more resource being available at the time. The default CPU sharing mechanism is TS and we if we want to use the FSS we should enable it. To replace the TS with FSS we can use the following command # dispadmin -d FSS After the above command we need to restart the machine but in order to make TSS take effect immediately we can use the priocntl CLI utility as follow: # priocntl -s -c FSS -i all Before getting down to the business of controlling the zone resources we need to install a required package, which is a Solaris service, for resource controlling. The package name is SUNWrcap and to install it we can use the pkg command as shown below. # pkg install SUNWrcap Now we can get down to the actual work and add some CPU and memory control on our zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:9:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To configure and apply resource control for a zone we use the zonecfg command and change the value of different zone attributes. Following table shows the resource that we can control and specify its value for a zone along with a basic description of attribute. Resource Type/ Corresponding Zone property Description CPU Shares/ cpu-shares Specifies how much of the system CPU power we want the zone to use. Maximum number of processes/ max-lwps Specifies how many process a zone can spawn. Maximum number of semaphores/ max-sem-ids Specifies how many semaphore lock a zone can acquire Maximum size of a shared memory segment /max-shm-ids Specifies what is the maximum size of a memory segment for a zone Total amount of shared memory/max-shm-memory Specifies how much RAM/ SWAP a zone can allocate. Maximum number of message queues/max-msg-ids Specifies how many message queue a zone can create for asynchronous processing. Let’s assume that we specified a cpu-share value for our zone and we are using FSS as the scheduler. Assuming that we have two zones, one the global zone and one the local zone named fzone, we allocate 3 shares of CPU for the fzone, it means that the whole CPU power of the machine will be divided into 4 shares which 3 goes for the none-global zone and one share goes for the global zone. If at a later time we add another zone to the system with 2 share then we the FSS will divide the CPU power into 3+1+2 shares. From this 16% goes to the global zone, 48% goes to the fzone and 32% goes to the zone arrived at last. All of the above numbers are flexible when using FSS, for example if all other zones are idle then all available CPU power goes to fzone and if other zones are busy each one get the amount of CPU we specified. Now let’s get down to the work and apply the following control over the resources that fzone can utilize. Now to check the resource control restrictions we have just applied we can use the the zonecfg command as shown in the following figure. In the above figure, the resource names are cited in the full name instead of the simplified names and syntax we used to set the attributes value. In the beginning of the recipe we discussed the CPU and memory capping as a mechanism to prevent a zone from using more a certain cap even if the resource is available in the system, like when we use FSS and CPU power is available for the zone to use. To apply capping we should use resource scopes in the zonecfg shell. For example to cap the fzone memory usage we can use the fzone as shown in the following figure. For the fzone we are setting the maximum amount of memory to 1 GB, the maximum amount of SWAP to 2GB and the maximum amount of lockable memory to 16 MB. The SWAP space we specified is the amount of swap that the zone’s user applications can use. Next comes CPU capping in which we can specify a maximum amount of CPU which a zone can use despite the system being configured with FSS which means a zone can use al CPU cycles which are not in use by other process. In our configuration we specified that the fzone cannot use more than 1.5 CPU even if more CPUs are free. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:10:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… To configure zone resources we have two ways, the first is using resource pools and the second way is to directly allocate resource to a zone. In the second method the Solaris kernel dynamically creates a pool, allocate the resources we assigned to the zone and use that pool as the zone resource pool. The pool will be created when the zone starts and it will be destroyed when the zone shouts down. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:11:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can use dedicated-cpu scope to restrict the zone on the number of CPUs it can use and dedicate the CPUs to that particular zone when zone boots up. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:12:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To review read more about resource management check recipe XX and recipe XY in chapter ZZ. We can limit the bandwidth that a zone can use which is discussed in recipe XX of chapter ZZ. 4. Cloning a Zone In this recipe we will look at how we can clone a zone which basically allows us to create a new zone based on an already existing zone to save time in file system management and configuration, software installation and so on. Usually we clone a zone when we want to have several identical zones in term of file system layout and installed software. A simple case can be a zone with all required software fully configured or periodical clones of a test zone to ensure that we can quickly initialize a new test zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:13:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we assume that we have the fzone created according to the first recipe of this chapter. We will clone the zone configuration into a new zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:14:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… In our hands-on we will clone fzone into a zone named fzone-c. Before we could clone the fzone file system into the fzone-c we should create fzone-c, For sake of simplicity the only thing that we specify during the zone creation is the zone file system path. Following figure shows how we can create the zone. Now that we have created the zone and specified its file system root we can clone the template zone into the newly configured zone instead of installing the zone. To clone the zone we use the clone subcommand of the zoneadm command as shown below. # zoneadm –z fzone-c clone fzone We can check the status of our newly cloned zone using the list subcommand of the zoneadm command as shown in the following figure. The new zone will be cloned in few seconds as it is a snapshot of the template zone. Following figure shows the difference in size of the fzone and its clone fzone-c in term of initial disk space usage. Now we can boot the zone and login using the following commands which we discussed in details in the first recipe of this chapter. #zoneadm -z fzone-c boot #zlogin -C fzone-c When we want to clone a zone we should ensure that the zone is not running because zoneadm will not clone a running zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:15:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The whole zone cloning benefits from creating a ZFS snapshot of the template zone and then a ZFS clone based on that ZFS snapshot for the new Zone. Using this way, the new zone will have an identical file system layout with the template zone without using that much of a disk space and its creating will be immediate. The size of the clone will grow in time based on what we change in the new zone and in the template zone from which we created the snapshot. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:16:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… Instead of using ZFS cloning we can use simply copy to copy the template zone file system into the new zone file system. The zone copying takes longer and consumes more space compared to zone cloning. We can use the following syntax of zoneadm to copy a zone. #zoneadm -z fzone-c clone -m copy fzone In this syntax the fzone-c is the new zone, the fzone is the template zone and the -m copy specifies the cloning method which is plain copying compared to the default method which is ZFS cloning. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:17:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also It is recommended that you review recipe 1 of this chapter and recipe ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:18:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"of ZFS chapter. 5. Migrating a zone In this recipe we will look at how we can relocate a zone inside the same machine or migrate it from one machine to another. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:19:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready For this recipe we assume that we have a zone named fzone created according to the first recipe of this chapter. We will move the zone to another dataset and then migrate it from its current host machine to a secondary machine. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:20:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… First let’s look at how we can move the zone. In order to move a zone we should halt the zone either by invoking halt inside the zone or by using halt subcommand of the zoneadm in the global zone. To move a zone we can use the move subcommand of the zoneadm command as shown below. # zoneadm -z fzone move /fpool/zones/fzone-moved The following figure shows the effect of moving a zone because the zone file system root remains constant from inside the zone and the only changes are physically moving the file system and also altering the zone configurations in the zone configuration store. Now let’s see how we can migrate a zone from one machine to another. The steps for migrating a zone form one machine to another is as follow: Halt the zone Dump the zone configuration and detach the zone Create and move the zone snapshot to target machine Create a zone with same name in the target machine Configure the new zone’s zonepath attribute to point to the received snapshot Apply the zone manifest Now that we know the steps let’s see how we can put these steps into practice. Let’s start with detaching the zone and saving the zone manifest. #zoneadm -z fzone halt # zoneadm -z fzone detach \\\u003e /fpool/fzone -manifest.mft # zfs snapshot -r fpool/zones/fzone@opensolaris # zfs send fpool/zones/fzone@opensolaris \\| ssh new-host zfs recv /zones/fzone # zfs send /fpool/fzone -manifest.mft \\| ssh new-host zfs recv /zones/ fzone -manifest.mft Now that we have the manifest file and the zone file system snapshot in the new host we can use login to the new host and continue with creating the zone in the new host with the rest of steps. The first step in the new host is creating a non-global zone and using the received snapshot as its zone path. I assume that we created the zone in the new machine and continue with the rests of steps. To learn how we can create a new zone review recipe 1 of this chapter. # zoneadm -z myzone attach /zones/ fzone -manifest.mft Now the zone is migrated to its new host. We can boot the zone and login. We may need to change the hostname and the networking as we are now residing on a new host. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:21:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When migrating a zone, we are moving both its settings and its file system from one machine to the second one. The manifest file contains basic zone configurations like resource control, networking, package inheritance, and so on. Creating a new zone using the same zonepath and attaching the configuration gives us the same zone that we had in the previous host without the need to go through configurations again. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:22:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about ZFS you can review chapter 1 and chapter 2. These two chapters contain detailed instructions for each possible task with ZFS. 6. Controlling Zone network resource In this recipe we will use the features that project crossbow introduced into Solaris operating system to apply resource control on a zone network usage. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:23:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we continue with this recipe I assume that we have a zone named fzone which uses a dedicated virtual interface named vnic1 for its network access. We discussed how to create a zone and how to create the zone and how to configure it with the dedicated network interface review recipe 1 and recipe 2 of this chapter. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:24:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… In order to configure anything regarding the zone networking we should have access to the global zone and the network resource control is not an exception. We can use the dladm we discussed in chapter X recipe Y to apply resource allocation on network resources. Before getting down to applying the resource controls let’s see what is the default value for different resources allocation properties of the vnic1. To do so we can use the dladm command as shown in following figure. As we can see there is no limit on the CPU and bandwidth, the MTU is set to default 1500 and the priority to high. Now we will lower the priority to low, set a maximum bandwidth of 250 Mb per second, and delegate the packet processing for this interface to CPU number 3 using the following commands. # dladm set-linkprop -p cpus=1 vnic1 # dladm set-linkprop -p priority=low vnic1 # dladm set-linkprop -p maxbw=250 vnic1 We can use the dladm command to check the interface property changes as shown in the following figure. Different types of links like physical, aggregation, virtual can have less or more properties but the resource control properties are shared between all of them. We can also impose restrictions on the bandwidth and flow of data on different port and different protocols using the flowadm command and the flow control capability of Solaris. To impose limitation on different protocols and the traffic of different ports we should create a flow control and then add the restrictions to the flow control. For example let’s see how we can limit the traffic on port 21 to 10Mb. First we should create the flow control as shown below # flowadm add-flow -l vnic1 -a transport=tcp,local_port=21 ftp-flow Now we can add the bandwidth restriction to the flow control we have just created using the following command # flowadm set-flowprop -p priority=low,maxbw=10m ftp-flow Now that we applied the flow control let’s get the list of flow control to double check the restrictions we imposed. Following figure shows the restriction we just applied on the vnic1 flow. In addition to setting the maximum bandwidth we changed the priority of FTP port packet processing to low as it is not a critical protocol after all. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:25:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we change the resource allocation property of a network interface independent of which zone, global or none-global, it belongs to; the Solaris kernel place the caps on the interface resources as soon as the interface is brought online. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:26:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about applying resource consumption restriction on Solaris zones review recipe 3 of this chapter. To see a complete list of all flow controlling properties consult the documentation available at http://docs.sun.com/app/docs/doc/819-2240/flowadm-1m?l=en\u0026n=1\u0026a=view 7. Creating Linux zone In this recipe we will see how we can create a Linux zone on top of the Solaris kernel. In the first recipe of this chapter we discussed the zone brands which can be lx, pkg, native, solaris8 and so on; the lx brand is the Linux zone brand which we are going to create in this recipe. The lx brand, based on the BrandZ framework, provides the runtime environment equal to CentOS 3.x or any compatible distribution like Redhat Enterprise Linux 3.x as a Solaris zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:27:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we start creating and installing a lx zone we need to have the CentOS VERSIONN tarball or iso image available in the global zone for installation phase. A tarball of CentOS 3.VERSION is available at http://hub.opensolaris.org/bin/view/Community+Group+brandz/downloads The second perquisite is the lx branded zone template which facilitate the zone creation. The template is available in the SUNWlx package. We can check whether the package is installed or not using the following command # pkg info SUNWlx \\|grep State And if it is not already installed we can install it using the following command. # pkg install SUNWlx ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:28:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Now that we have all the required pieces prepared we go get down to the zone creation and installation. The only difference in the creation phase is the fact that we will use the SUNWlx template to create the zone, all other details like network configuration, resource allocation and so on remains the same. Following figure shows how we can create the lz-zone based on the SUNWlx template. I omitted adding networking and resource control during zone creation as we already discussed resource control in the 3rd recipe of this chapter. Now that we created the zone it is time to install he zone using the zoneadm command and the tar archive we grabbed form the OpenSolaris website. After installing the zone, it is time to review the list of zones and see status of our newly created zone using the list subcommand of the zoneadm command as shown in the following figure. The brand name of the lz-zone is lx which reminds us that this is a Linux branded zone. Now let’s boot the zone and login into the zone console. Following figure shows how to use boot subcommand of the zoneadm and zloging command to boot and login into the zone. Finally we can use the uname CLI utility inside the zone to see the zone version. The Linux zone’s networking is disabled by default; we can enable it by adding the required hostname and networking property in the /etc/sysconfig/network using any editor or a simple echo command which is shown below. Note that these commands should be executed inside the lz-zone shell. # echo \"NETWORKING=yes\"\\\u003e /etc/sysconfig/network # echo \"HOSTNAME=lx-zone\"\\\u003e /etc/sysconfig/network ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:29:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When the runtime environment of the none-global zone is not the same as the global zone a translation is required for all system calls from the non-native environment to the native environment in order for the guest to work on top of Solaris host. The lx branded zones require all of the calls from the guest zone which is a Linux runtime with its own kernel to be translated to Solaris API and then executed on the hardware and the result will go back in the same translation phase to carry the final execution result to the Linux environment. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:30:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about creating a zone and applying zone resource control reviewing the recipe 1, 2, and 3 of this chapter is highly recommended. 8. Removing a Zone In this recipe we cover how we can remove a none-global zone from the system safely. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:31:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We assume that we have a zone named fzone and we want to remove the zone. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:32:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… In order to remove a zone we should follow the steps listed below, Ensuring that no one is logged-in to the zone because we need to halt the zone before deleting it and halting a non-global zone from the global zone does not notify the users who are logged into the zone about the upcoming halt. To halt the zone we can use the following command from the global zone. # zoneadm -z fzone halt Uninstalling the zone: After halting the zone we should uninstall it using the following command # zoneadm -z fzone uninstall Deleting the zone: To delete the zone we will use the same command that we used to create the zone, the zonecfg command. #zonecfg -z fzone delete The delete and the uninstall sub-commands interactively ask for the confirmation of the operation. We can use the -F after each sub-command to force the deletion without further confirmation. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:33:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we use the uninstall sub-command of the zoneadm command we are basically removing the zone file system from hard drive and when we call the delete subcommand of zonecfg command we are removing the zone manifest and configuration from the system configuration store. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:34:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about the zones structure and layout refer to first recipe of this chapter. ","date":"2010-07-12","objectID":"/2010/07/managing-and-administrating-opensolaris-zones/:35:0","tags":["Solaris","Zone","Container","Services","OpenSolaris"],"title":"Managing and Administrating OpenSolaris Zones","uri":"/2010/07/managing-and-administrating-opensolaris-zones/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction In this chapter we are going to cover basic networking capabilities of OpenSolaris. While we will some of common utilities in the recipes, we will learn some more trivial ones here. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:0:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning netstat command The netstat command is well known for checking the active connections status in a system but it can provide a fair deal of other diagnostics. Following sample command shows some of the netstat use cases. List all ports: \\# netstat -a \\| more Inspecting an interface: \\# netstat -I e1000g0 Viewing the routing table: \\# netstat -rn ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:1:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning ping command We can use ping, which uses Internet Control Message Protocol (ICMP) packets, to check whether a target address is accessible or not. For example to check whether 10.0.2.24 is accessible for us, we can use \\# ping 10.0.2.24 When we check whether we have internet connection or not, it is better to ping an IP address instead of a domain name because pinging a domain require a Domain Name System (DNS) resolution which may not be possible when we have DNS failure. For example, we can use following command to check whether we have internet access or not. \\# ping 4.2.2.4 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:2:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Learning traceroute command The traceroute command allows us to see which hops a packet goes through before it reaches its destination. By examining the hops we can determine which one of our network components causes delay or even drops the packet or has no route to destination address. For example: \\# traceroute -n 4.2.2.4 \\# traceroute -n [www.google.com](http://www.google.com/) We usually use the -n parameter to prevent DNS resolution of intermediary hops to accelerate the overall process or because we do not have a DNS server configured for our network or the specific connection we are examining. 1. Listing network Interfaces In this recipe we will see how we can get a complete list of all plumbed network interfaces using the ifconfig command. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:3:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready As we are using a command line interface (CLI) utility to get the list of available interfaces we should execute it in the console environment. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:4:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Following command shows how we can get list of all available interfaces. \\# ifconfig -a Following figure shows the output for the above command. As we can see in the figure, the command lists all interfaces including the loopback interface and Ethernet ones. In addition the sample command we used lists both IPv4 and IPv6 enabled interfaces and separately show their addresses. OpenSolaris follows a default naming schema for network interfaces it detects in the system. Basically all Ethernet interfaces’ names starts with e100 or e1000 as the prefix and then g to gN as the distinguisher which distinguish each instance of the installed Ethernet card with another using an integer number. All wireless card names are prefixed with wpi followed by a number indicating the card instance number. Loopback interfaces are named with lo and then followed by a distinguisher number. There are more schemas for other cards like pcnN, bgeN, switch which we will discuss in next recipes. Variety of details might be shown in the flags area depending on the network interface driver. Some of most important flags are as follow: The UP flag indicates that interface sends and receives packets. The NOTRAILERS flag indicates that no trailer included at the end of the Ethernet frame. The RUNNING flag indicates that interface is recognized by the kernel. The MULTICAST flag indicates that the interface supports a multicast address. We can change an interface name to a more meaningful and human-readable name. Detailed instructions about renaming an interface is available at http://www.opensolaris.com/use/Migrating.pdf The ifconfig command can list the interfaces which are already plumbed (available for the kernel to use). If an interface is unplumbed, the command will not show it. We can use dladm command to list all plumbed and unplumbed interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:5:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The command reads the network configurations from network configuration files located at /etc/sysconfig/network-scripts/ and /etc/hostname.interface_name files or directly from memory when we do not have any of our interfaces configuration persisted. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:6:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… The ifconfig command is one of the most useful and handy commands when we deal with network interfaces and network configuration. A complete set of its parameters and options is accessible in the man pages or using the short help format which is as follow \\# ifconfig --help ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting information for an specific network interface If we know our interface name and we want to see its detailed information we can pass the interface name to ifconfig command. For example to see detailed information for e1000g0 we can use the following command. \\# ifconfig e1000g0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:1","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Other commands for listing network interfaces We can use some other commands like dladm to list the available network interfaces. dladm can give more details about an interface including speed, state and so on. For example the following command shows the dladm command usage. The dladm shows both plumbed and unplum-ed interfaces in contrast with ifconfig which only shows plumbed interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:7:2","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 2 and recipe three of this chapter discuss how we can configure the network interface in OpenSolaris. Reading them is recommended to grasp better understanding of OpenSolaris networking. 2. Configuring a network interface to obtain IP from a DHCP Server In chapter 1 recipe we discussed how we can use Network Auto-Magic to configure automatically configure the network interfaces. Now in this recipe we will discuss how we can configure the interface manually without using the NWAM. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:8:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To configure a network interface manually we should disable the NWAM using the following commands. \\# svcadm disable network/physical:nwam Do not worry if you cannot understand meaning of the two commands. Simply accept that the svcadm disable the nwam service. In chapter recipe we will discuss svcadm in more details. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:9:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The ifconfig command we discussed in the previous recipe is our tool for configuring the network interfaces the way we want. Following command enable the Dynamic Host Configuration Protocol (DHCP) client layer for the e1000g0 network card. \\# ifconfig e1000g0 dhcp start Following figure shows how the new IP address is applied on e1000g0 interface. The command assumes that the interfaces are already plumbed, known to kernel. If not we should plumb the interface using the following command: \\# ifconfig e1000g0 plumb When we configure an interface manually we the configuration will not survive the reboot unless we make the configuration persistence. To make the configuration for e1000g0 persistent should create two files in the file system as follow: To ask the OS to configure the interface we should create /etc/hostname.e1000g0. The extension specify the interface we want the network/physical:default to configure the interface on boot-up. To ask network/physical:default service to use DHCP to assign an IP address to this interface we should create /etc/dhcp.e1000g0. We can create these files using the following command or using normal text editors like gedit. \\# touch /etc/hostname.e1000g0 /etc/dhcp.e1000g0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:10:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we stop the network/physical:nwam service, the OS check whether any configuration files for each one of the interfaces exists or not, if it exists OS will configure the interface accordingly and otherwise the interface will be left un-configured. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:11:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… Sometimes we want to get a new IP address for our interface or we just do not need the assigned IP anymore and we want to release it to the pool. Following command release the IP assigned to our interface back to the pool \\# ifconfig e1000g0 dhcp release The command formally send an information packet to DHCP server telling it that the IP address is no longer required and then stop the DHCP client. Another way which is not recommended is dropping the IP addressee without letting the server know that we are not using the address anymore. The DHCP server will eventually take the address back into the pool but it will take some times based on the DHCP server configuration \\# ifconfig e1000g0 dhcp drop ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:12:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next recipe we will see how we can configure an interface with a static IP instead of using DHCP. I recommend you review it to get more information about how network interfaces work in OpenSolaris. 3. Configuring network interfaces with static IP address Sometimes there is no DHCP server around for us to acquire an IP address from its pool or we just want to use a static IP address for our interface. Some use cases for static IP address can be the router, network gateway or our database server. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:13:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To configure a network interface manually we should disable the NWAM using the following commands. \\# svcadm disable network/physical:nwam Do not worry if you cannot understand meaning of the two commands. Simply accept that the svcadm disable the nwam service. In chapter recipe we will discuss svcadm in more details. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:14:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The ifconfig command we discussed in recipe 1 and recipe 2 of this chapter provides us with required options to assign a static IP address to a network interface. Following command assign 192.168.1.110 to e1000g0 with 255.255.255.0 as its subnet mask. \\# ifconfig e1000g0 inet 192.168.1.110/24 Before we could use the interface we should bring it up, to bring up the interface we can use the following command: \\# ifconfig e1000g0 inet up The IP_Address/Bit_count syntax specifies the IP address we want to assign to the interface and the number of bits associated with the subnet mask. In the subnet mask, 24 means that first 24 bits of the subnet mask should be 1 and the remaining 8 bits should be 1. Following figure shows how new IP address is assigned to e1000g0 interface. This configuration cannot survive a system reboot unless we persist it using the network interface declaration files. To ask the network/physical:default service to configure an interface using an static IP address we should create /etc/host_name.interface_name file and place the configuration information inside it. For example to configure the e1000g0 interface with the same configuration we used above we should create /etc/hostname.e1000g0 file and place the following line inside it. 192.168.1.110/24 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:15:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When OpenSolaris is booting, the network/physical:default check for any available file with a known interface as its extension it execute the ifconfig interface_name inet \u003ceach_line_of_the_file\u003e commands sequentially to configure the interface and finally it executes the ifconfig interface_name inet up to bring the interface online. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:16:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… To bring down an interface we can use the following command: \\# ifconfig interface_name inet down This command takes the interface offline. To remove the persisted configuration for an interface we should remove the content of its configuration file or completely remove the corresponding file from the system. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:17:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also First two recipes of this chapter discuss the network interfaces and using DHCP to configure an interface. We can read them to get more information about the network interfaces and configuring them. 6. Configuring FTP Server The FTP (File Transfer Protocol) is one of the widely used services to distribute files publicly. OpenSolaris comes with both FTP client, ftp command, and FTP server, ftpd daemon. In this recipe we setup an FTP server and use the ftp command to access the FTP server content. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:18:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use a console window or a virtual console to invoke necessary commands to enable the FTP server. So open the console window by executing gnome-terminal command in the Run Application (ALT+F2) window before getting down to business. We also need to have a ZFS pool available to us, in our sample commands I used fpool as the practicing pool while you can use any other pool you have previously created. Creating ZFS pools is discussed in recipe 9 of chapter 2. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:19:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… First we should create a file system for the FTP account and shared FTP content. We can do it using the following commands \\# zfs create fpool/ftp_fs \\# ftpconfig /fpool/ftp_fs So far, we shared a file system for our FTP users but we did not specify which users can access this shared content. The default configuration of OpenSolaris allows everyone to access the FTP server content using the anonymous access. Following command the FTP user details. \\# grep ftp /etc/passwd Now we can enable the FTP service using the following command. FTP server is managed by SMF as a service available at svc:/network/ftp. \\# svcadm enable ftp Following figure shows what we can expect to see when we execute each of the above commands. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:20:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Majority of setting up FTP server tasks is automated and the ftpconfig script is responsible for doing it. What happens when we invoke the command is summarized below: A user is created with ftp only access The script enriches the file system we specified with binary files required. For example the ls command is copied into the /fpool/ftp_fs/bin to ensure that FTP user can execute it to get the list of files. Later on it write down the configuration required for the FTP service so when we run the service, clients can access the /fpool/ftp_fs/pub trough the FTP server. Following figure shows content of the /fpool/ftp_fs/ which is created by ftpconfig script. As we can see in the figure, all directories are restricted for the root access except the pub directory which will contain the files we want to share over the network. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:21:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 10 of this chapter is recommended to see how we can setup Secure Shell (SSH) to ensure the data transfer confidentiality. 7. Configuring DHCP Server ==Done A Dynamic Host Configuration Protocol (DHCP) server leases IP address to clients connected to the network and has DHCP client enabled on their network interface. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:22:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can setup a start the DHCP server we need to install DHCP configuration packages. Detail information about installing packages in provided in recipe of chapter 1. But to save the time we can use the following command to install the packages. \\# pkg install SUNWdhcs After installing these packages we can continue with the next step. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:23:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… First thing to setup the DHCP server is creating the storage and initial settings for the DHCP server. Following command does the trick for us. \\# dhcpconfig -D -r SUNWfiles -p /fpool/dhcp_fs -a 10.0.2.254 -d domain.nme -h files -l 86400 In the above command we used several parameters and options, each one of these options are explained below. The -D specifies that we are setting up a new instance of the DHCP service. The -r SUNWfiles specifies the storage type. Here we are using plain-text storage while SUNWbinfiles and SUNWnisplus are available as well. The -p /fpool/dhcp_fs specifies the absolute path to where the configuration files should be stored. The -a 10.0.2.15 specifies the DNS server to use on the LAN. We can multiple comma separated addresses for DNS servers. The -d domain.nme specifies the network domain name. The -h files specifies where the host information should be stored. Other values are nisplus and dns. The -l 86400 specifies the lease time in seconds. Now that the initial configuration is created we should proceed to the next step and create a network. # dhcpconfig -N 10.0.2.0 -m 255.255.255.0 -t 10.0.2.1 Parameters we used in the above command are explained below. The -N 10.0.2.0 specifies the network address. The -m 255.255.255.0 specifies the network mask to use for the network The -t 10.0.2.1 specifies the default gateway All configurations that we created are stored in DHCP server configuration files. We can manage the configurations using the dhtadm command. For example to view all of the current DHCP server configuration assemblies we can use the following command. \\# dhtadm -P This command’s output is similar to the following figure. Each command we invoked previously is stored as a macro with a unique name in the DHCP configuration storage. Later on we will use these macros in subsequent commands. Now we need to create a network of addresses to lease. Following command adds the addresses we want to lease. \\# pntadm -C 10.0.2.0 If we need to reserve an address for a specific host or a specific interface in a host we should add the required configuration to the system to ensure that our host or interface receives the designated IP address. For example: \\# pntadm -A 10.0.2.22 -f MANUAL -i 01001BFC92BC10 -m 10.0.2.0 -y 10.0.2.0 In the above command we have: The -A 10.0.2.22 adds the IP address 10.0.2.22. The -f MANUAL sets the flag MANUAL in order to only assign this IP address to the MAC address specified. The -i 01001BFC92BC10 sets the MAC address for the host this entry assigned to it. The -m 10.0.2.0 specifies that this host is going to use the 192.168.0.0 macro. The –y asks the command to verify that the macro entered actually exists. The 10.0.2.0 Specifies the network the address is assigned to. Finally we should restart the DHCP server in order for all the changes to take effect. Following command restarts the corresponding service. \\# svcadm restart dhcp-server ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:24:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we setup the DHCP service, we store the related configuration in the storage of our choice. When we start the service, it reads the configuration from the storage and wait dormant until it receives a request for leasing an IP address. The service checks the configuration and if an IP was available for lease, it leases the IP to the client. Prior to leasing the IP, DHCP service checks all leasing conditions like leasing a specific IP address to a client to ensure that it leases the right address to a client. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:25:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can use the DHCP Manager GUI application to configure a DHCP server. The DHCP manager can migrate the DHCP storage from one format to another. To install the DHCP manager package we can use the following command. \\# pkg install SUNWdhcm Now we can invoke the DHCP manager using the following command which opens the DHCP Manager welcome page shown in the following figure. \\# dhcpmgr ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:26:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 5, 6 of this chapter recommended understanding IP assignment better. 8. Assigning multiple IP address to an interface Like other operating system we can assign multiple IP address to a network interface. This secondary address are called logical interfaces and we can use them to make one machine with one single network interface own multiple IP addresses for different purposes. We may need to assign multiple IP address to an interface to make it available to both internal and external networks or for testing purposes. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:27:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We should have one network interface configured in our system in order to create additional logical interfaces. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:28:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We are going to add a logical interface to e1000g1 interface with a 10.0.2.24 as its static IP address. Before doing so let’s see what network interface we have using the ifconfig command. Now to add the logical interface we only need to execute the following command: \\# ifconfig e1000g1 addif 10.0.2.24/24 up Invoking this command performs the following tasks: Create a logical interface named e1000g1:1. The naming schema for logical interfaces conforms with interface_name:logical_interface_number which the number element can be from 1 to 4096. Assign 10.0.2.24/24 as its IP address, net mask and broadcast address. Bring up the interface. Now if we invoke ifconfing -a command the output should contain the logical interface status as well. The following figure shows a fragment of ifconfig -a command. Operating system does not retain this configuration over a system reboot and to make the configuration persistent we need to make some changes in the interface configuration file. For example to make the configuration we applied in this recipe persistent the content of /etc/opensolaris.e1000g1 should something similar to the following snippet. 10.0.2.23/24 addif 10.0.2.24/24 The first line as we discussed in recipe 3 of this chapter assign the given address to this interface and the second like adds the logical interface with the given address to this interface. To remove a logical interface we can simply un-plumb it using the ifconfig command as shown below. \\# ifconfig e1000g1:1 unplumb ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:29:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a logical interface, OpenSolaris register that interface in the network and any packet received by the interface will be delivered to the same stack that handles the underlying physical interface. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:30:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2 and 3 of this chapter is recommended as they discuss how to configure a network interface with DHCP or how to manually assign a static IP address to a network interface. The next recipe can be another source for learning about Virtual Network interfaces capability in OpenSolaris which allows us to add network interfaces to the system using an available physical interface. 9. Configuring virtual LAN interfaces Here in this recipe we will see how we can create multiple link layer virtual interfaces on top a physical network interface. Each one of the VLANs is accessible from outside and we can configure them to be used in our services and applications. For example, we can configure our DHCP server to use a VLAN or we can run GlassFish server on it. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:31:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can proceed with this recipe we should at least have one network interface installed in our system. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:32:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can create Virtual Interfaces using dladm command which is the utility we will use anytime we need to manage some data link layer entities. Following command shows how we can create a Virtual LAN interface (VLAN). \\# dladm create-vlan -l e1000g0 -v 1 vlan0 This command creates a VLAN on top of e1000g0 interface with 1 associated as its distinguisher tag. We will discuss distinguisher tags in more details in how it works section. Following command lists all available interfaces either configured or not. The shown interfaces include both virtual and physical interfaces. \\# dladm show-link Now that the vlan0 is created we should plumb it so operating system detects it. After plumbing we can configure it like any other interface. We can assign its IP address manually or we can configure it to acquire an IP address from a DHCP server. \\# ifconfig vlan0 plumb \\# ifconfig vlan0 inet 192.168.1.110/24 Following figure shows steps the step we described and the system response to each step. Although the result of dladm command invocations like creating a VLAN is persisted automatically but the ipconfig commands effect will not persist unless we use the interface configuration files as discussed in the recipe 2 and recipe 3 of this chapter. We can delete a VLAN using delete-vlan subcommand of dladm command as shown bellow. \\# dladm delete-vlan vlan0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:33:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a VLAN, operating system register its availability and we can see the interface using dladm command. Each VLAN has a distinguished numerical tag which we specified it using the -v option along with a unique name. The name we specified let us to manage and administrate the interface while the specified tag makes it possible for the OS to separate packets intended for this interface in the link layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:34:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 6,5 and 8 in this chapter is recommended to grasp better understanding of network interfaces and IP address allocation. 10. Using IP multipathing and link based failure detection OpenSolaris provides some features to enhance network reliability including IP Multipathing. In IP multipathing multiple network interfaces installed in the system forms a group of network interfaces and handles each other’s load when one of them fails. Each interface in the group can be active or standby, meaning that the interface is either handling the traffic or it is reserved to get in and handle the traffic when one of the active interfaces fails. When we configure a group and we look for network reliability and the reliability requires that system detect the failed interface and let another interface handles its load. Two failure detection mechanisms are supported by OpenSolaris which are as follow: The probe based, active, failure detection: In active failure detection model, OS constantly probe the interface by sending a probe packet to an exclusive address provided by the interface for failure detection. If system does not receives a reply for its probe packet the interface will be considered faulty. The link based, passive, failure detection: This model is supported by network interface driver and is the default model OpenSolaris uses. In this model the network interface driver set a flag for the network interface indicating that the interface failed. In this recipe we configure IPMP group with two active network interface members using the passive failure detection. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:35:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we can create an IP multipathing group we should make sure that we met the following criteria. Each interface must have a unique MAC address. Each interface must be configured with a static IP address. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:36:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We will use ifconfig utility for our entire configuration as we are dealing with network layer. In our sample commands we will use e1000g0 and e1000g1 as the two interfaces which form the sm_group. Fowling figure shows fragment of ifconfig -a command prior to forming the sm_group. ![](post-img/=“media/image12.png” style=“width:5.5in;height:0.84722in” /\u003e insert image 3180_04_10.png As we can see there is nothing indicating that these interfaces are member of a group and as specified in the figure we two interfaces uses static IP addresses. Following command will make the group and joins the e1000g0 to the group. One interface can only join one group. \\# ifconfig e1000g0 group sm_group The first interface joining a group cause the group to be created and when last interface leave the group, system delete the group afterward. Following command joins the e1000g1 to the group. \\# ifconfig e1000g1 group sm_group Now if we check the status of our network interfaces we see a slight change in the e1000g0 and e1000g1 interface statuses as shown in the following figure. Like all network configuration we discussed in recipe and 3, the configuration we just made is not persistent over a reboot and we need to add some changes in the network interface configuration files to make the changes persistent on reboot. In recipe 2 and 3 we discussed how to configure static IP address for an interface and make that configuration persistent over a reboot using hostname.interface_name files stored in /etc/ directory. To make the grouping configuration persistent we just need to add the group command followed by group_name in the configuration line. For example to configure the e1000g0 we should have a /etc/hostname.e1000g0 file with the following content: 10.0.2.21/24 group sm_group To remove an interface from a group we just need to invoke grouping command with no group name. For example to remove e1000g0 from sm_group we can use the following command: \\# ifconfig e1000g0 group “” ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:37:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a multipathing group, OpenSolaris starts probing all interfaces by ICMP packets. When an interface fails to respond to the ICMP packet, the interface RUNNING flag clears and the IP address(es) assigned to the faulty interface get assigned to either a standby interface or to an interface with smallest number of IPs. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:38:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2, 3 and 11 in this chapter are recommended to learn more about networking in OpenSolaris. The IPMP-Tutorial Sneak Preview available at http://sun.systemnews.com/articles/144/1/Solaris/22782 is recommended for getting more insight into the IPMP. More detailed reference is available System Administration Guide: IP Services located at http://docs.sun.com/app/docs/doc/816-4554/ipmptm-1?a=view 11. Configuring Link Aggregation (Ethernet bonding) Link aggregation or commonly known Ethernet bonding allows us to enhance the network availability and performance by combining multiple network interfaces together and form an aggregation of those interfaces which act as a single network interface with greatly enhanced availability and performance. When we aggregate two or more network interfaces, we are forming a new network interface on top of those physical interfaces combined in the link layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:39:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We need to have at least two network interfaces in our machine to create a link aggregation. The interfaces must be unplumb-ed in order for us to use them in a link aggregation. Following command shows how to unplumb a network interface. \\# ifconfig e1000g1 down unplumb We should disable the NWAM because link aggregation and NWAM cannot co-exist. \\# svcadm disable network/physical:nwam The interfaces we want to use in a link aggregation must not be part of virtual interface; otherwise it will not be possible to create the aggregation. To ensure that an interface is not part of a virtual interface checks the output for the following command. \\# dladm show-link Following figure shows that my e1000g0 has a virtual interface on top of it so it cannot be used in an aggregation. To delete the virtual interface we can use the dladm command as follow \\# dladm delete-vlan vlan0 ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:40:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The link aggregation as the name suggests works in the link layer and therefore we will use dladm command to make the necessary configurations. We use create-aggr subcommand of dladm command with the following syntax to create aggregation links. dladm create-aggr \\[-l interface_name\\]\\* aggregation_name In this syntax we should have at least two occurrence of -l interface_name option followed by the aggregation name. Assuming that we have e1000g0 and e1000g1 in our disposal following commands configure an aggregation link on top of them. \\# dladm create-aggr -l e1000g0 -l e1000g1 aggr0 Now that the aggregation is created we can configure its IP allocation in the same way that we configure a physical or virtual network interface. Following command plumb the aggr0 interface, assign a static IP address to it and bring the interface up. \\# ifconfig aggr0 plumb 10.0.2.25/24 up Now we can use ifconfig command to see status of our new aggregated interface. \\# ifconfig aggr0 The result of the above command should be similar to the following figure. To get a list of all available network interfaces either virtual or physical we can use the dladm command as follow \\# dladm show-link And to get a list of aggregated interfaces we can use another subcommand of dladm as follow. \\# dladm show-aggr The output for previous dladm commands is shown in the following figure. We can change an aggregation link underlying interfaces by adding an interface to the aggregation or removing one from the aggregation using add-aggr and remove-aggr subcommands of dladm command. For example: \\# dladm add-aggr -l e1000g2 aggr0 \\# dladm remove-aggr -l e1000g1 aggr0 The aggregation we created will survive the reboot but our ifconfig configuration will not survive a reboot unless we persist it using the interface configuration files. To make the aggregation IP configuration persistent we just need to add create /etc/hostname.aggr0 file with the following content: 10.0.2.25/24 The interface configuration files are discussed in recipe 2 and 3 of this chapter in great details. Reviewing them is recommended. To delete an aggregation we can use delete-aggr subcommand of dladm command. For example to delete aggr0 we can use the following commands. \\# ifconfig aggr0 down unplumb \\# dladm delete-aggr aggr0 As you can see before we could delete the aggregation we should bring down its interface and unplumb it. In recipe 11 we discussed IPMP which allows us to have high availability by grouping network interfaces and when required automatically failing over the IP address of any failed interface to a healthy one. In this recipe we saw how we can join a group of interfaces together to have a better performance. By grouping a set of aggregations we can have the high availability that IPMP offers along with the performance boost that link aggregation offers. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:41:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The link aggregation works in layer 2 meaning that the aggregation groups the interfaces in layer 2 of network where network packets are dealt with. In this layer the network layer’s packets are extracted or created with the designated IP address of the aggregation and then are delivered to the lower, higher layer. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:42:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing recipe 2 and 3 in addition to recipe 8 and 9 is recommended. 12. IP tunnelling An IP tunnel connects one machine to another over a network without the two computers being aware of the intermediate network. The most basic use of IP tunnels is to securely connect two networks through two endpoint interface over an unreliable network. This use case is called Virtual Private Network (VPN). In OpenSolaris the point to point connection is created using a specific type of interface called IP Tunnel. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:43:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We should have a network connection between the two computers we want to create the point to point connection. We can use ping command to check the connectivity between our computers before proceeding with the tunnel setup. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:44:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The IP Tunnel interfaces are configured using ifconfig command. Following sequence of commands creates a tunnel between 192.168.1.110 which is our local machine and 66.10.12.1 which is the server. We need to make the interface available to the kernel. \\# ifconfig ip.tun0 plumb Now we should configure the tunnel itself. A tunnel has a source and a destination address at minimum. \\# ifconfig ip.tun0 tsrc 192.168.1.110 tdst 66.10.12.1 thoplimit 60 And finally we should bring up the interface. \\# ifconfig ip.tun0 up In the second step of creating the tunnel we used several parameters which are described below. The ip.tun0 specifies the IP tunnel interface name. The interface name format is ip.tun# which the digit specifies a 0-4096. The tsrc 192.168.1.110 specifies the tunnel source or the client machine IP address. When we are behind firewall we need to adjust the firewall to let IP protocol 41 packet pass. The tdst 66.10.12.1 specifies the tunnel destination or server machine. The destination machine must have the service running so we could connect to it. The thoplimit 60 specifies the maximum number of intermediately hops allowed between the client and the server to form the connection. We could use one single command to create, configure and bring up the interface. ifconfig ip.tun0 plumb ip.tun0 tsrc 192.168.1.110 tdst 66.10.12.1 thoplimit 60 up We used similar syntax in recipe 11 as well. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:45:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In this recipe we only setup the tunnel between two machines, for two networks to connect over a tunnel we should define some routing elements. In chapter 5 recipe # we discuss how we can setup routing tables. ","date":"2010-07-09","objectID":"/2010/07/opensolaris-networking-in-details/:46:0","tags":["Solaris","Networking","OpenSolaris"],"title":"OpenSolaris Networking In Details","uri":"/2010/07/opensolaris-networking-in-details/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In two previous entries I covered Introducing NIO.2 (JSR 203) Part 1: What are new features? and Introducing NIO.2 (JSR 203) Part 2: The Basics In this entry I will discuss Attributes introduced in NIO.2. Using attributes we can read platform specific attributes of an element in the file system. For example to hide a file system in DOS file system or to check the last access date of a file in a UNIX machine. Using NIO.2 we can check which attributes are supported in the platform we are running on and then we can decide how to deal with the available attributes. Following sample code shows how we can detect the available attributes and then how to manipulate them. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); //checking available attributes: Set\u003cString\u003e supportedViews = fs.supportedFileAttributeViews(); //We always have at least one member in the set, the basic view. BasicFileAttributes ba = p.getFileAttributeView(BasicFileAttributeView.class, LinkOption.NOFOLLOW_LINKS).readAttributes(); //Printing some basic attributes System.out.println(p.toString() + \" last access: \" + ba.lastAccessTime()); System.out.println(p.toString() + \" last modified \" + ba.lastModifiedTime()); // As I know I am in NIX machine I access the unix attributes. // If I didnt I should have iterate over the set to determine which // attributes are supported if (supportedViews.contains(\"unix\")) { PosixFileAttributes pat = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); System.out.println(pat.group().getName()); System.out.println(pat.owner().getName()); } I placed plethora of comments on the code so reading and understanding it get easier. In the next snippet we will see how we can read permissions of file system element. The first step in checking permissions is using the checkAccess method as shown below. the method throw an exception if the permission is not present or it will execute with no exception if the permission is present. FileSystem fs = FileSystems.getDefault(); Path p = fs.getPath(\"/home/masoud/netbeans-6.9-ml-linux.sh\"); try { // A method to check the access permissin p.checkAccess(AccessMode.EXECUTE); } catch (IOException ex) { Logger.getLogger(perm.class.getName()).log(Level.SEVERE, null, ex); } // Extracting all permissions of a file and iterating over them. //I know that I am dealing with NIX fs so I go directly with that attributes // otherwise we should check which attributes are supported and then we can // use them. PosixFileAttributes patts = Attributes.readPosixFileAttributes(p, LinkOption.NOFOLLOW_LINKS); Set\u003cPosixFilePermission\u003e st = patts.permissions(); for (Iterator\u003cPosixFilePermission\u003e it = st.iterator(); it.hasNext();) { System.out.println(it.next().toString()); } // Using PosixFilePermissions to convert permissions to different representations System.out.println(PosixFilePermissions.toString(st)); As you can see in the code we can use the helper class to convert the permission set to a simpl OS represeted permission of the element. for example the set can be translated to rwx— if the file has owner read, write and execute permissions attached to it. The helper calss can convert the os represenation of the permission to the permissions set for later use in other nio classess or methods. In the next entry I will conver more on permissions and security by tackling the Access Control List (ACL) support in the nio.2 ","date":"2010-06-23","objectID":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 3: File System Attributes and Permissions support in NIO.2","uri":"/2010/06/introducing-nio-2-jsr-203-part-3-file-system-attributes-and-permissions-support-in-nio-2/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction In this chapter we will cover fault and services administration and management. The fault management mostly deals with failing hardware components while service management and administration deals with software failures. A hardware failure is a faulty RAM module and a sample of failing service component is an HTTP server stopped functioning properly. OpenSolaris is an enterprise operating system meaning that it should be resilient to both software and hardware failures and automatically recover from the failure and repair itself or in cases when it is not possible for the OS to repair and recover, it should notify some administrator to take action. OpenSolaris goes further and provides the administrators with relevant knowledge and diagnosis for the failure using the unified infrastructure for fault management which includes the following features. Both faults and service management is built on top of a unified fault management model capable of reconfiguring the faulty hardware and restarting a failing service. The framework connects administrators to the unified knowledge repository which contains information about the error, what could have caused the error and how the administrator can treat the problem. The fault management service is called Fault Management Daemon (FMD) and we may use SMF for the Service Management Framework which provides us with the required tools and infrastructure to manage the software services. The knowledge repository is accessible through the web interface by navigating to http://www.sun.com/msg/ address. Each entry in the knowledge repository is uniquely identified by its URI and the fault and service management framework is aware about all of these identifiers and how each one of them is associated with a fault in the hardware components or software services. 1. Administrating faults using fmadm command In this recipe we will study how we can use the fmadm command to get the list of faulty components along with the detailed information about the fault. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:0:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before starting this recipe we should have a command console open and then we can proceed with using the fmadm command. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:1:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The most basic form of using fmadm command is using its faulty subcommand as follow #fmadm faulty When we have no error in the system, this command will not show anything and exit normally but with a faulty component the output will be different, for example in the following sample we have a faulty ZFS pool because some of its underlying devices are missing. Starting from top we have Identification record: This record consisting of the timestamp, a unique event ID, a message Id letting us know which knowledge repository article we can refer to for learning more about the problem and troubleshooting and finally the fault severity which can be Minor or Major. Fault Class: This field allows us to know what is the device hierarchy causing this fault Affects: tells us which component of our system is affected and how. In this instance some devices are missing and therefore the fault manager takes the Zpool out of service. Problem in: shows more details about the problem root. In this case the device id. Description: this field refer us to the a knowledge base article discussing this type of faults Response: Shows what action(s) were executed by fault manager to repair the problem. Impact: describe the effect of the fault on the overall system stability and the component itself Action: a quick tip on the next step administrator should follow to shoot the problem. This step is fully described in the knowledge base article we were referred in the description field. Following figure shows the output for proceeding with the suggested action. As we can see the same article we were referred to, is mentioned here again. We can see that two of the three devices have failed and fpool had no replica for each of those failed device to replace them automatically. If we had a mirrored pool and one of the three devices were out of service, the system could automatically take corrective actions and replace continue working in a degraded status until we replace the faulty device. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:2:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The fault management framework is a plug-able framework consisting of diagnosis engines and subsystem agents. Agents and diagnosis engine contains the logic for assessing the problem, performing corrective actions if possible and filing the relevant fault record into the fault database. To see a list of agents and engines plugged into the fault management framework we can use config subcommand of the fmadm command. Following figure shows the output for this command. As we can see in the figure, there are two engines deployed with OpenSolaris, eft and the zfs-diagnosis. The eft, standing for Eversholt Fault Diagnosis Language, is responsible for assessing and analyzing hardware faults while the zfs-diagnosis is a ZFS specific engine which analyzes and diagnoses ZFS problems. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:3:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… The fmadm is a powerful utility we which can perform much more than what we discussed. Here we can see few other tasks we can perform using the fmadm. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:4:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Notifying the Fault Manager about a repair or replacement We can use the repaired subcommand of the fmadm utility to notify the FMD about a fault being repaired so it changes the component status and allows it to get enabled and utilized. For example to notify the FMD about repairing the missing underlying device of the ZFS pool we can use the following command. #fmadm repaired zfs://pool=fpool/vdev=7f8fb1c77433c183 ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:4:1","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Administrating Fault Manager Log files We can rotate the log files created by the FMD when we want to keep a log file in a specific status or when we want to have a fresh log using the rotate subcommand as shown below. #fmadm rotate errlog \\| fltlog The fltlog and errlog are two log files residing in the /var/fm/fmd/ directory storing all event information regarding faults and the errors causing them. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:4:2","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about fmadm command we can use the man pages and the short help provided with the distribution. Following commands shows how we can invoke the man pages and the short help message. #man fmadm #fmadm --help Recipe 2 and Recipe 3 of this chapter are good steps to follow after learning the basics in this recipe. Oracle self healing article repository located at http://www.sun.com/msg/ is our friend and provides us with fair deal of knowledge when we just need to feed in the message id we retrieved from the fault reports. A good starting resource is the FM wiki page which is located at http://wikis.sun.com/display/OpenSolarisInfo/Fault+Management 2. Using fmstat to view fault management modules statistics In this recipe we will see how we can see statistics about faults in our operating system and its underlying hardware using the fmstat command. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:5:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use fmstat which is a console command therefore we will need to be in the console window or a virtual terminal to use the command. To open a terminal hit ALT+F2 and type gnome-terminal. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:6:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Invoking fmstat without any parameter will show all statistics for all modules briefly like the following figure. There are several columns in the output, following list summarize important columns. The ev_recv indicates the number of event received for this module The ev_acpt indicates the number of events accepted for this module The svc_t indicates the time spent for servicing the events The above command only shows overall statistics for the each module and does not provide what were the faults for each module. To see the detailed statistics for each module we can use the following syntax of fmstat command. fmdstat -m \u003cmodule/\u003e -a For example the fmd-self-diagnosis which is a module for keeping the FMD healthy had 4 errors; let’s see more detailed stats for this module. The fmstat command can do more than showing one time snapshot of the errors, it can collect and display statist continually which we can use it in our scripts or task automation. We can use following syntax to view the stats in time based interval. fmstat interval count For example to see the zfs-diagnosis module stats we 10 times with 5 seconds interval we can use the following command #fmstat –m zfs-diagnosis 5 10 ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:7:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The fault management system stores all of its messages in the /var/fmd/errlog and /var/fm/fltlog and when we invoke a fault management command the command looks for the relevant records in this file, cross reference them with the fault management database to connect the fault with knowledge base URLs and descriptions if required and then show the result. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:8:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also We can use the man pages for fmstat or its short help for more information about the command. #man fmstat #fmstat --help The recipe 1 and recipe 3 of this chapter can be a great help for understanding and using the fault management system of OpenSolaris. 3. Using fmdump to analyze the fault management logs The fmdump command is the command we will use when we need to see further details compared to what the fmadm can gives us. Basically fmdump gives us a more direct and low level access to the error logs content rather than formatting and filtering before displaying the messages. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:9:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We should fire up a console or a virtual terminal before using this command as it is a CLI command. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:10:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The command is very flexible and relatively simple to use. The most basic use of the command is using it with no parameter which will show a list of one line records each associated with a fault similar to the following figure. The column starts with the fault timestamp, the fault unique ID and the message id which we can use to find knowledge base article related to this type of fault in the sun fault management knowledgebase. For example we can use the http://sun.com/msg/ZFS-8000-D3 to see what the reason for the second fault is and what we can do to fix the problem. The fmdump command provides us with a fair level of flexibility in using it. Following table shows what are its important parameters and use cases. Parameter Description, Sample use -u \u003cfault ID\u003e Select the fault identified with the given uuid # fmdump -u 252601b6-98e6-4b5d-e216-cb42d1f028f0 -v Shows the selected fault brief information # fmdump -v -u 252601b6-98e6-4b5d-e216-cb42d1f028f0 -V Shows detailed information about the selected fault # fmdump -V -u 252601b6-98e6-4b5d-e216-cb42d1f028f0 -c Select a class of faults for fmdump to operate on -e Shows errors listed stored in the fault management logs [for the selected fault class] # fmdump -e -c ereport.fs.zfs.zpool -t and -T Select events occurred after the selected time or before the specified time. # fmdump -t 20:50 -V -e -c ereport.fs.zfs.zpool Let’s see what is output for one the commands we discussed above. Assuming that we want to see brief information about 252601b6-98e6-4b5d-e216-cb42d1f028f0 we can use the following command: #fmdump -v -u 252601b6-98e6-4b5d-e216-cb42d1f028f0 Following figure shows the output for this command: We can combine the parameters in any way we want, they just apply filters over the log entries before displaying them like -t and -T or include the detailed information about the event like -V or -v. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:11:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… This utility acts as an interface between administrator and raw log files. The command basically reads the log files and sort, filter; show its content in a readable way for administrators. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:12:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… To learn more about the fmdump command we can review its man pages or the short help. #fmdump --help #man fmdump ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:13:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 1 and recipe 2 of this chapter can be helpful for understanding the overall fault management in OpenSolaris. 4. Checking fault management support for the system components We cannot expect all of our hardware components and software services to be supported by the fault management framework because basically different components shows different symptoms when they are failing and not all of them are known to the fault management system. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:14:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We can use the fmtopo CLI utility to check which system components are supported by fault management and which components are not. To start we need to fire a console or switch to a virtual terminal. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:15:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Invoking the fmtopo command with no parameter will show all of the fault management supported resources while we can filter the result using the wildcard characters. Following figure shows a part of the output we might get when invoking the fmtopo -p command. Let’s see what does each of the abbreviation means. The hc stands for Hardware Component and specifies a resource that can detect an error Th ASRU stands for Automated System Recovery Unit and specifies any resource that must be disabled in the system to prevent a fault it is causing. The FRU stands for Field Replaceable Unit and specifies the resource which should be replaced to repair a fault. The Label string specifies the location of the resource. For example MB or SLOT #. For example to see anything related to motherboard in the supported list we can use the following command. #fmtopo -V '\\*/motherboard=0' The fmtopo command is very flexible and lets us extract fine amount of information about the supported components. For example we can use the following command to export the entire result as xml format and store it into a file. #fmtopo -x \u003e/export/home/masoud/aa.xml Or we can use it to view the CPU units in the system as shown in the following figure. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:16:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The fmtopo command checks the available resources in the system and cross reference the list of these resources with the list of know and supported resources and finally print the result. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:17:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also You can learn more about the fmtopo and its usage by referring to its man pages or the online reference guide located at http://docs.sun.com/app/docs/doc/819-3196/gemgw?a=view 5. Authoring SMF service manifest SMF services are basically daemons staying in background and waiting for the requests which they should server, when the request come the daemon wake ups, serve the request and then wait for the next request to come. The services are building using software development platforms and languages but they have one common aspect which we are going to discuss here. The service manifests which describe the service for the SMF and let the SMF manage and understand the service life cycle. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:18:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To write a service manifest we should be familiar with XML and the service manifest schema located at /usr/share/lib/xml/dtd/service_bundle.dtd.1. This file specifies what elements can be used for describing a service for the SMF. Next thing we need is a text editor and preferable an XML aware text editor. The Gnome gedit can do the task for us. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:19:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The service manifest file composed of 6 important elements which are listed below. The service declaration: specifies the service name, type and instantiation model Zero or more dependency declaration elements: Specifies the service dependencies Lifecycle methods: Specifies the start, stop and refresh methods Property groups: Which property groups the service description has. Stability element: how stable the service interface is considering version changes Template element: more human readable information for the service. To describe a service, first thing we need to do is identifying the service itself. Following snippet shows how we can declare a service named jws. \u003cservice name='network/jws’ type='service' version='1'/\u003e \u003csingle_instance/\u003e The first line specifies the service name, version and its type. The service name attribute forms the FMRI of the service which for this instance will be svc:/network/jws. In the second line we are telling SMF that it should only instantiate one instance of this service which will be svc:/network/jws:default. We can use the create_default_instance element to manipulate automatic creation of the default instance. All of the elements which we are going to mention in the following sections of this recipe are immediate child elements of the service element which itself is a child element of the service_bundle element. The next important element is dependency declaration element. We can have one or more of this element in our service manifest. \u003cdependency name='net-physica' grouping='require_all ' restart_on='none' type='service'/\u003e \u003cservice_fmri value='svc:/network/physical'/\u003e \u003c/dependency\u003e Here we are telling that our service depends on the svc:/network/physical service and this service needs to be online before our service can start. We will discuss service dependencies in recipe 7 of this chapter. Other values for the grouping attribute are as follow: The require_all which represent that all services marked with this grouping must be online before our service came online The require_any which represents that any of the services in this grouping suffice and our service can become online if one of them is online The optional_all presence of the services marked with this grouping is optional for our service. Our service works with or without them. The exclude_all: specifies the services which may have conflict with our service and we cannot become online in presence of them The next important elements are specifying how the SMF should start, stop and refresh the service. For these tasks we use three exec_method elements as follow: \u003cexec_method name='start' type='method' exec='/opt/jws/runner start' timeout_seconds='60'\u003e \u003c/exec_method\u003e This is the start method, SMF will invoke what the exec attribute specifies when it want to start the service. \u003cexec_method name='stop' type='method' exec=':kill' timeout_seconds='60'\u003e \u003c/exec_method\u003e The SMF will terminate the process it started for the service using a kill signal. By default it uses the SIGTERM but we can specify our own signal. For example we can use ‘kill -9’ or ‘kill -HUP’ or any other signal we find appropriate for our service termination. \u003cexec_method name='refresh' type='method' exec='/opt/jws/runner reload_cof' timeout_seconds='60'\u003e \u003c/exec_method\u003e The final method which we should describe is the refresh method in which we should reload the service configuration without disturbing its function. The start and stop methods description are required to be present in the manifest in order to import it into the SMF repository but the refresh method description and implementation is optional. The timeout_seconds=‘60’ specifies that the SMF should wait for 60 seconds before aborting the method execution and retrying it over. We can use longer timeout when we know that the execution of the method take longer or lower timeout when we know that our service starts sooner. The property group elements specify which property groups the SMF ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:20:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we import the service manifest into the service repository, SMF will scan the service profiles and check whether this service is required to be enabled either directly or because of a dependent service or not. If required, the SMF will use the start method specified in the manifest to start the service. But before starting it, SMF will check its dependencies and start any required service recursively if they are not already online. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:21:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing the XML schema located at /usr/share/lib/xml/dtd/service_bundle.dtd.1 and the next three recipes is recommended for grasping a better understanding of this recipe. 6. Listing Services and viewing Service details OpenSolaris service management framework or simple SMF is an extendible and customizable framework for managing software services. The management includes definition, dependency management, starting, stopping and watching the services for fail over recovery. In this recipe we will study how we can use svcs command to see list of the installed services and how to check the service health and diagnose its problems. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:22:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The svcs is a CLI utility so we should either open a console or switch to a virtual terminal. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:23:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The svcadm command has three primary functions listed below. Listing all services, either online, offline, enabled, disabled or faulty Showing a service detailed information Displaying service dependencies. The first use case is possible by simply invoking the svcs command, it will show all services which are in normal stat. Normal state means services which are enabled either online or offline. To see a list of services which are installed either disabled or enabled we can use the -a parameter when invoking the command. To see a list of all faulty services we can use the -x parameter when invoking the svcs command. In my case the DHCP service is faulty and the status message shows me some details about the problem along with the link of the knowledge base article that I can review to resolve the problem. In all of the above use cases we can add -pHv to see mode details about the listed services. Next important function of svcadm is checking status of a service. To see a service details or brief status record we can use one of the following svcs command’s syntaxes. svcs \u003cservice name\u003e svcs -l \u003cservice name\u003e The important part of the command is the argument. The service name part can be one service name, multiple separate names separated by space or it can contain a wildcard representation of services we want to operate on. We can use any of the following formats for service name part or we can combine them together for example when we want to view details of all XVM services and a network service like ssh. The Fault Managed Resource Identifier (FMRI) format, for example svc:/network/smtp:sendmail The abbreviation format, for example network/smtp:sendmail The name matching patterns like network/*mail, network/smtp, smtp:sendmail, smtp or sendmail. Following figure shows how we can check the detailed status of our FTP service using the scvs command. All the details we can see are self explanatory except for the restarter which we will discuss in recipe 5 of this chapter. The pattern matching syntax of passing service name argument to svcs is very useful when we want to check status of several services with matching FMRIs. For example to see detailed status of all XVM services we can use the following command. #svcs -l svc:/system/xvm/\\* To view details of all XVM services and the SSH service we can use the following combination of naming syntaxes. #svcs -l svc:/system/xvm/\\* ssh Finally the last important function of this utility is checking a service dependency using the -d option. For example to check all dependencies of the SMTP service we can use the following command. #svcs -d smtp We can see the result of invoking this command below. The list shows which services the smtp service depends on to function properly and what is status of those services. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:24:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Majority of the svcs utility tasks involves manipulating the service repository and service manifests which we discussed in recipe 5 and will discuss further in recipe 8 and 9. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:25:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting list of process a service depends on To see the list of processes which a service depends on, we can use the -l -p parameters. For example to see detailed list of process which smtp service depends on them for functioning properly we can use the following command. #svcs -l -p smtp Following figure shows the sample output for the above command. Pay attention to the list of processes and their viability for the smtp service. We can see some similarities between the list of services the SMTP service depends of and the list of process. In the list of process we have an additional column letting us know whether the dependency is optional or not. This fall useful when we need to kill some process or services but we need to ensure that the service is not required for any other services to function properly. For example to check whether any of the network services is depending on the autofs service we can use a command similar to the following one. #svcs -l -p svc:/network/\\* \\|grep autofs For me the output is similar to the following figure suggesting that at least two other services have optional dependencies on this one. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:25:1","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about SMF and services review recipe 5 of this chapter which discuss how to create a service manifest and basically install a service into OpenSolaris SMF. 7. Administrating services using svcadm utility In this recipe we learn how to use the svcadm, which is one of the four common commands for dealing with services, to administrate the services. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:26:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The svcadm is a CLI utility so we should either be in a virtual terminal or fire a console to run it. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:27:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The svcadm utility has a set of subcommands which we use them to perform different administrative tasks. The command syntax is similar to the following schema: svcadm -v subcommand [parameters] \u003cservice name(s)\u003e When we use the -v before a subcommand svcadm displays a verbose output of what it is doing. Following table shows the list of subcommands and their options -r -s -t -I enable * * * disable * * mark * * The -r option specifies that all services required for the operand service will be enabled recursively The -s option specifies synchronous execution of commands which can be enabling services or disabling them The -t option specifies that the command should affect the service temporarily until the next restart. The -I option specifies that the operation must take place immediately. The following list shows major tasks we can perform using the svcadm subcommands. Restarting and Refreshing a service Enabling and disabling a service Setting and clearing the maintenance stat For each of the tasks named in the above list we have a subcommand in svcadm to use. For example let’s study the first task. #svcadm restart ssh #svcadm refresh ssh In the first command we stop and then start the service again to perform a full cycle of the service and in the second command we are reading the service configuration from its manifest so any changes in the life cycle methods get reflected in the next execution. We can use any of the service name schemas discussed in the previous recipe except for the wildcard name matching which can result in tens of services get restarted. When we need to operate on more than one service we can simply pass all the service names to the command. For example to restart ssh and smtp services we can use #svcadm disable ssh smtp The next task is enabling and disabling a service, the following commands shows how we can use the enable and disable subcommands. #svcadm disable ssh #svcadm enable ssh The first command takes the service offline and then disables it in the profile so the service won’t start in the next system restart unless we enable it. A disabled service cannot be restarted but we can refresh its configuration for the next use of its life cycle methods. When we enable a service using the above syntax it does not guarantee that the service will work properly because it might depends on other services which are not enabled. To enable a service and all services which it depends on we can use the -r option. For example the following command enables the ssh service and all services which it depends on them for proper functioning. #svcadm enable -r ssh Enabling a service cause the service start-up to bootstrap asynchronously. To ensure that the service is started and see the result of the command invocation we can use the -s parameter as follow: #svcadm enable -s ssh A service may automatically enter maintenance state when SMF fails to start it or we may need to push it into the maintenance state when required. The mark command marks a service with one of the maintenance, or degraded tags the tagging can be imitate or temporary for this session only. For example to move ssh service into maintenance mode temporarily we can use the following command. #svcadm mark -t maintenance ssh Now if we look at the service details we can see the state is now changed to maintenance. To clear the ssh service state from maintenance or degraded we can use the clear subcommand as follow. #svcadm clear ssh This command will clear the service state and put it back to one of the major states including offline and online and disabled. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:28:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The svcs command uses the service manifest file and the SMF repository to execute our administrative commands. For example when we disable a service, the service state changes to disabled in the repository and when we restart a service the start method of the service manifest file is invoked to perform all the necessary steps to start the service. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:29:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also It will be good to review recipe 5 in which we discussed service manifest file and recipe 8 and 9 which discusses the service repository to grasp a better understanding of this recipe. Reviewing the man pages and short help message is another good point to get better understanding of the subcommands and options. #svdadm --help #man svcadm 8. Configuring services We can configure a service by adjusting the parameters which affect the way that SMF interact with the service like changing timeouts, dependencies, and name and so on by accessing the SMF repository data. The utility command for configuring services is named svccfg. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:30:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready The svccfg is a console utility and we should use it in a console or virtual terminal. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:31:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… The svccfg operates both in interactive mode and also in headless or script mode to be used in administration scripts. Working with svccfg in command line mode involves specifying all of the options prior to invoking the command. For example to view list of the smtp service properties we can use the following command. #svccfg -s smtp listprop To start working with the command in interactive mode we can enter its shell by invoking the svccfg with no parameter. Following figure shows the svccfg environment when we execute the help command in its shell. The help commands shows a two column message, The first column shows the commands category and the second column list of commands fitting into that category. We will discuss using one command from each category. From the general command, we can use the end command to exit the shell and commit all changes to the repository. We can use the help command to get help about the commands independent of the category we can use the following syntax. svcs:\u003e help \u003ccommand name\u003e The manifest commands lets us import and export the manifest of each service or basically the entire SMF database. For example to export ssh service manifest file we can use the following command. svcs:\u003e export ssh\u003e /opt/bck/ssh.smf We can use export command without the \u003efile name to export the manifest into the standard output (monitor) and review its content. To import he service manifest into the repository if managed to mess the current manifest we can simply use the import command as follow. svc:\u003e import /opt/bck/ssh.smf Using the archive command we can export the entire SMF repository containing all manifests into a file svc:\u003earchive \u003e /opt/bck/arch.smf We will discuss the profile command in the next recipe as it requires more attention than other commands. The entity commands allow us to perform CRUD operations on SMF database. We can get a list of all system services by using svc:\u003e list system/\\* And then select a service which we want to work on its properties using the select command. For example if we need to change the ssh service properties we can select it using the following command before operating on it. svc:\u003e select ssh Selecting a service will add another level in the shell environment and allows us to execute commands operating on services themselves. The snapshot commands allow us to list instance snapshots or revert a service instance to its previous snapshots. For example in the following figure we can see how to get list of ssh default instance snapshots. The instance category only has one command named refresh which perform the same task that the svcadm refresh performs, reloading the service configuration from the repository so they affect the service lifecycle in the next invocation of a lifecycle function like enable, disable or restart. For example to refresh the default instance of the ssh service we can invoke refresh command while we have selected it. The group property manipulation commands allows us to list, add or remove a property group from the service. For example to add a property named reason and then remove it we can use the following commands. The property commands provide us with CRUD operation over the service properties. For example to list all properties of ssh service default instance command we can invoke listprop commands when we have the ssh service selected. To add a property we can use addprop command. For example to add a new integer property inside the custom group we can use the following command. svc:/network/ssh\u003e setprop custom/iss2=integer: 1270 We can check a property’s value using the listprop command as we can see in the following figure. To update an already existing property we can use the setprop command with the same syntax but we should omit the property type. Finally we can use the value manipulation commands to change or add a value to a property. For example to change the value of custom/iss we can use the following command. svc:/networ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:32:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The svccfg like other service administration commands manipulates the SMF repository to change a service configuration. The SMF repository is a registry where all services configuration (manifests) are stored. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:33:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about serve repository and service management check recipe ## and two previous recipes. The man pages for svcadm are another source of knowledge and understanding for the svccfg shell commands. 9. Using Service Profiles The service profiles are a concept allowing us to quickly change the OpenSolaris installation from serving one purpose to providing another, for example from serving as a storage server to a Web server or directory server. The default service profile called the generic profile is a multipurpose profile in which all of the basic services are started to make the OS a multipurpose server while we have other service profiles which we can enable on top of the generic profile to quickly tailor the system for other purposes. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:34:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use the svccfg utility to manage the profiles. So it would be good to review the recipe 5 and 8 before continuing this recipe. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:35:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To start using a profile we need the profile description in the format acceptable for the SMF saved as file accessible to the svccfg command. OpenSolaris come bundled with some predefined profiles which are stored in /var/svc/profile directory which is show in the following figure. The names are self explanatory and content of each file contains the explanation about the profile purpose. To see content of each file we can simply use cat command or gedit. For example to view the content of the ns_nis.xml we can use one of the following commands. #gedit /var/svc/profile/ns_nis.xml #cat /var/svc/profile/ns_nis.xml To apply a profile we can use the apply command in the svccfg shell along with passing the profile storage file. svc:\u003e apply /var/svc/profile/ns_nis.xml We can extract the current running profile of system services using the extract command as follow. scv:\u003e extract \u003e /var/svc/profile/current.xml We can apply the extracted profile when we need the system to load all the services with their current status as we had them when we extracted the profile. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:36:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Each of the profiles storage file contain a list of services along with their instances and instance statuses. The status can be enabled or disabled based on what we require the profile to do. The files are well formed XML files conforming to /usr/share/lib/xml/dtd/service_bundle.dtd.1 schema. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:37:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Reviewing the recipe 5 and 8 is recommended for learning this recipe more effectively. ","date":"2010-06-09","objectID":"/2010/06/managing-faults-and-services-in-opensolaris/:38:0","tags":["Solaris","Networking","Fault Handling","Services","OpenSolaris"],"title":"Managing faults and services in OpenSolaris","uri":"/2010/06/managing-faults-and-services-in-opensolaris/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"In this part we will discuss the basic classes that we will work with them to have file system operations like copying a file, dealing with symbolic links, deleting a file, and so on. I will write a separate entry to introduce classes which are new to Java 7 for dealing with streams and file contents, watching service and directory tree walking. If you want to know what are new features in Java SE 7 for dealing with IO take a look at Introducing NIO.2 (JSR 203) Part 1: What are new features? Before NIO.2, dealing with file system was mainly done using the File class and no other base class was available. In NIO.2 it there are some new classes at our disposal to take advantage of their existence to do our job. FileSystems: Everything starts with this factory class. We use this class to get an instance of the FileSystem we want to work on. The nio.2 provides a SPI to developed support for new file systems. For example an in-memory file system, a ZIP file system and so on. Following two methods are most important methods in FileSystems class. The getDefault() returns the default file system available to the JVM. Usually the operating system default files system. The getFileSystem(URI uri) returns a file system from the set of available file system providers that match the given uir schema. Path: This is the abstract class which provides us with all File system functionalities we may need to perform over a file, a directory or a link. FileStore: This class represents the underplaying storage. For example /dev/sda2 in *NIX machines and I think c: in windows machines. We can access the storage attributes using FileStoreSpaceAttributes object. Available space, empty space and so on. Following two sample codes shows how to copy a file and then how to copy it. public class Main { public static void main(String\\[\\] args) { try { Path sampleFile = FileSystems.getDefault().getPath(\"/home/masoud/sample.txt\"); sampleFile.deleteIfExists(); sampleFile.createFile(); // create an empty file sampleFile.copyTo(FileSystems.getDefault().getPath(\"/home/masoud/sample2.txt\"), StandardCopyOption.COPY_ATTRIBUTES.REPLACE_EXISTING); // Creating a link Path dir = FileSystems.getDefault().getPath(\"/home/masoud/dir\"); dir.deleteIfExists(); dir.createSymbolicLink(sampleFile); } catch (IOException ex) { Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex); } } And the next sample shows how we can use the FileStore class. In this sample we get the underlying store for a file and examined its attributes. We can an iterator over all available storages using FileSystem.getFileStores() method and examine all of them in a loop. public class Main { public static void main(String\\[\\] args) throws IOException { long aMegabyte = 1024 \\* 1024; FileSystem fs = FileSystems.getDefault(); Path sampleFile = fs.getPath(\"/home/masoud/sample.txt\"); FileStore fstore = sampleFile.getFileStore(); FileStoreSpaceAttributes attrs = Attributes.readFileStoreSpaceAttributes(fstore); long total = attrs.totalSpace() / aMegabyte; long used = (attrs.totalSpace() - attrs.unallocatedSpace()) / aMegabyte; long avail = attrs.usableSpace() / aMegabyte; System.out.format(\"%-20s %12s %12s %12s%n\", \"Device\", \"Total Space(MiB)\", \"Used(MiB)\", \"Availabile(MiB)\"); System.out.format(\"%-20s %12d %12d %12d%n\", fstore, total, used, avail); } In next entry I will discuss how we can manage file attributes along with discussing the security features of the nio.2 file system. ","date":"2010-06-01","objectID":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 2: The Basics","uri":"/2010/06/introducing-nio-2-jsr-203-part-2-the-basics/"},{"categories":["Programming","Old Blog Migrated Content"],"content":"I will write a series of blog to discuss what are the new features introduced in NIO.2 (JSR 203). The NIO.2 implementation is a part of OpenJDK project and we can alreay use it by downloading a copy of OpenJDK binary. In the first entry I will just go through what are these new I/O features of Java 7, which help developer iron out better applications easier. Talking about File systems support and features which let us deal with file system we can name the following features: Platform friendly-ness of NIO.2: We can deal with all file systems in a unified model. File tree walk: We can walk on a file tree and examine each node using the built-in APIs, NIO.2 to let us know whether the current member is a file, a directory or a symbolic link. We can then perform any operation we want on that node. File Operations (Copy, Delete, Move): Basic operations are supported with plethora of options. The move operation can be performed in atomic way. Symbolic links support: count soft and hard links as well as managing them. Support for file attributes in NIO.2: Managing file systems attributes is fully supported for different file systems including DOS, POSIX… File system change notification: We can setup a watch service and receive notification when a change happens on the path we are watching. SPI for providing new file systems support, for example to support zip, zfs, btrfs, we can implement the provider interfaces and use the unified API to access that specific file system using our implementation Working with sockets and reding/ writing files we can name the following features: Multicasting is now supported in the DatagramChannel meaning that we can send and receive IP datagrams from a complete group. Both IPv4 and IPv6 are supported. Asynchronous I/O for sockets and Files: Now we can have Asynchronous read and write both on channles and files. It basically means we can have event driven I/O over both networks and files. Other improvement, features: Support for very large multi gigabyte buffers Some MXBeans are included to monitor IO specific buffers. Interoperability between Java 7 IO and previous versions using the Path API. I will post sample codes for each of these features in upcomming entries. So stay tuned if you want to learn more about NIO.2 ","date":"2010-05-20","objectID":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/:0:0","tags":["Java","JCP"],"title":"Introducing NIO.2 (JSR 203) Part 1: What are new features?","uri":"/2010/05/introducing-nio-2-jsr-203-part-1-what-are-new-features/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Introduction ZFS pools provide us with the underlying storage with which we can create files and directories inside it right after we create it. But OpenSolaris and ZFS provides more than that by introducing ZFS datasets… In this recipe we will work on top of a zfs pool named fpool and the default root pool named rpool. So before continuing on this recipe we should have a pool named fpool created. Creating ZFS pools is discussed in the recipe 9, Creating and Destroying ZFS pools, of chapter 2. 1. Getting list of ZFS datasets In this recipe we will discuss how we can get the list of ZFS datasets and also how to get detailed information about a dataset. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:0:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will work with list subcommand of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:1:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can use the list subcommand of the zfs command to get list of available datasets. Like zpool command we can use this command with a pool name or without a pool name to get list of all available datasets in the system. The simplified schema of the list subcommand is shown below: list \\[-rH\\] \\[list_of_pools\\] In this command: The -r option specifies that we want to see a recursive list of all child datasets in addition to root datasets. The -H specify that we do not want to see the header row of the printed information. The list_of_pool arguments specifies a space separated list of pools we want to see their datasets, passing no argument means we want all datasets of all pools to be included in the output. For example to view list of all datasets recursively in both rpool and fpool we can use the following command. \\# zfs list -r fpool rpool The output for the above command is similar to the following figure. As we can see in the figure, the child datasets are also listed in the output. One thing which we might notice is the differences between a ZFS file system name and its mount point, especially for the zpools. Each ZFS dataset has a mount point which can be different than its name and its location in the parent dataset. When we are using commands like zfs and zpool we will refer to the name of the dataset and when we ware accessing a file system dataset we use the mount point. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:2:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The command reads metadata related to the questioned pools and print them in the output. Metadata information of all ZFS objects are stored in /etc/zfs directory. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:3:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also We can get more information about datasets using their properties which is discussed in the last recipe of this chapter. 2. Creating and destroying ZFS File System We can create and destroy a ZFS file systems using create and destroy sub commands of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:4:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool. We can find how we can create ZFS pools in recipe 9 of chapter 2. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:5:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To create a ZFS file systems we should use the create subcommand of zfs command. The command syntax is shown below. create \\[-p\\] \\[-o property=value\\] ... \\\u003cfilesystem\\\u003e In this syntax: The -p option specifies that any non-existing parent addressed should be created. The -o can be used to pass ZFS properties to be assigned to the command. For example we can use it to enable the compression during dataset creation. We will discuss more about these properties in recipe number…. The file systems argument specifies the dataset path we want to create. Any of the non-existing parent file systems specified in this argument will be created recursively provided that we use -p option. Now to create file systems named parent/fset inside the fpool we can use the following command based on the above syntax. \\# zfs create -p -o compression=on fpool/parent/fset The command enables the compression for the file systems during the dataset creation time. The following image shows how this dataset appears in the pool. To destroy a dataset we can use the destroy subcommand of the zfs command. The command schema is as follow: destroy \\[-rRf\\] filesystem\\|volume\\|snapshot In this syntax: The -r option asks the command to destroy all descendents of the target dataset. The -R option asks the command to destroy all depended objects like clones and snapshots. We will discuss clones and snapshots in recipe number The -f option force unmount any mount point. The argument specifies type of the dataset we want to delete; either it is a volume, a file system or a snapshot. We will discuss volumes in next recipe. To delete the parent dataset with its entire child we can use the following command. \\# zfs destroy -rf fpool/parent The following figure shows the fpool datasets after we execute the command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:6:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS dataset its related records are inserted in ZFS metadata storages located at /etc/zfs and any further access to the file system will update these records. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:7:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Renaming a ZFS file system We can rename a ZFS dataset using the rename subcommand of the zfs command. For example to rename the parent file system we created previously we can use the following command \\# zfs rename fpool/parent fpool/parent2 ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:7:1","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also In the next three recipes we discuss managing other types of datasets available in ZFS including volumes, snapshots. 3. Creating and Destroying ZFS volumes In this recipe we will see how we can create or destroy a ZFS volume using the create and destroy subcommand of the zfs command. ZFS volumes provide the software systems which require a block or character device to benefit from the storage pooling of ZFS pools and in the same time has the freedom of accessing the storage in block or character mode. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:8:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:9:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… OpenSolaris installation creates two ZFS volumes one for SWAP area and the other for storing different dumps created when the OS face a problem. To see the list of volumes we can use the following command. \\# zfs list -r rpool Thefollowing figure shows the result of executing the given command. As you can see none of the volumes has a mount point because they are block devices and not file systems unless we format them and mount them manually. To create a ZFS volume we can use the following syntax of the create subcommand of the zfs command. create \\[-ps\\] \\[-b blocksize\\] \\[-o property=value\\] ... -V \\\u003csize\\\u003e \\\u003cvolume\\\u003e In this syntax we have: The -p option similar to the -p option in creating dataset specifies that all non-existing parents addressed in the volume name should be created. The -s option specifies that we want to create a sparse volume. In contrast with normal volumes sparse volumes does not occupy any space after we created them and the volume size grows when we copy some data into it. The maximum size is specified using the -V parameter. The -b option is similar to using -o volblocksize option which let us determine the volume block size. The -V parameter specifies the volume capacity or size, if we use the -s parameter the size will not get allocated immediately but rather the volume grows when we copy some data into it. The volume argument specifies the volume name. For example to create a volume named fvol with a fixed size of 100 MB we can use the following command. \\# zfs create -v 100m fpool/fvol To create a sparse volume named svol with a maximum size of 500 MB we can use the following command. \\# zfs create -v 500m fpool/svol Following figure shows the fpool datasets after creating these two volumes. When we create a volume we can access it using its associated raw and character node names. The raw name for each ZFS volume consists with /dev/zvol/rdsk/pool_name/volume_name naming schema The character name for each ZFS volume consists with /dev/zvol/dsk/pool_name/volume_name naming schema To understand the naming schema better, the following figure shows how these volumes appear in the block and character devices list. Using sparse volumes is not a good move when we have sensitive applications utilizing the volume because the volume may not be able to grow when there might be no empty space left in the pool when the volume requires growing. Destroying a volume is similar to destroying a file system which we discussed in recipe 2. For example we can use the following command to destroy the fvol \\# zfs destroy -rf fpool/fvol ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:10:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS volume, all related metadata for the volume will be stored in the ZFS metadata storage located at /etc/zfs and the space we assigned to the volume will be reserved for the volume and will be subtracted from the underlying pool’s available space. The volume’s node will be created in the /dev/zvol subdirectories to allow both raw and block access level to the volume. Destroying a pool return back the allocated space, remove the associated records and also delete the volume node names created in the /dev/zvol subdirectories. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:11:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also You can take a look at recipe 2 and 4 which contains more information about different types of datasets supported by ZFS. 4. Creating and Destroying a ZFS snapshot In this recipe we will learn another ZFS dataset named snapshots. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:12:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named fset to continue with this recipe. Creating file systems dicussed in recipe 2 of this chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:13:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… To create a snapshot we can use the snapshot subcommand of the zfs command. The command syntax is as follow: snapshot \\[-r\\] \\[-o property=value\\] ... \\\u003cfilesystem@snapname\\|volume@snapname\\\u003e In this syntax: The -r specifies that we want to create snapshot from all descendent datasets of the target dataset. The -o specifies a property we want to set during the snapshot creation. The filesystem@snapname | volume@snapname specifies the snapshot name. A snapshot is created from a file system or a volume and conforms to a naming schema similar to dataset_name@snapshot name. In the naming schema the first part is the dataset name for example fpool/fset followed by @ character and then the snapshot name like before_migration. To create a snapshot named initial from the fpool/fset we can use the following command \\# zfs snapshot \u003cfpool/fset@initial\u003e Following figure shows the snapshot status before anything changes in the fset and after we delete some files and add some other in their place. The main purpose of creating snapshot is to revert back to the point when we created the snapshot. To rollback the changes that a dataset have seen after creating a snapshot we can use rollback subcommand of the zfs command as follow. \\# zfs rollback \u003cfpool/fset@initial\u003e This command rollbacks the fset file system back to the state when we created the fpool/fset@initial snapshot. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:14:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… A snapshot as its name implies is an image of a file system or a volume in a specific point in time. The main reason behind creating snapshots is possibility to revert a file system or a volume back to a specific point in time. Utilising the copy-on-write and metadata blocks, a ZFS snapshot does not occupy any space after we create it, and the creating process is amazingly fast. After the snapshot is created, any change in the file system will cause the snapshot size to increase in order to reflect the changed blocks. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:15:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Renaming a snapshot ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:1","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Accessing snapshot data without reverting active dataset ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:16:2","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe number 3 and 5 are closely related to the snapshots and reading them can give you a better understanding of this recipe. 5. Creating and Destroying a ZFS clone In this recipe we will learn how to deal with ZFS clones. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:17:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready Before we get down to the actual business of managing clones we should have an accessible pool and a dataset, either a ZFS volume or a ZFS file system, to study on. I assume that we have fpool and a file system named fset in our disposal to work with. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:18:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can use the clone subcommand of the zfs command to create a clone. The command syntax is as follow: zfs clone \\[-p\\] \\[-o property\\] ... source_snapshot target_dataset In this syntax: The -p option specifies that all parent datasets need to be created if not existing already The -o allows us to specify some options for the clone. List of all properties for a dataset is discussed in recipe number 12 The source_snapshot specifies which snapshot we want to base the clone on. The target_dataset specifies where we want the clone to be created. The target_dataset is of the same type as the source_snapshot is based on. To create a clone we need a snapshot so, lets create one. \\# zfs snapshot \u003cfpool/fset@initial\u003e Now we can create a clone based on the state which this snapshot represent from the underlaying file system which is fset. \\# zfs clone -p fpool/fset@initial fpool/initial_clone Now if we get the list of all datasets we can see the initial_clone as a file system in the fpool. A clone can not be created in any pool other than the pool which original dataset resides because of the copy-on-write nature of the ZFS and use of metadata blocks for referring to unchanged data blocks. Now that we have a clone to experiment on, we may need to replace the original dataset with the clone after we succeeded changing the dataset in the way we need. This is called promoting as we are promoting a clone to become the original. The following command shows syntax of the promote command which we can use to swap the clone place with the original dataset. promote clone_name In this syntax the clone_name argument specifies the clone we want to swap its place with the original dataset it is initialized with. To promote the initial_clone we can use the following command. \\# zfs promote fpool/initial_clone When the clone replaces the original dataset, any dependent of the dataset, which includes clones and snapshots, will cease its dependencies on the original dataset and point the dependency toward the newly promoted clone. Destroying a dataset which has dependencies like snapshots and clones is easy when we want to destroy the clones as well. We just need to use the -R option to destroy all dependencies including snapshots and clones. But if we only want to destroy an underlying dataset we should promote one of its clones to take its place and then destroy the dataset without harming dependent datasets. For example following set of commands promote the initial_clone and then destroy the fset file system. \\# zfs promote fpool/initial_clone \\# zfs destroy fpool/fset ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:19:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… A clone is a writeable copy of a dataset, a volume or a file system, which its initial data is the same as the original dataset it is created from. A clone can be initialized from a snapshot of the file system and its initial creation is as fast as creating a snapshot because of the copy-on-write nature of the ZFS file system. A clone size grows in time when we change the clone or the original dataset content. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:20:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also Recipe 2, 3 and 4 are closely related to this recipe as they explain ZFS file systems, ZFS volumes and ZFS snapshots. Reviewing them can help understanding this recipe better. 6. Specifying ZFS mount point In order to use a file system, we should mount it to a mount point so we can read from the file system or write into it. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:21:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool.We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named fset to continue with this recipe. Creating file systems dicussed in recipe 2 of this chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:22:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Let’s create a file system and see where it is mounted by default. Following command creates fset in the fpool. \\# zfs create fpool/fset And following figure shows how it is mounted in the default location. Now let’s use -o option and its argument to change the mount point during file system creation. \\# zfs create -o mountpoint=/fpool/cu-mp fpool/file-set And following figure shows how our -o option works and how we can use the set subcommand to change the mountpoint property of a file system after we create the file system. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:23:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we create a ZFS file system, the ZFS assign a mount point to the file system we created and which complies the default naming schema which is pool_name/file_system/ though the file_system argument itself can have multiple levels. This mount point is stored as a part of the file system metadata in the /etc/zfs and when system is booting it uses this storage to layout the file system as we configured. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:24:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also We can use the mount table to mount a ZFS file system for integrating with legacy software. More information about legacy mode is available at http://docs.sun.com/app/docs/doc/819-5461/gaynd?a=view 7. Replicating a ZFS dataset In t recipe 5 of this chapter we saw how we can clone a dataset but that has its own limitation as it only works inside the mother pool containing the dataset. What if we need to move a dataset from one computer to another or from one pool to another. You may suggest using commands like tar, cpio, and dd which we discussed in recipe NO of chapter 1. Sure we can use them but those commands have their associated drawbacks which we discussed in the related recipe. To facilitate replicating a dataset between different pools or different machines ZFS provides a send and receive subcommands of the zfs command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:25:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready For this recipe we assume that we have two pools named fpool and mpool in one machine, we also assume that we have another machine with ssh server running in a remote machine networked with our workstation. To learn more about ssh review recipe in chapter ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:26:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… In brief, the send and receive subcommands of the zfs command can do the following tasks for us. Sending a ZFS Snapshot means that we can read a ZFS snapshot and send it to a destination. The destination can be a binary file or the receive command. Receiving a ZFS Snapshot means that we receive a ZFS snapshot sent by the send command and writes it in the file system in the same order it was in the origination pool. Remote Replication of ZFS Data: we can use combination of piping zfs second command with ssh and zfs receive commands to send a snapshot from one machine, piping it with ssh executing zfs receive command in a remote machine to write the snapshot on the remote machine. The following command sends a snapshot of the fset file system from fpool to mpool/nset. So the content of the mpool/nset will be an exact clone of the fset when the snapshot was taken. \\# zfs send fpool/fset@snap1 \\| zfs receive mpool/nset The following figure shows more details about the operation. Now lets see a more complicated sample of using the send and receive commands by sending the snapshot over SSH to another machine. \\# zfs send fpool/fset@snap1 \\| ssh machine-02 zfs receive mpool/nset The command will pipe the send output to ssh command which itself is running the receive command. When using SSH client command, ssh, without passing any username it assumes that we want to authenticate using the same user we are loged-in in our local machine. For more details about using SSH review recipe in chapter But the send and receive commands has much more to offer, we can incrementally by sending a group of snapshots to the destination in the same order we have created them. For example following command will send two snapshots named sn1 and sn2 to create the destination file system identical to the state of original file system included in the sn2 which itself is an increment of sn1. \\# zfs send -i fpool/fset@sn1 fpool/fset@sn2 \\| ssh machine-02 zfs recv mpool/fset For this command to succeed, the fset should exist in the remote machine. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:27:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… The send command sends the dumped dataset as a stream to the designated output which can be the default output (screen) or any other output target specified using the pipe operand. The recv command write the file system it receives into the specified dataset as it receives it. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:28:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To understand this recipe better reviewing recipe 4 of this chapter and recipe N of chapter is recommended. To learn more about send and receive commands we can refer to man pages and also the online document available at http://docs.sun.com/app/docs/doc/819-5461/gbchx?a=view. To see whish other additional software are available to replicate a ZFS dataset take a look at http://hub.opensolaris.org/bin/view/Community+Group+zfs/faq/#backupsoftware 8. Managing datasets’ properties In recipe8 in chapter one list of a zpool properties are listed, the list is quite short because of the ZPools nature but for the ZFS datasets we have tens of properties which are either specific to a dataset type (volume, file system) or are shared between both of them. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:29:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready In this recipe we will work on read-only and writeable properties of ZFS datasets. I assume that we have a pool named fpool and a file system named fpool/com to demonstrate the related commands. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:30:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We said that tens of properties are at our disposal to use for our benefit but these properties has some values assigned to them when we create a dataset. These values come from three sources which are listed below. Default: Some properties have default values, like block size for the dataset, or compression status. Inherited: each dataset can inherit its parent properties’ values if we specify Assigned: We assign some properties’ values when we create a dataset like the dataset name. The following table shows important properties of ZFS datasets. Each a description of these properties is given in the http://docs.sun.com/app/docs/doc/817-2271/gazss?a=view . The table specifies whether the volume, file system, clone and snapshot have the property or not. Property File System Volume Clone Snapshot atime available checksum compresiion compressionratio mountpoint quota readonly type used copies volblocksize secondarycache usedbysnapshot Some attributes are solely available to file system and some for a volume. But snapshot and clones share a subset of the properties available for their base dataset. For example a clone made from a file system has the mount point property while a clone made from a volume does not. Some of these properties are read-only meaning that the system itself sets their values, for example available and used, which contain the amount of space available to the dataset and amount of space used from are read-only and only operating system can change them. We can use two zfs subcommands to set a value for a property or get the current value. Following commands show how we can do it. \\# zfs get available fpool/com \\# zfs set mountpoint=fpool/newmp fpool/com To view a complete list of all properties for a dataset along with its source, current value, and write-ablity we can use the all argument for the get command as we can see in the following figure. ZFS datasets inherits many of their properties from their parent datasets. For example if we enable compression in the fpool/comp and then create fpool/comp/inner, the compression algorithm for the fpool/comp/inner will be the same as the compression state of the parent dataset. Sometimes we do not want a property to get inherited by a parent dataset, for example we may specify a custom mount point for a file system when we create it and we do not required the descendent file systems to use the same mount point we can use inherit subcommand of the zfs command to prevent descendent datasets to inherit the custom mount point. \\# zfs create -o mountpoint=/fpool/customs/fs1 fpool/fs1 \\# zfs inherit mountpoint fpool/fs1 From now on, any descendent file system of fs1 will not use the mount point relative to /fpool/customs/fs1 and either uses the default mount point pattern discussed in recipe 6 or uses the mount point we specified. The combination of get and set commands helps us in developing shell scripts to automate our daily jobs. We will discuss more about shell script development in chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:31:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… All ZFS attributes and properties are stored in the ZFS metadata storage located at /etc/zfs. When we invoke a set subcommand or any of the dataset creation command we either insert new records into this storage or we edit the already existing information. When we use get subcommand or any other zfs subcommands, we read the metadata stored in this database. OpenSolaris uses the information stored here to layout the file system during operating system boot-up. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:32:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"There’s more… We can define and later on use custom properties for a ZFS dataset specifying descriptive information for our administrators and users. For example we can create a custom property named used-for to specify what we are using a dataset for. All custom dataset are pre-fixed with : to let us know a property is custom or native to ZFS. To set a custom property we can use set command as shown below. \\# zfs set :used-for=backup fpool/comp To view the value of a property we can use the get subcommand as follow. \\#zfs get used-for fpool/comp Following figure shows how the custom properties appear in the output. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:33:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also For some example of using the set and get commands review recipe 9 and 10 which deals with deduplication and the compression in ZFS file systems. 9. ZFS De-Duplication Deduplication is a file system level space saving method in which redundant data is eliminated. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:34:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready To get started with this recipe we need to have a pool named fpool, so before proceeding further create a ZFS pool named fpool. We can find how we can create ZFS pools in recipe 9 of chapter 2. We should create a file system named dedup inside this pool to makes it possible for the sample commands to execute properly. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:35:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… We can enable deduplication during file system creation or afterward using the set command. Following snippet shows how we can enable it for an fpool/dedup file system during file system creation. \\# zfs create –o deduplocation=on fpool/dedup Following figure shows value of the deduplication property for the fpool/dedup. We can also use the set command to enable deduplication for a file system after we create it. Following command shows how we can turn off the deduplication in the fpool/dedup file system. \\# zfs set deduplication=off fpool/dedup ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:36:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Deduplication operates in the file level so the file system checks every file and decide whether the file already exists in the system or not if the file already exists, the file will not be stored and instead a reference to the already existing file will be used. For example in a typical enterprise mail server one email attachment may find its way into hundreds of users mail boxes, Deduplication will store the actual file once and all other instance of the file are referenced to the same actual file. When we enable deduplication for a file system either when we create the file system or afterward using the set subcommand, we are asking OpenSolaris to only keep one copy of each single file stored in the hard disk and use reference to that single copy when necessary. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:37:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also 1.To learn more about ZFS dataset properties and their usage review recipe number 8. 10. ZFS compression In this recipe we will see how we can enable the ZFS compression to save space. We usually enable the compression when we rarely need to read the dataset content or the reading operation does not need to be performed fast. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:38:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We can enable the compression on both file systems and volumes. Compression can be enabled during the dataset creation or we can enable it after we created the file system and we felt that we need more space rather than higher I/O speed. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:39:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Following command creates a ZFS file system named comp with compression enabled and a custom mount point instead of the default mount point. \\# zfs create -o compression=on -o mountpoint=/fpool/custom fpool/comp We can check the compression ratio of the file system using the get command as follow: \\# zfs get compressratio fpool/comp Following figure shows how the output for this command, however I copied some files into the dataset before checking the compression ration so you can see how it actually works. We can use the set command to enable the compression or change its algorithm on a dataset. For example to change the compression algorithm in the fpool/comp we can use the following command \\# zfs set compression=gzip-9 fpool/comp There are multiple values for the compression option which varies in compression and CPU usage factors. Following table shows these values along with their explanations. Value Description on Turn on the compression using the default algorithm (lzjb). gzip-N same algorithm and compression ration which gzip provides. The N can be 1 to 9 which specifies the compression level. While 1 means no compression 9 is the highest compression level with most CPU usage gzip The gzip algorithm with default compression level set to 6. lzjb default compression algorithm which is a performance friendly algorithm with an acceptable compression ratio off turn off the compression for the given pool. When we create a clone from a dataset with compression enabled, by default the clone will not be compressed unless we enable it either during creation or afterward using the set command. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:40:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… When we enable the compression, our data get compressed before the get written into the disk using the algorithm we specified. When reading the data, each block of data go trough decompression and then upper level applications receives the uncompressed data. When we disable the compression, no further compression is applied on the data we write into file system but the data which are already compressed will stay the same. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:41:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about ZFS dataset properties and their usage review recipe number 8. 11. Defining quota for ZFS ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:42:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"Getting ready We will use the quota and reservation properties of ZFS file system to set a limit on the amount of space the file system can use and amount of space which the pool must have in reserve for the file system. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:43:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How to do it… Assuming that we want to limit the fpool/fset to 1 GB we can use the following command. \\# zfs set quota=1g fpool/fset And to ensure that the fpool always keep at least 200 MB free space for the fset we can use the following command. \\# zfs set reservation=200m fpool/fset The two commands we used means that The overall size of the fset and all of its descendent datasets can not exceed the 1GB limit At each point at time, the fpool has at least 200 MB space ready to be used by the fset. This space will be allocated and counted as used space of the pool. Following figure shows how the reservation and quota properties’ value can affect the status of a zfs dataset. As we can see before we specify the quota, the whole 2 GB of the fpool is available for the fpool/fset while after we specify the quota only 1024 MB of that space is available for the fpool/fset. On the next step, before we specify the reservation, just 102 K of the pool was used while after specifying the reservation; 200 MB of the pool were gone for good. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:44:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"How it works… Defining quota in ZFS is very different compared to defining it in UFS which is described in recipe 7 of chapter 2. In ZFS we define the quotas in file system level for the file system itself and not for users’ usage. In ZFS we specify how much space a file system can use in the pool, quotas of that file system, and how much space the pool must have reserved for a specific file system. To limit how much space a user can use we will assign a file system to the specific user to prevent it from using more than quoted space. To lean more about security and user access management review chapter ? recipe ? ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:45:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"},{"categories":["Solaris","Old Blog Migrated Content"],"content":"See also To learn more about quota take a look at recipe 7 of chapter2. For leaning limiting the amount of space a user can have take a look at recipe of chapter. ","date":"2010-05-09","objectID":"/2010/05/managing-zfs-datasets/:46:0","tags":["Solaris","ZFS","File System"],"title":"In depth tutorial on managing ZFS Datasets","uri":"/2010/05/managing-zfs-datasets/"}]